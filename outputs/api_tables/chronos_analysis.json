[{"Path":"chronos.BaseChronosPipeline.from_pretrained","Module":"chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, pathlib.Path], *model_args, force_s3_download=False, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"force_s3_download\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix, or from the HuggingFace Hub.\nSupports the same ","Parent":"BaseChronosPipeline"},{"Path":"chronos.BaseChronosPipeline.predict","Module":"chronos","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None)","Arguments":"[\"self\", \"inputs\", \"prediction_length\"]","Docstring":"Get forecasts for the given time series. Predictions will be\nreturned in fp32 on the cpu.\n\nParameter","Parent":"BaseChronosPipeline"},{"Path":"chronos.BaseChronosPipeline.predict_df","Module":"chronos","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"BaseChronosPipeline"},{"Path":"chronos.BaseChronosPipeline.predict_fev","Module":"chronos","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"BaseChronosPipeline"},{"Path":"chronos.BaseChronosPipeline.predict_quantiles","Module":"chronos","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"kwargs\"]","Docstring":"Get quantile and mean forecasts for given time series.\nPredictions will be returned in fp32 on the c","Parent":"BaseChronosPipeline"},{"Path":"chronos.BaseChronosPipeline","Module":"chronos","Name":"BaseChronosPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":""},{"Path":"chronos.Chronos2ForecastingConfig.editable_fields","Module":"chronos","Name":"editable_fields","Type":"method","Signature":"() -> list[str]","Arguments":"[]","Docstring":"Fields that maybe modified during the fine-tuning stage.","Parent":"Chronos2ForecastingConfig"},{"Path":"chronos.Chronos2ForecastingConfig","Module":"chronos","Name":"Chronos2ForecastingConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, input_","Parent":""},{"Path":"chronos.Chronos2Model.active_adapters","Module":"chronos","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.add_adapter","Module":"chronos","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.add_memory_hooks","Module":"chronos","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.add_model_tags","Module":"chronos","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.add_module","Module":"chronos","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.apply","Module":"chronos","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.bfloat16","Module":"chronos","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.buffers","Module":"chronos","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.can_generate","Module":"chronos","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.children","Module":"chronos","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.compile","Module":"chronos","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.cpu","Module":"chronos","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.create_extended_attention_mask_for_decoder","Module":"chronos","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.cuda","Module":"chronos","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.delete_adapter","Module":"chronos","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.dequantize","Module":"chronos","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.disable_adapters","Module":"chronos","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.disable_input_require_grads","Module":"chronos","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.double","Module":"chronos","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.enable_adapters","Module":"chronos","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.enable_input_require_grads","Module":"chronos","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.encode","Module":"chronos","Name":"encode","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False)","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.estimate_tokens","Module":"chronos","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.eval","Module":"chronos","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.extra_repr","Module":"chronos","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.float","Module":"chronos","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.floating_point_ops","Module":"chronos","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.forward","Module":"chronos","Name":"forward","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False) -> chronos.chronos2.model.Chronos2Output","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"Forward pass of the Chronos2 model.\n\nParameters\n----------\ncontext\n    Input tensor of shape (batch_","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.from_pretrained","Module":"chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_adapter_state_dict","Module":"chronos","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_buffer","Module":"chronos","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_compiled_call","Module":"chronos","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_correct_attn_implementation","Module":"chronos","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_decoder","Module":"chronos","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_extended_attention_mask","Module":"chronos","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_extra_state","Module":"chronos","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_head_mask","Module":"chronos","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_init_context","Module":"chronos","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_input_embeddings","Module":"chronos","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_memory_footprint","Module":"chronos","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_output_embeddings","Module":"chronos","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_parameter","Module":"chronos","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_parameter_or_buffer","Module":"chronos","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_position_embeddings","Module":"chronos","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.get_submodule","Module":"chronos","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.gradient_checkpointing_disable","Module":"chronos","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.gradient_checkpointing_enable","Module":"chronos","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.half","Module":"chronos","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.init_weights","Module":"chronos","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.initialize_weights","Module":"chronos","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.invert_attention_mask","Module":"chronos","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.ipu","Module":"chronos","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.is_backend_compatible","Module":"chronos","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.kernelize","Module":"chronos","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.load_adapter","Module":"chronos","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.load_state_dict","Module":"chronos","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.modules","Module":"chronos","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.mtia","Module":"chronos","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.named_buffers","Module":"chronos","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.named_children","Module":"chronos","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.named_modules","Module":"chronos","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.named_parameters","Module":"chronos","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.num_parameters","Module":"chronos","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.parameters","Module":"chronos","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.post_init","Module":"chronos","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.prune_heads","Module":"chronos","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.push_to_hub","Module":"chronos","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_backward_hook","Module":"chronos","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_buffer","Module":"chronos","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_for_auto_class","Module":"chronos","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_forward_hook","Module":"chronos","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_forward_pre_hook","Module":"chronos","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_full_backward_hook","Module":"chronos","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_full_backward_pre_hook","Module":"chronos","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_load_state_dict_post_hook","Module":"chronos","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_load_state_dict_pre_hook","Module":"chronos","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_module","Module":"chronos","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_parameter","Module":"chronos","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_state_dict_post_hook","Module":"chronos","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.register_state_dict_pre_hook","Module":"chronos","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.requires_grad_","Module":"chronos","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.reset_memory_hooks_state","Module":"chronos","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.resize_position_embeddings","Module":"chronos","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.resize_token_embeddings","Module":"chronos","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.retrieve_modules_from_names","Module":"chronos","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.reverse_bettertransformer","Module":"chronos","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.save_pretrained","Module":"chronos","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.set_adapter","Module":"chronos","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.set_attn_implementation","Module":"chronos","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.set_decoder","Module":"chronos","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.set_extra_state","Module":"chronos","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.set_input_embeddings","Module":"chronos","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.set_output_embeddings","Module":"chronos","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.set_submodule","Module":"chronos","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.share_memory","Module":"chronos","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.state_dict","Module":"chronos","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.tie_embeddings_and_encoder_decoder","Module":"chronos","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.tie_weights","Module":"chronos","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.to","Module":"chronos","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.to_bettertransformer","Module":"chronos","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.to_empty","Module":"chronos","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.train","Module":"chronos","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.type","Module":"chronos","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.warn_if_padding_and_no_attention_mask","Module":"chronos","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.xpu","Module":"chronos","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model.zero_grad","Module":"chronos","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2Model"},{"Path":"chronos.Chronos2Model","Module":"chronos","Name":"Chronos2Model","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all models.\n\n[`PreTrainedModel`] takes care of storing the configuration of the model","Parent":""},{"Path":"chronos.Chronos2Pipeline.embed","Module":"chronos","Name":"embed","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray]], batch_size: int = 256, context_length: int | None = None) -> tuple[list[torch.Tensor], list[tuple[torch.Tensor, torch.Tensor]]]","Arguments":"[\"self\", \"inputs\", \"batch_size\", \"context_length\"]","Docstring":"Get encoder embeddings for the given time series.\n\nParameters\n----------\ninputs\n    The time series ","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline.fit","Module":"chronos","Name":"fit","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]]], prediction_length: int, validation_inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]], NoneType] = None, finetune_mode: Literal['full', 'lora'] = 'full', lora_config: 'LoraConfig | dict | None' = None, context_length: int | None = None, learning_rate: float = 1e-06, num_steps: int = 1000, batch_size: int = 256, output_dir: pathlib.Path | str | None = None, min_past: int | None = None, finetuned_ckpt_name: str = 'finetuned-ckpt', callbacks: list['TrainerCallback'] | None = None, remove_printer_callback: bool = False, disable_data_parallel: bool = True, **extra_trainer_kwargs) -> 'Chronos2Pipeline'","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"validation_inputs\", \"finetune_mode\", \"lora_config\", \"context_length\", \"learning_rate\", \"num_steps\", \"batch_size\", \"output_dir\", \"min_past\", \"finetuned_ckpt_name\", \"callbacks\", \"remove_printer_callback\", \"disable_data_parallel\", \"extra_trainer_kwargs\"]","Docstring":"Fine-tune a copy of the current Chronos-2 model on the given inputs and return a new pipeline.\n\nPara","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline.from_pretrained","Module":"chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path, *args, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"args\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix or from the HuggingFace Hub.\nSupports the same a","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline.predict","Module":"chronos","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]]]], prediction_length: int | None = None, batch_size: int = 256, context_length: int | None = None, cross_learning: bool = False, limit_prediction_length: bool = False, **kwargs) -> list[torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"batch_size\", \"context_length\", \"cross_learning\", \"limit_prediction_length\", \"kwargs\"]","Docstring":"Generate forecasts for the given time series.\n\nParameters\n----------\ninputs\n    The time series to g","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline.predict_df","Module":"chronos","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', future_df: 'pd.DataFrame | None' = None, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str | list[str] = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], batch_size: int = 256, context_length: int | None = None, cross_learning: bool = False, validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"future_df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"batch_size\", \"context_length\", \"cross_learning\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline.predict_fev","Module":"chronos","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 256, as_univariate: bool = False, finetune_kwargs: dict | None = None, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"as_univariate\", \"finetune_kwargs\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline.predict_quantiles","Module":"chronos","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]]]], prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **predict_kwargs) -> tuple[list[torch.Tensor], list[torch.Tensor]]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"predict_kwargs\"]","Docstring":"Refer to ``Chronos2Pipeline.predict`` for shared parameters.\n\nAdditional parameters\n----------------","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline.save_pretrained","Module":"chronos","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: str | pathlib.Path, *args, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"args\", \"kwargs\"]","Docstring":"Save the underlying model to a local directory or to HuggingFace Hub.","Parent":"Chronos2Pipeline"},{"Path":"chronos.Chronos2Pipeline","Module":"chronos","Name":"Chronos2Pipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":""},{"Path":"chronos.ChronosBoltConfig","Module":"chronos","Name":"ChronosBoltConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patch_st","Parent":""},{"Path":"chronos.ChronosBoltPipeline.embed","Module":"chronos","Name":"embed","Type":"method","Signature":"(self, context: Union[torch.Tensor, List[torch.Tensor]]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]","Arguments":"[\"self\", \"context\"]","Docstring":"Get encoder embeddings for the given time series.\n\nParameters\n----------\ncontext\n    Input series. T","Parent":"ChronosBoltPipeline"},{"Path":"chronos.ChronosBoltPipeline.from_pretrained","Module":"chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path, *args, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"args\", \"kwargs\"]","Docstring":"Load the model, either from a local path S3 prefix or from the HuggingFace Hub.\nSupports the same ar","Parent":"ChronosBoltPipeline"},{"Path":"chronos.ChronosBoltPipeline.predict","Module":"chronos","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, limit_prediction_length: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"limit_prediction_length\"]","Docstring":"Get forecasts for the given time series.\n\nRefer to the base method (``BaseChronosPipeline.predict``)","Parent":"ChronosBoltPipeline"},{"Path":"chronos.ChronosBoltPipeline.predict_df","Module":"chronos","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"ChronosBoltPipeline"},{"Path":"chronos.ChronosBoltPipeline.predict_fev","Module":"chronos","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"ChronosBoltPipeline"},{"Path":"chronos.ChronosBoltPipeline.predict_quantiles","Module":"chronos","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **predict_kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"predict_kwargs\"]","Docstring":"Refer to the base method (``BaseChronosPipeline.predict_quantiles``).","Parent":"ChronosBoltPipeline"},{"Path":"chronos.ChronosBoltPipeline","Module":"chronos","Name":"ChronosBoltPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":""},{"Path":"chronos.ChronosConfig.create_tokenizer","Module":"chronos","Name":"create_tokenizer","Type":"method","Signature":"(self) -> 'ChronosTokenizer'","Arguments":"[\"self\"]","Docstring":"","Parent":"ChronosConfig"},{"Path":"chronos.ChronosConfig","Module":"chronos","Name":"ChronosConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"This class holds all the configuration parameters to be used\nby ``ChronosTokenizer`` and ``ChronosMo","Parent":""},{"Path":"chronos.ChronosModel.add_module","Module":"chronos","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.apply","Module":"chronos","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.bfloat16","Module":"chronos","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.buffers","Module":"chronos","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.children","Module":"chronos","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.compile","Module":"chronos","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.cpu","Module":"chronos","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.cuda","Module":"chronos","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.double","Module":"chronos","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.encode","Module":"chronos","Name":"encode","Type":"method","Signature":"(self, input_ids: torch.Tensor, attention_mask: torch.Tensor)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Extract the encoder embedding for the given token sequences.\n\nParameters\n----------\ninput_ids\n    Te","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.eval","Module":"chronos","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.extra_repr","Module":"chronos","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.float","Module":"chronos","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.forward","Module":"chronos","Name":"forward","Type":"method","Signature":"(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, prediction_length: Optional[int] = None, num_samples: Optional[int] = None, temperature: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None) -> torch.Tensor","Arguments":"[\"self\", \"input_ids\", \"attention_mask\", \"prediction_length\", \"num_samples\", \"temperature\", \"top_k\", \"top_p\"]","Docstring":"Predict future sample tokens for the given token sequences.\n\nArguments ``prediction_length``, ``num_","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.get_buffer","Module":"chronos","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.get_extra_state","Module":"chronos","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.get_parameter","Module":"chronos","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.get_submodule","Module":"chronos","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.half","Module":"chronos","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.ipu","Module":"chronos","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.load_state_dict","Module":"chronos","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.modules","Module":"chronos","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.mtia","Module":"chronos","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.named_buffers","Module":"chronos","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.named_children","Module":"chronos","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.named_modules","Module":"chronos","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.named_parameters","Module":"chronos","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.parameters","Module":"chronos","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_backward_hook","Module":"chronos","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_buffer","Module":"chronos","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_forward_hook","Module":"chronos","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_forward_pre_hook","Module":"chronos","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_full_backward_hook","Module":"chronos","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_full_backward_pre_hook","Module":"chronos","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_load_state_dict_post_hook","Module":"chronos","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_load_state_dict_pre_hook","Module":"chronos","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_module","Module":"chronos","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_parameter","Module":"chronos","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_state_dict_post_hook","Module":"chronos","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.register_state_dict_pre_hook","Module":"chronos","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.requires_grad_","Module":"chronos","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.set_extra_state","Module":"chronos","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.set_submodule","Module":"chronos","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.share_memory","Module":"chronos","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.state_dict","Module":"chronos","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.to","Module":"chronos","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.to_empty","Module":"chronos","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.train","Module":"chronos","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.type","Module":"chronos","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.xpu","Module":"chronos","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel.zero_grad","Module":"chronos","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"ChronosModel"},{"Path":"chronos.ChronosModel","Module":"chronos","Name":"ChronosModel","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transformers``\nand uses it to predict s","Parent":""},{"Path":"chronos.ChronosPipeline.embed","Module":"chronos","Name":"embed","Type":"method","Signature":"(self, context: Union[torch.Tensor, List[torch.Tensor]]) -> Tuple[torch.Tensor, Any]","Arguments":"[\"self\", \"context\"]","Docstring":"Get encoder embeddings for the given time series.\n\nParameters\n----------\ncontext\n    Input series. T","Parent":"ChronosPipeline"},{"Path":"chronos.ChronosPipeline.from_pretrained","Module":"chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path, *args, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"args\", \"kwargs\"]","Docstring":"Load the model, either from a local path S3 prefix or from the HuggingFace Hub.\nSupports the same ar","Parent":"ChronosPipeline"},{"Path":"chronos.ChronosPipeline.predict","Module":"chronos","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, num_samples: Optional[int] = None, temperature: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, limit_prediction_length: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"num_samples\", \"temperature\", \"top_k\", \"top_p\", \"limit_prediction_length\"]","Docstring":"Get forecasts for the given time series.\n\nRefer to the base method (``BaseChronosPipeline.predict``)","Parent":"ChronosPipeline"},{"Path":"chronos.ChronosPipeline.predict_df","Module":"chronos","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"ChronosPipeline"},{"Path":"chronos.ChronosPipeline.predict_fev","Module":"chronos","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"ChronosPipeline"},{"Path":"chronos.ChronosPipeline.predict_quantiles","Module":"chronos","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **predict_kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"predict_kwargs\"]","Docstring":"Refer to the base method (``BaseChronosPipeline.predict_quantiles``).","Parent":"ChronosPipeline"},{"Path":"chronos.ChronosPipeline","Module":"chronos","Name":"ChronosPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosPipeline`` uses the given tokenizer and model to forecast\ninput time series.\n\nUse the ``f","Parent":""},{"Path":"chronos.ChronosTokenizer.context_input_transform","Module":"chronos","Name":"context_input_transform","Type":"method","Signature":"(self, context: torch.Tensor) -> Tuple","Arguments":"[\"self\", \"context\"]","Docstring":"Turn a batch of time series into token IDs, attention map, and tokenizer_state.\n\nParameters\n--------","Parent":"ChronosTokenizer"},{"Path":"chronos.ChronosTokenizer.label_input_transform","Module":"chronos","Name":"label_input_transform","Type":"method","Signature":"(self, label: torch.Tensor, tokenizer_state: Any) -> Tuple","Arguments":"[\"self\", \"label\", \"tokenizer_state\"]","Docstring":"Turn a batch of label slices of time series into token IDs and attention map\nusing the ``tokenizer_s","Parent":"ChronosTokenizer"},{"Path":"chronos.ChronosTokenizer.output_transform","Module":"chronos","Name":"output_transform","Type":"method","Signature":"(self, samples: torch.Tensor, tokenizer_state: Any) -> torch.Tensor","Arguments":"[\"self\", \"samples\", \"tokenizer_state\"]","Docstring":"Turn a batch of sample token IDs into real values.\n\nParameters\n----------\nsamples\n    A tensor of in","Parent":"ChronosTokenizer"},{"Path":"chronos.ChronosTokenizer","Module":"chronos","Name":"ChronosTokenizer","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosTokenizer`` defines how time series are mapped into token IDs\nand back.\n\nFor details, see","Parent":""},{"Path":"chronos.ForecastType","Module":"chronos","Name":"ForecastType","Type":"class","Signature":"","Arguments":"[]","Docstring":"Create a collection of name\/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED =","Parent":""},{"Path":"chronos.MeanScaleUniformBins.context_input_transform","Module":"chronos","Name":"context_input_transform","Type":"method","Signature":"(self, context: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"context\"]","Docstring":"Turn a batch of time series into token IDs, attention map, and tokenizer_state.\n\nParameters\n--------","Parent":"MeanScaleUniformBins"},{"Path":"chronos.MeanScaleUniformBins.label_input_transform","Module":"chronos","Name":"label_input_transform","Type":"method","Signature":"(self, label: torch.Tensor, scale: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"label\", \"scale\"]","Docstring":"Turn a batch of label slices of time series into token IDs and attention map\nusing the ``tokenizer_s","Parent":"MeanScaleUniformBins"},{"Path":"chronos.MeanScaleUniformBins.output_transform","Module":"chronos","Name":"output_transform","Type":"method","Signature":"(self, samples: torch.Tensor, scale: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"samples\", \"scale\"]","Docstring":"Turn a batch of sample token IDs into real values.\n\nParameters\n----------\nsamples\n    A tensor of in","Parent":"MeanScaleUniformBins"},{"Path":"chronos.MeanScaleUniformBins","Module":"chronos","Name":"MeanScaleUniformBins","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosTokenizer`` defines how time series are mapped into token IDs\nand back.\n\nFor details, see","Parent":""},{"Path":"chronos.utils.List","Module":"chronos.utils","Name":"List","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of list.","Parent":"chronos"},{"Path":"chronos.utils.interpolate_quantiles","Module":"chronos.utils","Name":"interpolate_quantiles","Type":"function","Signature":"(query_quantile_levels: torch.Tensor | list[float], original_quantile_levels: torch.Tensor | list[float], original_values: torch.Tensor) -> torch.Tensor","Arguments":"[\"query_quantile_levels\", \"original_quantile_levels\", \"original_values\"]","Docstring":"Interpolates quantile values at specified query levels using linear interpolation using original\nqua","Parent":"chronos"},{"Path":"chronos.utils.left_pad_and_stack_1D","Module":"chronos.utils","Name":"left_pad_and_stack_1D","Type":"function","Signature":"(tensors: List[torch.Tensor]) -> torch.Tensor","Arguments":"[\"tensors\"]","Docstring":"","Parent":"chronos"},{"Path":"chronos.utils.repeat","Module":"chronos.utils","Name":"repeat","Type":"function","Signature":"(tensor: Union[~Tensor, List[~Tensor]], pattern: str, **axes_lengths: Any) -> ~Tensor","Arguments":"[\"tensor\", \"pattern\", \"axes_lengths\"]","Docstring":"einops.repeat allows reordering elements and repeating them in arbitrary combinations.\nThis operatio","Parent":"chronos"},{"Path":"chronos.utils.weighted_quantile","Module":"chronos.utils","Name":"weighted_quantile","Type":"function","Signature":"(query_quantile_levels: torch.Tensor | list[float], sample_weights: torch.Tensor | list[float], samples: torch.Tensor)","Arguments":"[\"query_quantile_levels\", \"sample_weights\", \"samples\"]","Docstring":"Computes quantiles from a distribution specified by `samples` and their corresponding probability ma","Parent":"chronos"},{"Path":"chronos.df_utils.TYPE_CHECKING","Module":"chronos.df_utils","Name":"TYPE_CHECKING","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and Fa","Parent":"chronos"},{"Path":"chronos.df_utils.convert_df_input_to_list_of_dicts_input","Module":"chronos.df_utils","Name":"convert_df_input_to_list_of_dicts_input","Type":"function","Signature":"(df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_length: int, id_column: str = 'item_id', timestamp_column: str = 'timestamp', validate_inputs: bool = True, freq: str | None = None) -> tuple[list[dict[str, numpy.ndarray | dict[str, numpy.ndarray]]], numpy.ndarray, dict[str, 'pd.DatetimeIndex']]","Arguments":"[\"df\", \"future_df\", \"target_columns\", \"prediction_length\", \"id_column\", \"timestamp_column\", \"validate_inputs\", \"freq\"]","Docstring":"Convert from dataframe input format to a list of dictionaries input format.\n\nParameters\n----------\nd","Parent":"chronos"},{"Path":"chronos.df_utils.validate_df_inputs","Module":"chronos.df_utils","Name":"validate_df_inputs","Type":"function","Signature":"(df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_length: int, id_column: str = 'item_id', timestamp_column: str = 'timestamp') -> tuple['pd.DataFrame', 'pd.DataFrame | None', str, list[int], numpy.ndarray]","Arguments":"[\"df\", \"future_df\", \"target_columns\", \"prediction_length\", \"id_column\", \"timestamp_column\"]","Docstring":"Validates and prepares dataframe inputs\n\nParameters\n----------\ndf\n    Input dataframe containing tim","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ACT2FN","Module":"chronos.chronos_bolt","Name":"ACT2FN","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos"},{"Path":"chronos.chronos_bolt.AutoConfig.for_model","Module":"chronos.chronos_bolt","Name":"for_model","Type":"method","Signature":"(model_type: str, *args, **kwargs) -> transformers.configuration_utils.PretrainedConfig","Arguments":"[\"model_type\", \"args\", \"kwargs\"]","Docstring":"","Parent":"AutoConfig"},{"Path":"chronos.chronos_bolt.AutoConfig.from_pretrained","Module":"chronos.chronos_bolt","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike[str]], **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"Instantiate one of the configuration classes of the library from a pretrained model configuration.\n\n","Parent":"AutoConfig"},{"Path":"chronos.chronos_bolt.AutoConfig.register","Module":"chronos.chronos_bolt","Name":"register","Type":"method","Signature":"(model_type, config, exist_ok=False) -> None","Arguments":"[\"model_type\", \"config\", \"exist_ok\"]","Docstring":"Register a new configuration for this class.\n\nArgs:\n    model_type (`str`): The model type like \"ber","Parent":"AutoConfig"},{"Path":"chronos.chronos_bolt.AutoConfig","Module":"chronos.chronos_bolt","Name":"AutoConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"This is a generic configuration class that will be instantiated as one of the configuration classes ","Parent":"chronos"},{"Path":"chronos.chronos_bolt.BaseChronosPipeline.from_pretrained","Module":"chronos.chronos_bolt","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, pathlib.Path], *model_args, force_s3_download=False, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"force_s3_download\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix, or from the HuggingFace Hub.\nSupports the same ","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos_bolt.BaseChronosPipeline.predict","Module":"chronos.chronos_bolt","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None)","Arguments":"[\"self\", \"inputs\", \"prediction_length\"]","Docstring":"Get forecasts for the given time series. Predictions will be\nreturned in fp32 on the cpu.\n\nParameter","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos_bolt.BaseChronosPipeline.predict_df","Module":"chronos.chronos_bolt","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos_bolt.BaseChronosPipeline.predict_fev","Module":"chronos.chronos_bolt","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos_bolt.BaseChronosPipeline.predict_quantiles","Module":"chronos.chronos_bolt","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"kwargs\"]","Docstring":"Get quantile and mean forecasts for given time series.\nPredictions will be returned in fp32 on the c","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos_bolt.BaseChronosPipeline","Module":"chronos.chronos_bolt","Name":"BaseChronosPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ChronosBoltConfig","Module":"chronos.chronos_bolt","Name":"ChronosBoltConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"ChronosBoltConfig(context_length: int, prediction_length: int, input_patch_size: int, input_patch_st","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.active_adapters","Module":"chronos.chronos_bolt","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.add_adapter","Module":"chronos.chronos_bolt","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.add_memory_hooks","Module":"chronos.chronos_bolt","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.add_model_tags","Module":"chronos.chronos_bolt","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.add_module","Module":"chronos.chronos_bolt","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.apply","Module":"chronos.chronos_bolt","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.bfloat16","Module":"chronos.chronos_bolt","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.buffers","Module":"chronos.chronos_bolt","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.can_generate","Module":"chronos.chronos_bolt","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.children","Module":"chronos.chronos_bolt","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.compile","Module":"chronos.chronos_bolt","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.cpu","Module":"chronos.chronos_bolt","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.create_extended_attention_mask_for_decoder","Module":"chronos.chronos_bolt","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.cuda","Module":"chronos.chronos_bolt","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.decode","Module":"chronos.chronos_bolt","Name":"decode","Type":"method","Signature":"(self, input_embeds, attention_mask, hidden_states, output_attentions=False)","Arguments":"[\"self\", \"input_embeds\", \"attention_mask\", \"hidden_states\", \"output_attentions\"]","Docstring":"Parameters\n----------\ninput_embeds: torch.Tensor\n    Patched and embedded inputs. Shape (batch_size,","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.delete_adapter","Module":"chronos.chronos_bolt","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.dequantize","Module":"chronos.chronos_bolt","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.disable_adapters","Module":"chronos.chronos_bolt","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.disable_input_require_grads","Module":"chronos.chronos_bolt","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.double","Module":"chronos.chronos_bolt","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.enable_adapters","Module":"chronos.chronos_bolt","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.enable_input_require_grads","Module":"chronos.chronos_bolt","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.encode","Module":"chronos.chronos_bolt","Name":"encode","Type":"method","Signature":"(self, context: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor], torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"context\", \"mask\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.estimate_tokens","Module":"chronos.chronos_bolt","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.eval","Module":"chronos.chronos_bolt","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.extra_repr","Module":"chronos.chronos_bolt","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.float","Module":"chronos.chronos_bolt","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.floating_point_ops","Module":"chronos.chronos_bolt","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.forward","Module":"chronos.chronos_bolt","Name":"forward","Type":"method","Signature":"(self, context: torch.Tensor, mask: Optional[torch.Tensor] = None, target: Optional[torch.Tensor] = None, target_mask: Optional[torch.Tensor] = None) -> chronos.chronos_bolt.ChronosBoltOutput","Arguments":"[\"self\", \"context\", \"mask\", \"target\", \"target_mask\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.from_pretrained","Module":"chronos.chronos_bolt","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_adapter_state_dict","Module":"chronos.chronos_bolt","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_buffer","Module":"chronos.chronos_bolt","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_compiled_call","Module":"chronos.chronos_bolt","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_correct_attn_implementation","Module":"chronos.chronos_bolt","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_decoder","Module":"chronos.chronos_bolt","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_extended_attention_mask","Module":"chronos.chronos_bolt","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_extra_state","Module":"chronos.chronos_bolt","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_head_mask","Module":"chronos.chronos_bolt","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_init_context","Module":"chronos.chronos_bolt","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_input_embeddings","Module":"chronos.chronos_bolt","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_memory_footprint","Module":"chronos.chronos_bolt","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_output_embeddings","Module":"chronos.chronos_bolt","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_parameter","Module":"chronos.chronos_bolt","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_parameter_or_buffer","Module":"chronos.chronos_bolt","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_position_embeddings","Module":"chronos.chronos_bolt","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.get_submodule","Module":"chronos.chronos_bolt","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.gradient_checkpointing_disable","Module":"chronos.chronos_bolt","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.gradient_checkpointing_enable","Module":"chronos.chronos_bolt","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.half","Module":"chronos.chronos_bolt","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.init_weights","Module":"chronos.chronos_bolt","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.initialize_weights","Module":"chronos.chronos_bolt","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.invert_attention_mask","Module":"chronos.chronos_bolt","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.ipu","Module":"chronos.chronos_bolt","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.is_backend_compatible","Module":"chronos.chronos_bolt","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.kernelize","Module":"chronos.chronos_bolt","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.load_adapter","Module":"chronos.chronos_bolt","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.load_state_dict","Module":"chronos.chronos_bolt","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.load_tf_weights","Module":"chronos.chronos_bolt","Name":"load_tf_weights","Type":"method","Signature":"(model, config, tf_checkpoint_path)","Arguments":"[\"model\", \"config\", \"tf_checkpoint_path\"]","Docstring":"Load tf checkpoints in a pytorch model.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.modules","Module":"chronos.chronos_bolt","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.mtia","Module":"chronos.chronos_bolt","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.named_buffers","Module":"chronos.chronos_bolt","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.named_children","Module":"chronos.chronos_bolt","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.named_modules","Module":"chronos.chronos_bolt","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.named_parameters","Module":"chronos.chronos_bolt","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.num_parameters","Module":"chronos.chronos_bolt","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.parameters","Module":"chronos.chronos_bolt","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.post_init","Module":"chronos.chronos_bolt","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.prune_heads","Module":"chronos.chronos_bolt","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.push_to_hub","Module":"chronos.chronos_bolt","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_backward_hook","Module":"chronos.chronos_bolt","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_buffer","Module":"chronos.chronos_bolt","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_for_auto_class","Module":"chronos.chronos_bolt","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_forward_hook","Module":"chronos.chronos_bolt","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_forward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_full_backward_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_full_backward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_load_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_load_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_module","Module":"chronos.chronos_bolt","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_parameter","Module":"chronos.chronos_bolt","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.register_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.requires_grad_","Module":"chronos.chronos_bolt","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.reset_memory_hooks_state","Module":"chronos.chronos_bolt","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.resize_position_embeddings","Module":"chronos.chronos_bolt","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.resize_token_embeddings","Module":"chronos.chronos_bolt","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.retrieve_modules_from_names","Module":"chronos.chronos_bolt","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.reverse_bettertransformer","Module":"chronos.chronos_bolt","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.save_pretrained","Module":"chronos.chronos_bolt","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.set_adapter","Module":"chronos.chronos_bolt","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.set_attn_implementation","Module":"chronos.chronos_bolt","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.set_decoder","Module":"chronos.chronos_bolt","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.set_extra_state","Module":"chronos.chronos_bolt","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.set_input_embeddings","Module":"chronos.chronos_bolt","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.set_output_embeddings","Module":"chronos.chronos_bolt","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.set_submodule","Module":"chronos.chronos_bolt","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.share_memory","Module":"chronos.chronos_bolt","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.state_dict","Module":"chronos.chronos_bolt","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos_bolt","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.tie_weights","Module":"chronos.chronos_bolt","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.to","Module":"chronos.chronos_bolt","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.to_bettertransformer","Module":"chronos.chronos_bolt","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.to_empty","Module":"chronos.chronos_bolt","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.train","Module":"chronos.chronos_bolt","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.type","Module":"chronos.chronos_bolt","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos_bolt","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.xpu","Module":"chronos.chronos_bolt","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting.zero_grad","Module":"chronos.chronos_bolt","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"ChronosBoltModelForForecasting"},{"Path":"chronos.chronos_bolt.ChronosBoltModelForForecasting","Module":"chronos.chronos_bolt","Name":"ChronosBoltModelForForecasting","Type":"class","Signature":"","Arguments":"[]","Docstring":"This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic met","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ChronosBoltOutput.pop","Module":"chronos.chronos_bolt","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"ChronosBoltOutput"},{"Path":"chronos.chronos_bolt.ChronosBoltOutput.setdefault","Module":"chronos.chronos_bolt","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"ChronosBoltOutput"},{"Path":"chronos.chronos_bolt.ChronosBoltOutput.to_tuple","Module":"chronos.chronos_bolt","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"ChronosBoltOutput"},{"Path":"chronos.chronos_bolt.ChronosBoltOutput.update","Module":"chronos.chronos_bolt","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"ChronosBoltOutput"},{"Path":"chronos.chronos_bolt.ChronosBoltOutput","Module":"chronos.chronos_bolt","Name":"ChronosBoltOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"ChronosBoltOutput(loss: Optional[torch.Tensor] = None, quantile_preds: Optional[torch.Tensor] = None","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ChronosBoltPipeline.embed","Module":"chronos.chronos_bolt","Name":"embed","Type":"method","Signature":"(self, context: Union[torch.Tensor, List[torch.Tensor]]) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]","Arguments":"[\"self\", \"context\"]","Docstring":"Get encoder embeddings for the given time series.\n\nParameters\n----------\ncontext\n    Input series. T","Parent":"ChronosBoltPipeline"},{"Path":"chronos.chronos_bolt.ChronosBoltPipeline.from_pretrained","Module":"chronos.chronos_bolt","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path, *args, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"args\", \"kwargs\"]","Docstring":"Load the model, either from a local path S3 prefix or from the HuggingFace Hub.\nSupports the same ar","Parent":"ChronosBoltPipeline"},{"Path":"chronos.chronos_bolt.ChronosBoltPipeline.predict","Module":"chronos.chronos_bolt","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, limit_prediction_length: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"limit_prediction_length\"]","Docstring":"Get forecasts for the given time series.\n\nRefer to the base method (``BaseChronosPipeline.predict``)","Parent":"ChronosBoltPipeline"},{"Path":"chronos.chronos_bolt.ChronosBoltPipeline.predict_df","Module":"chronos.chronos_bolt","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"ChronosBoltPipeline"},{"Path":"chronos.chronos_bolt.ChronosBoltPipeline.predict_fev","Module":"chronos.chronos_bolt","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"ChronosBoltPipeline"},{"Path":"chronos.chronos_bolt.ChronosBoltPipeline.predict_quantiles","Module":"chronos.chronos_bolt","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **predict_kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"predict_kwargs\"]","Docstring":"Refer to the base method (``BaseChronosPipeline.predict_quantiles``).","Parent":"ChronosBoltPipeline"},{"Path":"chronos.chronos_bolt.ChronosBoltPipeline","Module":"chronos.chronos_bolt","Name":"ChronosBoltPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ForecastType","Module":"chronos.chronos_bolt","Name":"ForecastType","Type":"class","Signature":"","Arguments":"[]","Docstring":"Create a collection of name\/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED =","Parent":"chronos"},{"Path":"chronos.chronos_bolt.InstanceNorm.add_module","Module":"chronos.chronos_bolt","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.apply","Module":"chronos.chronos_bolt","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.bfloat16","Module":"chronos.chronos_bolt","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.buffers","Module":"chronos.chronos_bolt","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.children","Module":"chronos.chronos_bolt","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.compile","Module":"chronos.chronos_bolt","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.cpu","Module":"chronos.chronos_bolt","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.cuda","Module":"chronos.chronos_bolt","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.double","Module":"chronos.chronos_bolt","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.eval","Module":"chronos.chronos_bolt","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.extra_repr","Module":"chronos.chronos_bolt","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.float","Module":"chronos.chronos_bolt","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.forward","Module":"chronos.chronos_bolt","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor, loc_scale: tuple[torch.Tensor, torch.Tensor] | None = None) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]","Arguments":"[\"self\", \"x\", \"loc_scale\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.get_buffer","Module":"chronos.chronos_bolt","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.get_extra_state","Module":"chronos.chronos_bolt","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.get_parameter","Module":"chronos.chronos_bolt","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.get_submodule","Module":"chronos.chronos_bolt","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.half","Module":"chronos.chronos_bolt","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.inverse","Module":"chronos.chronos_bolt","Name":"inverse","Type":"method","Signature":"(self, x: torch.Tensor, loc_scale: tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor","Arguments":"[\"self\", \"x\", \"loc_scale\"]","Docstring":"","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.ipu","Module":"chronos.chronos_bolt","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.load_state_dict","Module":"chronos.chronos_bolt","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.modules","Module":"chronos.chronos_bolt","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.mtia","Module":"chronos.chronos_bolt","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.named_buffers","Module":"chronos.chronos_bolt","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.named_children","Module":"chronos.chronos_bolt","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.named_modules","Module":"chronos.chronos_bolt","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.named_parameters","Module":"chronos.chronos_bolt","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.parameters","Module":"chronos.chronos_bolt","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_backward_hook","Module":"chronos.chronos_bolt","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_buffer","Module":"chronos.chronos_bolt","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_forward_hook","Module":"chronos.chronos_bolt","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_forward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_full_backward_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_full_backward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_load_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_load_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_module","Module":"chronos.chronos_bolt","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_parameter","Module":"chronos.chronos_bolt","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.register_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.requires_grad_","Module":"chronos.chronos_bolt","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.set_extra_state","Module":"chronos.chronos_bolt","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.set_submodule","Module":"chronos.chronos_bolt","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.share_memory","Module":"chronos.chronos_bolt","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.state_dict","Module":"chronos.chronos_bolt","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.to","Module":"chronos.chronos_bolt","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.to_empty","Module":"chronos.chronos_bolt","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.train","Module":"chronos.chronos_bolt","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.type","Module":"chronos.chronos_bolt","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.xpu","Module":"chronos.chronos_bolt","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm.zero_grad","Module":"chronos.chronos_bolt","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"InstanceNorm"},{"Path":"chronos.chronos_bolt.InstanceNorm","Module":"chronos.chronos_bolt","Name":"InstanceNorm","Type":"class","Signature":"","Arguments":"[]","Docstring":"Apply standardization along the last dimension and optionally apply arcsinh after standardization.","Parent":"chronos"},{"Path":"chronos.chronos_bolt.List","Module":"chronos.chronos_bolt","Name":"List","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of list.","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ModelOutput.pop","Module":"chronos.chronos_bolt","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"ModelOutput"},{"Path":"chronos.chronos_bolt.ModelOutput.setdefault","Module":"chronos.chronos_bolt","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"ModelOutput"},{"Path":"chronos.chronos_bolt.ModelOutput.to_tuple","Module":"chronos.chronos_bolt","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"ModelOutput"},{"Path":"chronos.chronos_bolt.ModelOutput.update","Module":"chronos.chronos_bolt","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"ModelOutput"},{"Path":"chronos.chronos_bolt.ModelOutput","Module":"chronos.chronos_bolt","Name":"ModelOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer o","Parent":"chronos"},{"Path":"chronos.chronos_bolt.Optional","Module":"chronos.chronos_bolt","Name":"Optional","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Optional[X] is equivalent to Union[X, None].","Parent":"chronos"},{"Path":"chronos.chronos_bolt.Patch.add_module","Module":"chronos.chronos_bolt","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.apply","Module":"chronos.chronos_bolt","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.bfloat16","Module":"chronos.chronos_bolt","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.buffers","Module":"chronos.chronos_bolt","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.children","Module":"chronos.chronos_bolt","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.compile","Module":"chronos.chronos_bolt","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.cpu","Module":"chronos.chronos_bolt","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.cuda","Module":"chronos.chronos_bolt","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.double","Module":"chronos.chronos_bolt","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.eval","Module":"chronos.chronos_bolt","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.extra_repr","Module":"chronos.chronos_bolt","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.float","Module":"chronos.chronos_bolt","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.forward","Module":"chronos.chronos_bolt","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"x\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.get_buffer","Module":"chronos.chronos_bolt","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.get_extra_state","Module":"chronos.chronos_bolt","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.get_parameter","Module":"chronos.chronos_bolt","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.get_submodule","Module":"chronos.chronos_bolt","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.half","Module":"chronos.chronos_bolt","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.ipu","Module":"chronos.chronos_bolt","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.load_state_dict","Module":"chronos.chronos_bolt","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.modules","Module":"chronos.chronos_bolt","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.mtia","Module":"chronos.chronos_bolt","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.named_buffers","Module":"chronos.chronos_bolt","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.named_children","Module":"chronos.chronos_bolt","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.named_modules","Module":"chronos.chronos_bolt","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.named_parameters","Module":"chronos.chronos_bolt","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.parameters","Module":"chronos.chronos_bolt","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_backward_hook","Module":"chronos.chronos_bolt","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_buffer","Module":"chronos.chronos_bolt","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_forward_hook","Module":"chronos.chronos_bolt","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_forward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_full_backward_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_full_backward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_load_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_load_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_module","Module":"chronos.chronos_bolt","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_parameter","Module":"chronos.chronos_bolt","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.register_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.requires_grad_","Module":"chronos.chronos_bolt","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.set_extra_state","Module":"chronos.chronos_bolt","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.set_submodule","Module":"chronos.chronos_bolt","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.share_memory","Module":"chronos.chronos_bolt","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.state_dict","Module":"chronos.chronos_bolt","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.to","Module":"chronos.chronos_bolt","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.to_empty","Module":"chronos.chronos_bolt","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.train","Module":"chronos.chronos_bolt","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.type","Module":"chronos.chronos_bolt","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.xpu","Module":"chronos.chronos_bolt","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch.zero_grad","Module":"chronos.chronos_bolt","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Patch"},{"Path":"chronos.chronos_bolt.Patch","Module":"chronos.chronos_bolt","Name":"Patch","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos"},{"Path":"chronos.chronos_bolt.ResidualBlock.add_module","Module":"chronos.chronos_bolt","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.apply","Module":"chronos.chronos_bolt","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.bfloat16","Module":"chronos.chronos_bolt","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.buffers","Module":"chronos.chronos_bolt","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.children","Module":"chronos.chronos_bolt","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.compile","Module":"chronos.chronos_bolt","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.cpu","Module":"chronos.chronos_bolt","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.cuda","Module":"chronos.chronos_bolt","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.double","Module":"chronos.chronos_bolt","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.eval","Module":"chronos.chronos_bolt","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.extra_repr","Module":"chronos.chronos_bolt","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.float","Module":"chronos.chronos_bolt","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.forward","Module":"chronos.chronos_bolt","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor)","Arguments":"[\"self\", \"x\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.get_buffer","Module":"chronos.chronos_bolt","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.get_extra_state","Module":"chronos.chronos_bolt","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.get_parameter","Module":"chronos.chronos_bolt","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.get_submodule","Module":"chronos.chronos_bolt","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.half","Module":"chronos.chronos_bolt","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.ipu","Module":"chronos.chronos_bolt","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.load_state_dict","Module":"chronos.chronos_bolt","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.modules","Module":"chronos.chronos_bolt","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.mtia","Module":"chronos.chronos_bolt","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.named_buffers","Module":"chronos.chronos_bolt","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.named_children","Module":"chronos.chronos_bolt","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.named_modules","Module":"chronos.chronos_bolt","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.named_parameters","Module":"chronos.chronos_bolt","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.parameters","Module":"chronos.chronos_bolt","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_backward_hook","Module":"chronos.chronos_bolt","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_buffer","Module":"chronos.chronos_bolt","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_forward_hook","Module":"chronos.chronos_bolt","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_forward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_full_backward_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_full_backward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_load_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_load_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_module","Module":"chronos.chronos_bolt","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_parameter","Module":"chronos.chronos_bolt","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.register_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.requires_grad_","Module":"chronos.chronos_bolt","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.set_extra_state","Module":"chronos.chronos_bolt","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.set_submodule","Module":"chronos.chronos_bolt","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.share_memory","Module":"chronos.chronos_bolt","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.state_dict","Module":"chronos.chronos_bolt","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.to","Module":"chronos.chronos_bolt","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.to_empty","Module":"chronos.chronos_bolt","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.train","Module":"chronos.chronos_bolt","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.type","Module":"chronos.chronos_bolt","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.xpu","Module":"chronos.chronos_bolt","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock.zero_grad","Module":"chronos.chronos_bolt","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"ResidualBlock"},{"Path":"chronos.chronos_bolt.ResidualBlock","Module":"chronos.chronos_bolt","Name":"ResidualBlock","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos"},{"Path":"chronos.chronos_bolt.T5Config.dict_dtype_to_str","Module":"chronos.chronos_bolt","Name":"dict_dtype_to_str","Type":"method","Signature":"(self, d: dict[str, typing.Any]) -> None","Arguments":"[\"self\", \"d\"]","Docstring":"Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\nc","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.from_dict","Module":"chronos.chronos_bolt","Name":"from_dict","Type":"method","Signature":"(config_dict: dict[str, typing.Any], **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"config_dict\", \"kwargs\"]","Docstring":"Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n\nArgs:\n    config_dict (","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.from_json_file","Module":"chronos.chronos_bolt","Name":"from_json_file","Type":"method","Signature":"(json_file: Union[str, os.PathLike]) -> ~SpecificPretrainedConfigType","Arguments":"[\"json_file\"]","Docstring":"Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n\nArgs:\n    json_file","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.from_pretrained","Module":"chronos.chronos_bolt","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"pretrained_model_name_or_path\", \"cache_dir\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n\nArgs","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.from_text_audio_configs","Module":"chronos.chronos_bolt","Name":"from_text_audio_configs","Type":"method","Signature":"(text_config, audio_config, **kwargs)","Arguments":"[\"text_config\", \"audio_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and audio model\nconfig","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.from_text_vision_configs","Module":"chronos.chronos_bolt","Name":"from_text_vision_configs","Type":"method","Signature":"(text_config, vision_config, **kwargs)","Arguments":"[\"text_config\", \"vision_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and vision model\nconfi","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.get_config_dict","Module":"chronos.chronos_bolt","Name":"get_config_dict","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instan","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.get_text_config","Module":"chronos.chronos_bolt","Name":"get_text_config","Type":"method","Signature":"(self, decoder=None, encoder=None) -> 'PretrainedConfig'","Arguments":"[\"self\", \"decoder\", \"encoder\"]","Docstring":"Returns the text config related to the text input (encoder) or text output (decoder) of the model. T","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.push_to_hub","Module":"chronos.chronos_bolt","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the configuration file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.register_for_auto_class","Module":"chronos.chronos_bolt","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoConfig')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom configurations as t","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.save_pretrained","Module":"chronos.chronos_bolt","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"push_to_hub\", \"kwargs\"]","Docstring":"Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.to_dict","Module":"chronos.chronos_bolt","Name":"to_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Serializes this instance to a Python dictionary.\n\nReturns:\n    `dict[str, Any]`: Dictionary of all t","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.to_diff_dict","Module":"chronos.chronos_bolt","Name":"to_diff_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Removes all attributes from the configuration that correspond to the default config attributes for\nb","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.to_json_file","Module":"chronos.chronos_bolt","Name":"to_json_file","Type":"method","Signature":"(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)","Arguments":"[\"self\", \"json_file_path\", \"use_diff\"]","Docstring":"Save this instance to a JSON file.\n\nArgs:\n    json_file_path (`str` or `os.PathLike`):\n        Path ","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.to_json_string","Module":"chronos.chronos_bolt","Name":"to_json_string","Type":"method","Signature":"(self, use_diff: bool = True) -> str","Arguments":"[\"self\", \"use_diff\"]","Docstring":"Serializes this instance to a JSON string.\n\nArgs:\n    use_diff (`bool`, *optional*, defaults to `Tru","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.update","Module":"chronos.chronos_bolt","Name":"update","Type":"method","Signature":"(self, config_dict: dict[str, typing.Any])","Arguments":"[\"self\", \"config_dict\"]","Docstring":"Updates attributes of this class with attributes from `config_dict`.\n\nArgs:\n    config_dict (`dict[s","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config.update_from_string","Module":"chronos.chronos_bolt","Name":"update_from_string","Type":"method","Signature":"(self, update_str: str)","Arguments":"[\"self\", \"update_str\"]","Docstring":"Updates attributes of this class with attributes from `update_str`.\n\nThe expected format is ints, fl","Parent":"T5Config"},{"Path":"chronos.chronos_bolt.T5Config","Module":"chronos.chronos_bolt","Name":"T5Config","Type":"class","Signature":"","Arguments":"[]","Docstring":"This is the configuration class to store the configuration of a [`T5Model`] or a [`TFT5Model`]. It i","Parent":"chronos"},{"Path":"chronos.chronos_bolt.T5LayerNorm.add_module","Module":"chronos.chronos_bolt","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.apply","Module":"chronos.chronos_bolt","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.bfloat16","Module":"chronos.chronos_bolt","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.buffers","Module":"chronos.chronos_bolt","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.children","Module":"chronos.chronos_bolt","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.compile","Module":"chronos.chronos_bolt","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.cpu","Module":"chronos.chronos_bolt","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.cuda","Module":"chronos.chronos_bolt","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.double","Module":"chronos.chronos_bolt","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.eval","Module":"chronos.chronos_bolt","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.extra_repr","Module":"chronos.chronos_bolt","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.float","Module":"chronos.chronos_bolt","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.forward","Module":"chronos.chronos_bolt","Name":"forward","Type":"method","Signature":"(self, hidden_states)","Arguments":"[\"self\", \"hidden_states\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.get_buffer","Module":"chronos.chronos_bolt","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.get_extra_state","Module":"chronos.chronos_bolt","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.get_parameter","Module":"chronos.chronos_bolt","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.get_submodule","Module":"chronos.chronos_bolt","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.half","Module":"chronos.chronos_bolt","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.ipu","Module":"chronos.chronos_bolt","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.load_state_dict","Module":"chronos.chronos_bolt","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.modules","Module":"chronos.chronos_bolt","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.mtia","Module":"chronos.chronos_bolt","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.named_buffers","Module":"chronos.chronos_bolt","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.named_children","Module":"chronos.chronos_bolt","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.named_modules","Module":"chronos.chronos_bolt","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.named_parameters","Module":"chronos.chronos_bolt","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.parameters","Module":"chronos.chronos_bolt","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_backward_hook","Module":"chronos.chronos_bolt","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_buffer","Module":"chronos.chronos_bolt","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_forward_hook","Module":"chronos.chronos_bolt","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_forward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_full_backward_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_full_backward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_load_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_load_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_module","Module":"chronos.chronos_bolt","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_parameter","Module":"chronos.chronos_bolt","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.register_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.requires_grad_","Module":"chronos.chronos_bolt","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.set_extra_state","Module":"chronos.chronos_bolt","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.set_submodule","Module":"chronos.chronos_bolt","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.share_memory","Module":"chronos.chronos_bolt","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.state_dict","Module":"chronos.chronos_bolt","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.to","Module":"chronos.chronos_bolt","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.to_empty","Module":"chronos.chronos_bolt","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.train","Module":"chronos.chronos_bolt","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.type","Module":"chronos.chronos_bolt","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.xpu","Module":"chronos.chronos_bolt","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm.zero_grad","Module":"chronos.chronos_bolt","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"T5LayerNorm"},{"Path":"chronos.chronos_bolt.T5LayerNorm","Module":"chronos.chronos_bolt","Name":"T5LayerNorm","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.active_adapters","Module":"chronos.chronos_bolt","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.add_adapter","Module":"chronos.chronos_bolt","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.add_memory_hooks","Module":"chronos.chronos_bolt","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.add_model_tags","Module":"chronos.chronos_bolt","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.add_module","Module":"chronos.chronos_bolt","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.apply","Module":"chronos.chronos_bolt","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.bfloat16","Module":"chronos.chronos_bolt","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.buffers","Module":"chronos.chronos_bolt","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.can_generate","Module":"chronos.chronos_bolt","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.children","Module":"chronos.chronos_bolt","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.compile","Module":"chronos.chronos_bolt","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.cpu","Module":"chronos.chronos_bolt","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.create_extended_attention_mask_for_decoder","Module":"chronos.chronos_bolt","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.cuda","Module":"chronos.chronos_bolt","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.delete_adapter","Module":"chronos.chronos_bolt","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.dequantize","Module":"chronos.chronos_bolt","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.disable_adapters","Module":"chronos.chronos_bolt","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.disable_input_require_grads","Module":"chronos.chronos_bolt","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.double","Module":"chronos.chronos_bolt","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.enable_adapters","Module":"chronos.chronos_bolt","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.enable_input_require_grads","Module":"chronos.chronos_bolt","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.estimate_tokens","Module":"chronos.chronos_bolt","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.eval","Module":"chronos.chronos_bolt","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.extra_repr","Module":"chronos.chronos_bolt","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.float","Module":"chronos.chronos_bolt","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.floating_point_ops","Module":"chronos.chronos_bolt","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.forward","Module":"chronos.chronos_bolt","Name":"forward","Type":"method","Signature":"(self, *input: Any) -> None","Arguments":"[\"self\", \"input\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.from_pretrained","Module":"chronos.chronos_bolt","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_adapter_state_dict","Module":"chronos.chronos_bolt","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_buffer","Module":"chronos.chronos_bolt","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_compiled_call","Module":"chronos.chronos_bolt","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_correct_attn_implementation","Module":"chronos.chronos_bolt","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_decoder","Module":"chronos.chronos_bolt","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_extended_attention_mask","Module":"chronos.chronos_bolt","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_extra_state","Module":"chronos.chronos_bolt","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_head_mask","Module":"chronos.chronos_bolt","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_init_context","Module":"chronos.chronos_bolt","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_input_embeddings","Module":"chronos.chronos_bolt","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_memory_footprint","Module":"chronos.chronos_bolt","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_output_embeddings","Module":"chronos.chronos_bolt","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_parameter","Module":"chronos.chronos_bolt","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_parameter_or_buffer","Module":"chronos.chronos_bolt","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_position_embeddings","Module":"chronos.chronos_bolt","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.get_submodule","Module":"chronos.chronos_bolt","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.gradient_checkpointing_disable","Module":"chronos.chronos_bolt","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.gradient_checkpointing_enable","Module":"chronos.chronos_bolt","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.half","Module":"chronos.chronos_bolt","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.init_weights","Module":"chronos.chronos_bolt","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.initialize_weights","Module":"chronos.chronos_bolt","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.invert_attention_mask","Module":"chronos.chronos_bolt","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.ipu","Module":"chronos.chronos_bolt","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.is_backend_compatible","Module":"chronos.chronos_bolt","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.kernelize","Module":"chronos.chronos_bolt","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.load_adapter","Module":"chronos.chronos_bolt","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.load_state_dict","Module":"chronos.chronos_bolt","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.load_tf_weights","Module":"chronos.chronos_bolt","Name":"load_tf_weights","Type":"method","Signature":"(model, config, tf_checkpoint_path)","Arguments":"[\"model\", \"config\", \"tf_checkpoint_path\"]","Docstring":"Load tf checkpoints in a pytorch model.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.modules","Module":"chronos.chronos_bolt","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.mtia","Module":"chronos.chronos_bolt","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.named_buffers","Module":"chronos.chronos_bolt","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.named_children","Module":"chronos.chronos_bolt","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.named_modules","Module":"chronos.chronos_bolt","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.named_parameters","Module":"chronos.chronos_bolt","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.num_parameters","Module":"chronos.chronos_bolt","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.parameters","Module":"chronos.chronos_bolt","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.post_init","Module":"chronos.chronos_bolt","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.prune_heads","Module":"chronos.chronos_bolt","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.push_to_hub","Module":"chronos.chronos_bolt","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_backward_hook","Module":"chronos.chronos_bolt","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_buffer","Module":"chronos.chronos_bolt","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_for_auto_class","Module":"chronos.chronos_bolt","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_forward_hook","Module":"chronos.chronos_bolt","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_forward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_full_backward_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_full_backward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_load_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_load_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_module","Module":"chronos.chronos_bolt","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_parameter","Module":"chronos.chronos_bolt","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.register_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.requires_grad_","Module":"chronos.chronos_bolt","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.reset_memory_hooks_state","Module":"chronos.chronos_bolt","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.resize_position_embeddings","Module":"chronos.chronos_bolt","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.resize_token_embeddings","Module":"chronos.chronos_bolt","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.retrieve_modules_from_names","Module":"chronos.chronos_bolt","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.reverse_bettertransformer","Module":"chronos.chronos_bolt","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.save_pretrained","Module":"chronos.chronos_bolt","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.set_adapter","Module":"chronos.chronos_bolt","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.set_attn_implementation","Module":"chronos.chronos_bolt","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.set_decoder","Module":"chronos.chronos_bolt","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.set_extra_state","Module":"chronos.chronos_bolt","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.set_input_embeddings","Module":"chronos.chronos_bolt","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.set_output_embeddings","Module":"chronos.chronos_bolt","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.set_submodule","Module":"chronos.chronos_bolt","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.share_memory","Module":"chronos.chronos_bolt","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.state_dict","Module":"chronos.chronos_bolt","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos_bolt","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.tie_weights","Module":"chronos.chronos_bolt","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.to","Module":"chronos.chronos_bolt","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.to_bettertransformer","Module":"chronos.chronos_bolt","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.to_empty","Module":"chronos.chronos_bolt","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.train","Module":"chronos.chronos_bolt","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.type","Module":"chronos.chronos_bolt","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos_bolt","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.xpu","Module":"chronos.chronos_bolt","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel.zero_grad","Module":"chronos.chronos_bolt","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"T5PreTrainedModel"},{"Path":"chronos.chronos_bolt.T5PreTrainedModel","Module":"chronos.chronos_bolt","Name":"T5PreTrainedModel","Type":"class","Signature":"","Arguments":"[]","Docstring":"This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic met","Parent":"chronos"},{"Path":"chronos.chronos_bolt.T5Stack.active_adapters","Module":"chronos.chronos_bolt","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.add_adapter","Module":"chronos.chronos_bolt","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.add_memory_hooks","Module":"chronos.chronos_bolt","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.add_model_tags","Module":"chronos.chronos_bolt","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.add_module","Module":"chronos.chronos_bolt","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.apply","Module":"chronos.chronos_bolt","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.bfloat16","Module":"chronos.chronos_bolt","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.buffers","Module":"chronos.chronos_bolt","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.can_generate","Module":"chronos.chronos_bolt","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.children","Module":"chronos.chronos_bolt","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.compile","Module":"chronos.chronos_bolt","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.cpu","Module":"chronos.chronos_bolt","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.create_extended_attention_mask_for_decoder","Module":"chronos.chronos_bolt","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.cuda","Module":"chronos.chronos_bolt","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.delete_adapter","Module":"chronos.chronos_bolt","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.deparallelize","Module":"chronos.chronos_bolt","Name":"deparallelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Moves the model to cpu from a model parallel state.\n\nExample:\n\n```python\n# On a 4 GPU machine with g","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.dequantize","Module":"chronos.chronos_bolt","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.disable_adapters","Module":"chronos.chronos_bolt","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.disable_input_require_grads","Module":"chronos.chronos_bolt","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.double","Module":"chronos.chronos_bolt","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.enable_adapters","Module":"chronos.chronos_bolt","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.enable_input_require_grads","Module":"chronos.chronos_bolt","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.estimate_tokens","Module":"chronos.chronos_bolt","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.eval","Module":"chronos.chronos_bolt","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.extra_repr","Module":"chronos.chronos_bolt","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.float","Module":"chronos.chronos_bolt","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.floating_point_ops","Module":"chronos.chronos_bolt","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.forward","Module":"chronos.chronos_bolt","Name":"forward","Type":"method","Signature":"(self, input_ids=None, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, inputs_embeds=None, head_mask=None, cross_attn_head_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None, cache_position=None)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\", \"encoder_hidden_states\", \"encoder_attention_mask\", \"inputs_embeds\", \"head_mask\", \"cross_attn_head_mask\", \"past_key_values\", \"use_cache\", \"output_attentions\", \"output_hidden_states\", \"return_dict\", \"cache_position\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.from_pretrained","Module":"chronos.chronos_bolt","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_adapter_state_dict","Module":"chronos.chronos_bolt","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_buffer","Module":"chronos.chronos_bolt","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_compiled_call","Module":"chronos.chronos_bolt","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_correct_attn_implementation","Module":"chronos.chronos_bolt","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_decoder","Module":"chronos.chronos_bolt","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_extended_attention_mask","Module":"chronos.chronos_bolt","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_extra_state","Module":"chronos.chronos_bolt","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_head_mask","Module":"chronos.chronos_bolt","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_init_context","Module":"chronos.chronos_bolt","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_input_embeddings","Module":"chronos.chronos_bolt","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_memory_footprint","Module":"chronos.chronos_bolt","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_output_embeddings","Module":"chronos.chronos_bolt","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_parameter","Module":"chronos.chronos_bolt","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_parameter_or_buffer","Module":"chronos.chronos_bolt","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_position_embeddings","Module":"chronos.chronos_bolt","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.get_submodule","Module":"chronos.chronos_bolt","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.gradient_checkpointing_disable","Module":"chronos.chronos_bolt","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.gradient_checkpointing_enable","Module":"chronos.chronos_bolt","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.half","Module":"chronos.chronos_bolt","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.init_weights","Module":"chronos.chronos_bolt","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.initialize_weights","Module":"chronos.chronos_bolt","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.invert_attention_mask","Module":"chronos.chronos_bolt","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.ipu","Module":"chronos.chronos_bolt","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.is_backend_compatible","Module":"chronos.chronos_bolt","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.kernelize","Module":"chronos.chronos_bolt","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.load_adapter","Module":"chronos.chronos_bolt","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.load_state_dict","Module":"chronos.chronos_bolt","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.load_tf_weights","Module":"chronos.chronos_bolt","Name":"load_tf_weights","Type":"method","Signature":"(model, config, tf_checkpoint_path)","Arguments":"[\"model\", \"config\", \"tf_checkpoint_path\"]","Docstring":"Load tf checkpoints in a pytorch model.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.modules","Module":"chronos.chronos_bolt","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.mtia","Module":"chronos.chronos_bolt","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.named_buffers","Module":"chronos.chronos_bolt","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.named_children","Module":"chronos.chronos_bolt","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.named_modules","Module":"chronos.chronos_bolt","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.named_parameters","Module":"chronos.chronos_bolt","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.num_parameters","Module":"chronos.chronos_bolt","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.parallelize","Module":"chronos.chronos_bolt","Name":"parallelize","Type":"method","Signature":"(self, device_map=None)","Arguments":"[\"self\", \"device_map\"]","Docstring":"This is an experimental feature and is a subject to change at a moment's notice.\n\nUses a device map ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.parameters","Module":"chronos.chronos_bolt","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.post_init","Module":"chronos.chronos_bolt","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.prune_heads","Module":"chronos.chronos_bolt","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.push_to_hub","Module":"chronos.chronos_bolt","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_backward_hook","Module":"chronos.chronos_bolt","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_buffer","Module":"chronos.chronos_bolt","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_for_auto_class","Module":"chronos.chronos_bolt","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_forward_hook","Module":"chronos.chronos_bolt","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_forward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_full_backward_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_full_backward_pre_hook","Module":"chronos.chronos_bolt","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_load_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_load_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_module","Module":"chronos.chronos_bolt","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_parameter","Module":"chronos.chronos_bolt","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_state_dict_post_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.register_state_dict_pre_hook","Module":"chronos.chronos_bolt","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.requires_grad_","Module":"chronos.chronos_bolt","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.reset_memory_hooks_state","Module":"chronos.chronos_bolt","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.resize_position_embeddings","Module":"chronos.chronos_bolt","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.resize_token_embeddings","Module":"chronos.chronos_bolt","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.retrieve_modules_from_names","Module":"chronos.chronos_bolt","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.reverse_bettertransformer","Module":"chronos.chronos_bolt","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.save_pretrained","Module":"chronos.chronos_bolt","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.set_adapter","Module":"chronos.chronos_bolt","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.set_attn_implementation","Module":"chronos.chronos_bolt","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.set_decoder","Module":"chronos.chronos_bolt","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.set_extra_state","Module":"chronos.chronos_bolt","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.set_input_embeddings","Module":"chronos.chronos_bolt","Name":"set_input_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.set_output_embeddings","Module":"chronos.chronos_bolt","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.set_submodule","Module":"chronos.chronos_bolt","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.share_memory","Module":"chronos.chronos_bolt","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.state_dict","Module":"chronos.chronos_bolt","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos_bolt","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.tie_weights","Module":"chronos.chronos_bolt","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.to","Module":"chronos.chronos_bolt","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.to_bettertransformer","Module":"chronos.chronos_bolt","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.to_empty","Module":"chronos.chronos_bolt","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.train","Module":"chronos.chronos_bolt","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.type","Module":"chronos.chronos_bolt","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos_bolt","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.xpu","Module":"chronos.chronos_bolt","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack.zero_grad","Module":"chronos.chronos_bolt","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"T5Stack"},{"Path":"chronos.chronos_bolt.T5Stack","Module":"chronos.chronos_bolt","Name":"T5Stack","Type":"class","Signature":"","Arguments":"[]","Docstring":"This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic met","Parent":"chronos"},{"Path":"chronos.chronos_bolt.Tuple","Module":"chronos.chronos_bolt","Name":"Tuple","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Deprecated alias to builtins.tuple.\n\nTuple[X, Y] is the cross-product type of X and Y.\n\nExample: Tup","Parent":"chronos"},{"Path":"chronos.chronos_bolt.Union","Module":"chronos.chronos_bolt","Name":"Union","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Union type; Union[X, Y] means either X or Y.\n\nOn Python 3.10 and higher, the | operator\ncan also be ","Parent":"chronos"},{"Path":"chronos.chronos_bolt.dataclass","Module":"chronos.chronos_bolt","Name":"dataclass","Type":"function","Signature":"(cls=None, \/, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)","Arguments":"[\"cls\", \"init\", \"repr\", \"eq\", \"order\", \"unsafe_hash\", \"frozen\", \"match_args\", \"kw_only\", \"slots\", \"weakref_slot\"]","Docstring":"Add dunder methods based on the fields defined in the class.\n\nExamines PEP 526 __annotations__ to de","Parent":"chronos"},{"Path":"chronos.chronos_bolt.logger","Module":"chronos.chronos_bolt","Name":"logger","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Instances of the Logger class represent a single logging channel. A\n\"logging channel\" indicates an a","Parent":"chronos"},{"Path":"chronos.chronos2.Chronos2CoreConfig.dict_dtype_to_str","Module":"chronos.chronos2","Name":"dict_dtype_to_str","Type":"method","Signature":"(self, d: dict[str, typing.Any]) -> None","Arguments":"[\"self\", \"d\"]","Docstring":"Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\nc","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.from_dict","Module":"chronos.chronos2","Name":"from_dict","Type":"method","Signature":"(config_dict: dict[str, typing.Any], **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"config_dict\", \"kwargs\"]","Docstring":"Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n\nArgs:\n    config_dict (","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.from_json_file","Module":"chronos.chronos2","Name":"from_json_file","Type":"method","Signature":"(json_file: Union[str, os.PathLike]) -> ~SpecificPretrainedConfigType","Arguments":"[\"json_file\"]","Docstring":"Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n\nArgs:\n    json_file","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.from_pretrained","Module":"chronos.chronos2","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"pretrained_model_name_or_path\", \"cache_dir\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n\nArgs","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.from_text_audio_configs","Module":"chronos.chronos2","Name":"from_text_audio_configs","Type":"method","Signature":"(text_config, audio_config, **kwargs)","Arguments":"[\"text_config\", \"audio_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and audio model\nconfig","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.from_text_vision_configs","Module":"chronos.chronos2","Name":"from_text_vision_configs","Type":"method","Signature":"(text_config, vision_config, **kwargs)","Arguments":"[\"text_config\", \"vision_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and vision model\nconfi","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.get_config_dict","Module":"chronos.chronos2","Name":"get_config_dict","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instan","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.get_text_config","Module":"chronos.chronos2","Name":"get_text_config","Type":"method","Signature":"(self, decoder=None, encoder=None) -> 'PretrainedConfig'","Arguments":"[\"self\", \"decoder\", \"encoder\"]","Docstring":"Returns the text config related to the text input (encoder) or text output (decoder) of the model. T","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.push_to_hub","Module":"chronos.chronos2","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the configuration file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.register_for_auto_class","Module":"chronos.chronos2","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoConfig')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom configurations as t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.save_pretrained","Module":"chronos.chronos2","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"push_to_hub\", \"kwargs\"]","Docstring":"Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.to_dict","Module":"chronos.chronos2","Name":"to_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Serializes this instance to a Python dictionary.\n\nReturns:\n    `dict[str, Any]`: Dictionary of all t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.to_diff_dict","Module":"chronos.chronos2","Name":"to_diff_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Removes all attributes from the configuration that correspond to the default config attributes for\nb","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.to_json_file","Module":"chronos.chronos2","Name":"to_json_file","Type":"method","Signature":"(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)","Arguments":"[\"self\", \"json_file_path\", \"use_diff\"]","Docstring":"Save this instance to a JSON file.\n\nArgs:\n    json_file_path (`str` or `os.PathLike`):\n        Path ","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.to_json_string","Module":"chronos.chronos2","Name":"to_json_string","Type":"method","Signature":"(self, use_diff: bool = True) -> str","Arguments":"[\"self\", \"use_diff\"]","Docstring":"Serializes this instance to a JSON string.\n\nArgs:\n    use_diff (`bool`, *optional*, defaults to `Tru","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.update","Module":"chronos.chronos2","Name":"update","Type":"method","Signature":"(self, config_dict: dict[str, typing.Any])","Arguments":"[\"self\", \"config_dict\"]","Docstring":"Updates attributes of this class with attributes from `config_dict`.\n\nArgs:\n    config_dict (`dict[s","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig.update_from_string","Module":"chronos.chronos2","Name":"update_from_string","Type":"method","Signature":"(self, update_str: str)","Arguments":"[\"self\", \"update_str\"]","Docstring":"Updates attributes of this class with attributes from `update_str`.\n\nThe expected format is ints, fl","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.Chronos2CoreConfig","Module":"chronos.chronos2","Name":"Chronos2CoreConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.\n\nArguments\n-------","Parent":"chronos"},{"Path":"chronos.chronos2.Chronos2Dataset.convert_inputs","Module":"chronos.chronos2","Name":"convert_inputs","Type":"method","Signature":"(inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]]], context_length: int, prediction_length: int, batch_size: int, output_patch_size: int, min_past: int = 1, mode: str | chronos.chronos2.dataset.DatasetMode = <DatasetMode.TRAIN: 'train'>) -> 'Chronos2Dataset'","Arguments":"[\"inputs\", \"context_length\", \"prediction_length\", \"batch_size\", \"output_patch_size\", \"min_past\", \"mode\"]","Docstring":"Convert from different input formats to a Chronos2Dataset.","Parent":"Chronos2Dataset"},{"Path":"chronos.chronos2.Chronos2Dataset","Module":"chronos.chronos2","Name":"Chronos2Dataset","Type":"class","Signature":"","Arguments":"[]","Docstring":"A dataset wrapper for Chronos-2 models.\n\nArguments\n----------\ninputs\n    Time series data. Must be a","Parent":"chronos"},{"Path":"chronos.chronos2.Chronos2ForecastingConfig.editable_fields","Module":"chronos.chronos2","Name":"editable_fields","Type":"method","Signature":"() -> list[str]","Arguments":"[]","Docstring":"Fields that maybe modified during the fine-tuning stage.","Parent":"Chronos2ForecastingConfig"},{"Path":"chronos.chronos2.Chronos2ForecastingConfig","Module":"chronos.chronos2","Name":"Chronos2ForecastingConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, input_","Parent":"chronos"},{"Path":"chronos.chronos2.Chronos2Model.active_adapters","Module":"chronos.chronos2","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.add_adapter","Module":"chronos.chronos2","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.add_memory_hooks","Module":"chronos.chronos2","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.add_model_tags","Module":"chronos.chronos2","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.add_module","Module":"chronos.chronos2","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.apply","Module":"chronos.chronos2","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.bfloat16","Module":"chronos.chronos2","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.buffers","Module":"chronos.chronos2","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.can_generate","Module":"chronos.chronos2","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.children","Module":"chronos.chronos2","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.compile","Module":"chronos.chronos2","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.cpu","Module":"chronos.chronos2","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.create_extended_attention_mask_for_decoder","Module":"chronos.chronos2","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.cuda","Module":"chronos.chronos2","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.delete_adapter","Module":"chronos.chronos2","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.dequantize","Module":"chronos.chronos2","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.disable_adapters","Module":"chronos.chronos2","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.disable_input_require_grads","Module":"chronos.chronos2","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.double","Module":"chronos.chronos2","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.enable_adapters","Module":"chronos.chronos2","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.enable_input_require_grads","Module":"chronos.chronos2","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.encode","Module":"chronos.chronos2","Name":"encode","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False)","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.estimate_tokens","Module":"chronos.chronos2","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.eval","Module":"chronos.chronos2","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.extra_repr","Module":"chronos.chronos2","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.float","Module":"chronos.chronos2","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.floating_point_ops","Module":"chronos.chronos2","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.forward","Module":"chronos.chronos2","Name":"forward","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False) -> chronos.chronos2.model.Chronos2Output","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"Forward pass of the Chronos2 model.\n\nParameters\n----------\ncontext\n    Input tensor of shape (batch_","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.from_pretrained","Module":"chronos.chronos2","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_adapter_state_dict","Module":"chronos.chronos2","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_buffer","Module":"chronos.chronos2","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_compiled_call","Module":"chronos.chronos2","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_correct_attn_implementation","Module":"chronos.chronos2","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_decoder","Module":"chronos.chronos2","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_extended_attention_mask","Module":"chronos.chronos2","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_extra_state","Module":"chronos.chronos2","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_head_mask","Module":"chronos.chronos2","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_init_context","Module":"chronos.chronos2","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_input_embeddings","Module":"chronos.chronos2","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_memory_footprint","Module":"chronos.chronos2","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_output_embeddings","Module":"chronos.chronos2","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_parameter","Module":"chronos.chronos2","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_parameter_or_buffer","Module":"chronos.chronos2","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_position_embeddings","Module":"chronos.chronos2","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.get_submodule","Module":"chronos.chronos2","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.gradient_checkpointing_disable","Module":"chronos.chronos2","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.gradient_checkpointing_enable","Module":"chronos.chronos2","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.half","Module":"chronos.chronos2","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.init_weights","Module":"chronos.chronos2","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.initialize_weights","Module":"chronos.chronos2","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.invert_attention_mask","Module":"chronos.chronos2","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.ipu","Module":"chronos.chronos2","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.is_backend_compatible","Module":"chronos.chronos2","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.kernelize","Module":"chronos.chronos2","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.load_adapter","Module":"chronos.chronos2","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.load_state_dict","Module":"chronos.chronos2","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.modules","Module":"chronos.chronos2","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.mtia","Module":"chronos.chronos2","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.named_buffers","Module":"chronos.chronos2","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.named_children","Module":"chronos.chronos2","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.named_modules","Module":"chronos.chronos2","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.named_parameters","Module":"chronos.chronos2","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.num_parameters","Module":"chronos.chronos2","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.parameters","Module":"chronos.chronos2","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.post_init","Module":"chronos.chronos2","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.prune_heads","Module":"chronos.chronos2","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.push_to_hub","Module":"chronos.chronos2","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_backward_hook","Module":"chronos.chronos2","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_buffer","Module":"chronos.chronos2","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_for_auto_class","Module":"chronos.chronos2","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_forward_hook","Module":"chronos.chronos2","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_forward_pre_hook","Module":"chronos.chronos2","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_full_backward_hook","Module":"chronos.chronos2","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_full_backward_pre_hook","Module":"chronos.chronos2","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_load_state_dict_post_hook","Module":"chronos.chronos2","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_load_state_dict_pre_hook","Module":"chronos.chronos2","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_module","Module":"chronos.chronos2","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_parameter","Module":"chronos.chronos2","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_state_dict_post_hook","Module":"chronos.chronos2","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.register_state_dict_pre_hook","Module":"chronos.chronos2","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.requires_grad_","Module":"chronos.chronos2","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.reset_memory_hooks_state","Module":"chronos.chronos2","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.resize_position_embeddings","Module":"chronos.chronos2","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.resize_token_embeddings","Module":"chronos.chronos2","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.retrieve_modules_from_names","Module":"chronos.chronos2","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.reverse_bettertransformer","Module":"chronos.chronos2","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.save_pretrained","Module":"chronos.chronos2","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.set_adapter","Module":"chronos.chronos2","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.set_attn_implementation","Module":"chronos.chronos2","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.set_decoder","Module":"chronos.chronos2","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.set_extra_state","Module":"chronos.chronos2","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.set_input_embeddings","Module":"chronos.chronos2","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.set_output_embeddings","Module":"chronos.chronos2","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.set_submodule","Module":"chronos.chronos2","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.share_memory","Module":"chronos.chronos2","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.state_dict","Module":"chronos.chronos2","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos2","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.tie_weights","Module":"chronos.chronos2","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.to","Module":"chronos.chronos2","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.to_bettertransformer","Module":"chronos.chronos2","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.to_empty","Module":"chronos.chronos2","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.train","Module":"chronos.chronos2","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.type","Module":"chronos.chronos2","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos2","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.xpu","Module":"chronos.chronos2","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model.zero_grad","Module":"chronos.chronos2","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.Chronos2Model","Module":"chronos.chronos2","Name":"Chronos2Model","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all models.\n\n[`PreTrainedModel`] takes care of storing the configuration of the model","Parent":"chronos"},{"Path":"chronos.chronos2.Chronos2Pipeline.embed","Module":"chronos.chronos2","Name":"embed","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray]], batch_size: int = 256, context_length: int | None = None) -> tuple[list[torch.Tensor], list[tuple[torch.Tensor, torch.Tensor]]]","Arguments":"[\"self\", \"inputs\", \"batch_size\", \"context_length\"]","Docstring":"Get encoder embeddings for the given time series.\n\nParameters\n----------\ninputs\n    The time series ","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline.fit","Module":"chronos.chronos2","Name":"fit","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]]], prediction_length: int, validation_inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]], NoneType] = None, finetune_mode: Literal['full', 'lora'] = 'full', lora_config: 'LoraConfig | dict | None' = None, context_length: int | None = None, learning_rate: float = 1e-06, num_steps: int = 1000, batch_size: int = 256, output_dir: pathlib.Path | str | None = None, min_past: int | None = None, finetuned_ckpt_name: str = 'finetuned-ckpt', callbacks: list['TrainerCallback'] | None = None, remove_printer_callback: bool = False, disable_data_parallel: bool = True, **extra_trainer_kwargs) -> 'Chronos2Pipeline'","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"validation_inputs\", \"finetune_mode\", \"lora_config\", \"context_length\", \"learning_rate\", \"num_steps\", \"batch_size\", \"output_dir\", \"min_past\", \"finetuned_ckpt_name\", \"callbacks\", \"remove_printer_callback\", \"disable_data_parallel\", \"extra_trainer_kwargs\"]","Docstring":"Fine-tune a copy of the current Chronos-2 model on the given inputs and return a new pipeline.\n\nPara","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline.from_pretrained","Module":"chronos.chronos2","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path, *args, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"args\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix or from the HuggingFace Hub.\nSupports the same a","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline.predict","Module":"chronos.chronos2","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]]]], prediction_length: int | None = None, batch_size: int = 256, context_length: int | None = None, cross_learning: bool = False, limit_prediction_length: bool = False, **kwargs) -> list[torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"batch_size\", \"context_length\", \"cross_learning\", \"limit_prediction_length\", \"kwargs\"]","Docstring":"Generate forecasts for the given time series.\n\nParameters\n----------\ninputs\n    The time series to g","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline.predict_df","Module":"chronos.chronos2","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', future_df: 'pd.DataFrame | None' = None, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str | list[str] = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], batch_size: int = 256, context_length: int | None = None, cross_learning: bool = False, validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"future_df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"batch_size\", \"context_length\", \"cross_learning\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline.predict_fev","Module":"chronos.chronos2","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 256, as_univariate: bool = False, finetune_kwargs: dict | None = None, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"as_univariate\", \"finetune_kwargs\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline.predict_quantiles","Module":"chronos.chronos2","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]]]], prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **predict_kwargs) -> tuple[list[torch.Tensor], list[torch.Tensor]]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"predict_kwargs\"]","Docstring":"Refer to ``Chronos2Pipeline.predict`` for shared parameters.\n\nAdditional parameters\n----------------","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline.save_pretrained","Module":"chronos.chronos2","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: str | pathlib.Path, *args, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"args\", \"kwargs\"]","Docstring":"Save the underlying model to a local directory or to HuggingFace Hub.","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.Chronos2Pipeline","Module":"chronos.chronos2","Name":"Chronos2Pipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.add_callback","Module":"chronos.chronos2.trainer","Name":"add_callback","Type":"method","Signature":"(self, callback)","Arguments":"[\"self\", \"callback\"]","Docstring":"Add a callback to the current list of [`~transformers.TrainerCallback`].\n\nArgs:\n   callback (`type` ","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.autocast_smart_context_manager","Module":"chronos.chronos2.trainer","Name":"autocast_smart_context_manager","Type":"method","Signature":"(self, cache_enabled: Optional[bool] = True)","Arguments":"[\"self\", \"cache_enabled\"]","Docstring":"A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the des","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.call_model_init","Module":"chronos.chronos2.trainer","Name":"call_model_init","Type":"method","Signature":"(self, trial=None)","Arguments":"[\"self\", \"trial\"]","Docstring":"","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.compare_trainer_and_checkpoint_args","Module":"chronos.chronos2.trainer","Name":"compare_trainer_and_checkpoint_args","Type":"method","Signature":"(self, training_args, trainer_state)","Arguments":"[\"self\", \"training_args\", \"trainer_state\"]","Docstring":"","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.compute_loss","Module":"chronos.chronos2.trainer","Name":"compute_loss","Type":"method","Signature":"(self, model: torch.nn.modules.module.Module, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]], return_outputs: bool = False, num_items_in_batch: Optional[torch.Tensor] = None)","Arguments":"[\"self\", \"model\", \"inputs\", \"return_outputs\", \"num_items_in_batch\"]","Docstring":"How the loss is computed by Trainer. By default, all models return the loss in the first element.\n\nA","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.compute_loss_context_manager","Module":"chronos.chronos2.trainer","Name":"compute_loss_context_manager","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A helper wrapper to group together context managers.","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.create_accelerator_and_postprocess","Module":"chronos.chronos2.trainer","Name":"create_accelerator_and_postprocess","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.create_model_card","Module":"chronos.chronos2.trainer","Name":"create_model_card","Type":"method","Signature":"(self, language: Optional[str] = None, license: Optional[str] = None, tags: Union[str, list[str], NoneType] = None, model_name: Optional[str] = None, finetuned_from: Optional[str] = None, tasks: Union[str, list[str], NoneType] = None, dataset_tags: Union[str, list[str], NoneType] = None, dataset: Union[str, list[str], NoneType] = None, dataset_args: Union[str, list[str], NoneType] = None)","Arguments":"[\"self\", \"language\", \"license\", \"tags\", \"model_name\", \"finetuned_from\", \"tasks\", \"dataset_tags\", \"dataset\", \"dataset_args\"]","Docstring":"Creates a draft of a model card using the information available to the `Trainer`.\n\nArgs:\n    languag","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.create_optimizer","Module":"chronos.chronos2.trainer","Name":"create_optimizer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Setup the optimizer.\n\nWe provide a reasonable default that works well. If you want to use something ","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.create_optimizer_and_scheduler","Module":"chronos.chronos2.trainer","Name":"create_optimizer_and_scheduler","Type":"method","Signature":"(self, num_training_steps: int)","Arguments":"[\"self\", \"num_training_steps\"]","Docstring":"Setup the optimizer and the learning rate scheduler.\n\nWe provide a reasonable default that works wel","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.create_scheduler","Module":"chronos.chronos2.trainer","Name":"create_scheduler","Type":"method","Signature":"(self, num_training_steps: int, optimizer: torch.optim.optimizer.Optimizer = None)","Arguments":"[\"self\", \"num_training_steps\", \"optimizer\"]","Docstring":"Setup the scheduler. The optimizer of the trainer must have been set up either before this method is","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.evaluate","Module":"chronos.chronos2.trainer","Name":"evaluate","Type":"method","Signature":"(self, eval_dataset: Union[torch.utils.data.dataset.Dataset, dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'eval') -> dict[str, float]","Arguments":"[\"self\", \"eval_dataset\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Run evaluation and returns metrics.\n\nThe calling script will be responsible for providing a method t","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.evaluation_loop","Module":"chronos.chronos2.trainer","Name":"evaluation_loop","Type":"method","Signature":"(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput","Arguments":"[\"self\", \"dataloader\", \"description\", \"prediction_loss_only\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Prediction\/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\nWorks both with","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.floating_point_ops","Module":"chronos.chronos2.trainer","Name":"floating_point_ops","Type":"method","Signature":"(self, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]])","Arguments":"[\"self\", \"inputs\"]","Docstring":"For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_batch_samples","Module":"chronos.chronos2.trainer","Name":"get_batch_samples","Type":"method","Signature":"(self, epoch_iterator: collections.abc.Iterator, num_batches: int, device: torch.device) -> tuple[list, typing.Union[torch.Tensor, int, NoneType]]","Arguments":"[\"self\", \"epoch_iterator\", \"num_batches\", \"device\"]","Docstring":"Collects a specified number of batches from the epoch iterator and optionally counts the number of i","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_decay_parameter_names","Module":"chronos.chronos2.trainer","Name":"get_decay_parameter_names","Type":"method","Signature":"(self, model) -> list[str]","Arguments":"[\"self\", \"model\"]","Docstring":"Get all parameter names that weight decay will be applied to.\n\nThis function filters out parameters ","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_eval_dataloader","Module":"chronos.chronos2.trainer","Name":"get_eval_dataloader","Type":"method","Signature":"(self, eval_dataset: str | torch.utils.data.dataset.Dataset | None = None) -> torch.utils.data.dataloader.DataLoader","Arguments":"[\"self\", \"eval_dataset\"]","Docstring":"Returns the evaluation [`~torch.utils.data.DataLoader`].\n\nSubclass and override this method if you w","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_learning_rates","Module":"chronos.chronos2.trainer","Name":"get_learning_rates","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Returns the learning rate of each parameter from self.optimizer.","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_num_trainable_parameters","Module":"chronos.chronos2.trainer","Name":"get_num_trainable_parameters","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Get the number of trainable parameters.","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_optimizer_cls_and_kwargs","Module":"chronos.chronos2.trainer","Name":"get_optimizer_cls_and_kwargs","Type":"method","Signature":"(args: transformers.training_args.TrainingArguments, model: Optional[transformers.modeling_utils.PreTrainedModel] = None) -> tuple[typing.Any, typing.Any]","Arguments":"[\"args\", \"model\"]","Docstring":"Returns the optimizer class and optimizer parameters based on the training arguments.\n\nArgs:\n    arg","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_optimizer_group","Module":"chronos.chronos2.trainer","Name":"get_optimizer_group","Type":"method","Signature":"(self, param: Union[str, torch.nn.parameter.Parameter, NoneType] = None)","Arguments":"[\"self\", \"param\"]","Docstring":"Returns optimizer group for a parameter if given, else returns all optimizer groups for params.\n\nArg","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_test_dataloader","Module":"chronos.chronos2.trainer","Name":"get_test_dataloader","Type":"method","Signature":"(self, test_dataset: torch.utils.data.dataset.Dataset) -> torch.utils.data.dataloader.DataLoader","Arguments":"[\"self\", \"test_dataset\"]","Docstring":"Returns the test [`~torch.utils.data.DataLoader`].\n\nSubclass and override this method if you want to","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_total_train_batch_size","Module":"chronos.chronos2.trainer","Name":"get_total_train_batch_size","Type":"method","Signature":"(self, args) -> int","Arguments":"[\"self\", \"args\"]","Docstring":"Calculates total batch size (micro_batch * grad_accum * dp_world_size).\n\nNote: Only considers DP and","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_tp_size","Module":"chronos.chronos2.trainer","Name":"get_tp_size","Type":"method","Signature":"(self) -> int","Arguments":"[\"self\"]","Docstring":"Get the tensor parallel size from either the model or DeepSpeed config.","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.get_train_dataloader","Module":"chronos.chronos2.trainer","Name":"get_train_dataloader","Type":"method","Signature":"(self) -> torch.utils.data.dataloader.DataLoader","Arguments":"[\"self\"]","Docstring":"Returns the training [`~torch.utils.data.DataLoader`].\n\nWill use no sampler if `train_dataset` does ","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.hyperparameter_search","Module":"chronos.chronos2.trainer","Name":"hyperparameter_search","Type":"method","Signature":"(self, hp_space: Optional[Callable[[ForwardRef('optuna.Trial')], dict[str, float]]] = None, compute_objective: Optional[Callable[[dict[str, float]], float]] = None, n_trials: int = 20, direction: Union[str, list[str]] = 'minimize', backend: Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None, hp_name: Optional[Callable[[ForwardRef('optuna.Trial')], str]] = None, **kwargs) -> Union[transformers.trainer_utils.BestRun, list[transformers.trainer_utils.BestRun]]","Arguments":"[\"self\", \"hp_space\", \"compute_objective\", \"n_trials\", \"direction\", \"backend\", \"hp_name\", \"kwargs\"]","Docstring":"Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is ","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.init_hf_repo","Module":"chronos.chronos2.trainer","Name":"init_hf_repo","Type":"method","Signature":"(self, token: Optional[str] = None)","Arguments":"[\"self\", \"token\"]","Docstring":"Initializes a git repo in `self.args.hub_model_id`.","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.is_local_process_zero","Module":"chronos.chronos2.trainer","Name":"is_local_process_zero","Type":"method","Signature":"(self) -> bool","Arguments":"[\"self\"]","Docstring":"Whether or not this process is the local (e.g., on one machine if training in a distributed fashion ","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.is_world_process_zero","Module":"chronos.chronos2.trainer","Name":"is_world_process_zero","Type":"method","Signature":"(self) -> bool","Arguments":"[\"self\"]","Docstring":"Whether or not this process is the global main process (when training in a distributed fashion on se","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.log","Module":"chronos.chronos2.trainer","Name":"log","Type":"method","Signature":"(self, logs: dict[str, float], start_time: Optional[float] = None) -> None","Arguments":"[\"self\", \"logs\", \"start_time\"]","Docstring":"Log `logs` on the various objects watching training.\n\nSubclass and override this method to inject cu","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.log_metrics","Module":"chronos.chronos2.trainer","Name":"log_metrics","Type":"method","Signature":"(self, split, metrics)","Arguments":"[\"self\", \"split\", \"metrics\"]","Docstring":"Log metrics in a specially formatted way.\n\nUnder distributed environment this is done only for a pro","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.metrics_format","Module":"chronos.chronos2.trainer","Name":"metrics_format","Type":"method","Signature":"(metrics: dict[str, float]) -> dict[str, float]","Arguments":"[\"metrics\"]","Docstring":"Reformat Trainer metrics values to a human-readable format.\n\nArgs:\n    metrics (`dict[str, float]`):","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.num_examples","Module":"chronos.chronos2.trainer","Name":"num_examples","Type":"method","Signature":"(self, dataloader: torch.utils.data.dataloader.DataLoader) -> int","Arguments":"[\"self\", \"dataloader\"]","Docstring":"Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.num_tokens","Module":"chronos.chronos2.trainer","Name":"num_tokens","Type":"method","Signature":"(train_dl: torch.utils.data.dataloader.DataLoader, max_steps: Optional[int] = None) -> int","Arguments":"[\"train_dl\", \"max_steps\"]","Docstring":"Helper to get number of tokens in a [`~torch.utils.data.DataLoader`] by enumerating dataloader.","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.pop_callback","Module":"chronos.chronos2.trainer","Name":"pop_callback","Type":"method","Signature":"(self, callback)","Arguments":"[\"self\", \"callback\"]","Docstring":"Remove a callback from the current list of [`~transformers.TrainerCallback`] and returns it.\n\nIf the","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.predict","Module":"chronos.chronos2.trainer","Name":"predict","Type":"method","Signature":"(self, test_dataset: torch.utils.data.dataset.Dataset, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'test') -> transformers.trainer_utils.PredictionOutput","Arguments":"[\"self\", \"test_dataset\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Run prediction and returns predictions and potential metrics.\n\nDepending on the dataset and your use","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.prediction_loop","Module":"chronos.chronos2.trainer","Name":"prediction_loop","Type":"method","Signature":"(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput","Arguments":"[\"self\", \"dataloader\", \"description\", \"prediction_loss_only\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Prediction\/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\nWorks both with","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.prediction_step","Module":"chronos.chronos2.trainer","Name":"prediction_step","Type":"method","Signature":"(self, model: torch.nn.modules.module.Module, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]], prediction_loss_only: bool, ignore_keys: Optional[list[str]] = None) -> tuple[typing.Optional[torch.Tensor], typing.Optional[torch.Tensor], typing.Optional[torch.Tensor]]","Arguments":"[\"self\", \"model\", \"inputs\", \"prediction_loss_only\", \"ignore_keys\"]","Docstring":"Perform an evaluation step on `model` using `inputs`.\n\nSubclass and override to inject custom behavi","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.propagate_args_to_deepspeed","Module":"chronos.chronos2.trainer","Name":"propagate_args_to_deepspeed","Type":"method","Signature":"(self, auto_find_batch_size=False)","Arguments":"[\"self\", \"auto_find_batch_size\"]","Docstring":"Sets values in the deepspeed plugin based on the Trainer args","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.push_to_hub","Module":"chronos.chronos2.trainer","Name":"push_to_hub","Type":"method","Signature":"(self, commit_message: Optional[str] = 'End of training', blocking: bool = True, token: Optional[str] = None, revision: Optional[str] = None, **kwargs) -> str","Arguments":"[\"self\", \"commit_message\", \"blocking\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Upload `self.model` and `self.processing_class` to the \ud83e\udd17 model hub on the repo `self.args.hub_model_","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.remove_callback","Module":"chronos.chronos2.trainer","Name":"remove_callback","Type":"method","Signature":"(self, callback)","Arguments":"[\"self\", \"callback\"]","Docstring":"Remove a callback from the current list of [`~transformers.TrainerCallback`].\n\nArgs:\n   callback (`t","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.save_metrics","Module":"chronos.chronos2.trainer","Name":"save_metrics","Type":"method","Signature":"(self, split, metrics, combined=True)","Arguments":"[\"self\", \"split\", \"metrics\", \"combined\"]","Docstring":"Save metrics into a json file for that split, e.g. `train_results.json`.\n\nUnder distributed environm","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.save_model","Module":"chronos.chronos2.trainer","Name":"save_model","Type":"method","Signature":"(self, output_dir: Optional[str] = None, _internal_call: bool = False)","Arguments":"[\"self\", \"output_dir\", \"_internal_call\"]","Docstring":"Will save the model, so you can reload it using `from_pretrained()`.\n\nWill only save from the main p","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.save_state","Module":"chronos.chronos2.trainer","Name":"save_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model.\n\nUnder di","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.set_initial_training_values","Module":"chronos.chronos2.trainer","Name":"set_initial_training_values","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, dataloader: torch.utils.data.dataloader.DataLoader, total_train_batch_size: int)","Arguments":"[\"self\", \"args\", \"dataloader\", \"total_train_batch_size\"]","Docstring":"Calculates and returns the following values:\n- `num_train_epochs`\n- `num_update_steps_per_epoch`\n- `","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.store_flos","Module":"chronos.chronos2.trainer","Name":"store_flos","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.torch_jit_model_eval","Module":"chronos.chronos2.trainer","Name":"torch_jit_model_eval","Type":"method","Signature":"(self, model, dataloader, training=False)","Arguments":"[\"self\", \"model\", \"dataloader\", \"training\"]","Docstring":"","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.train","Module":"chronos.chronos2.trainer","Name":"train","Type":"method","Signature":"(self, resume_from_checkpoint: Union[str, bool, NoneType] = None, trial: Union[ForwardRef('optuna.Trial'), dict[str, Any], NoneType] = None, ignore_keys_for_eval: Optional[list[str]] = None, **kwargs: Any)","Arguments":"[\"self\", \"resume_from_checkpoint\", \"trial\", \"ignore_keys_for_eval\", \"kwargs\"]","Docstring":"Main training entry point.\n\nArgs:\n    resume_from_checkpoint (`str` or `bool`, *optional*):\n        ","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer.training_step","Module":"chronos.chronos2.trainer","Name":"training_step","Type":"method","Signature":"(self, model: torch.nn.modules.module.Module, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]], num_items_in_batch: Optional[torch.Tensor] = None) -> torch.Tensor","Arguments":"[\"self\", \"model\", \"inputs\", \"num_items_in_batch\"]","Docstring":"Perform a training step on a batch of inputs.\n\nSubclass and override to inject custom behavior.\n\nArg","Parent":"Chronos2Trainer"},{"Path":"chronos.chronos2.trainer.Chronos2Trainer","Module":"chronos.chronos2.trainer","Name":"Chronos2Trainer","Type":"class","Signature":"","Arguments":"[]","Docstring":"A custom trainer based on transformers Trainer. We need to override the dataloader getters because w","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.DataLoader.check_worker_number_rationality","Module":"chronos.chronos2.trainer","Name":"check_worker_number_rationality","Type":"method","Signature":"(self) -> 'None'","Arguments":"[\"self\"]","Docstring":"","Parent":"DataLoader"},{"Path":"chronos.chronos2.trainer.DataLoader","Module":"chronos.chronos2.trainer","Name":"DataLoader","Type":"class","Signature":"","Arguments":"[]","Docstring":"Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n\nThe ","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.Dataset","Module":"chronos.chronos2.trainer","Name":"Dataset","Type":"class","Signature":"","Arguments":"[]","Docstring":"An abstract class representing a :class:`Dataset`.\n\nAll datasets that represent a map from keys to d","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_epoch_begin","Module":"chronos.chronos2.trainer","Name":"on_epoch_begin","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the beginning of an epoch.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_epoch_end","Module":"chronos.chronos2.trainer","Name":"on_epoch_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of an epoch.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_evaluate","Module":"chronos.chronos2.trainer","Name":"on_evaluate","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after an evaluation phase.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_init_end","Module":"chronos.chronos2.trainer","Name":"on_init_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of the initialization of the [`Trainer`].","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_log","Module":"chronos.chronos2.trainer","Name":"on_log","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after logging the last logs.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_optimizer_step","Module":"chronos.chronos2.trainer","Name":"on_optimizer_step","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after the optimizer step but before gradients are zeroed out. Useful for monitoring gra","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_pre_optimizer_step","Module":"chronos.chronos2.trainer","Name":"on_pre_optimizer_step","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called before the optimizer step but after gradient clipping. Useful for monitoring gradients.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_predict","Module":"chronos.chronos2.trainer","Name":"on_predict","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, metrics, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"metrics\", \"kwargs\"]","Docstring":"Event called after a successful prediction.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_prediction_step","Module":"chronos.chronos2.trainer","Name":"on_prediction_step","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after a prediction step.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_save","Module":"chronos.chronos2.trainer","Name":"on_save","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after a checkpoint save.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_step_begin","Module":"chronos.chronos2.trainer","Name":"on_step_begin","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the beginning of a training step. If using gradient accumulation, one training step ","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_step_end","Module":"chronos.chronos2.trainer","Name":"on_step_end","Type":"method","Signature":"(self, args, state, control, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of a training step. If using gradient accumulation, one training step might ","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_substep_end","Module":"chronos.chronos2.trainer","Name":"on_substep_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of an substep during gradient accumulation.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_train_begin","Module":"chronos.chronos2.trainer","Name":"on_train_begin","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the beginning of training.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback.on_train_end","Module":"chronos.chronos2.trainer","Name":"on_train_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of training.","Parent":"EvaluateAndSaveFinalStepCallback"},{"Path":"chronos.chronos2.trainer.EvaluateAndSaveFinalStepCallback","Module":"chronos.chronos2.trainer","Name":"EvaluateAndSaveFinalStepCallback","Type":"class","Signature":"","Arguments":"[]","Docstring":"Callback to evaluate and save the model at last training step.","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.TYPE_CHECKING","Module":"chronos.chronos2.trainer","Name":"TYPE_CHECKING","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and Fa","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.Trainer.add_callback","Module":"chronos.chronos2.trainer","Name":"add_callback","Type":"method","Signature":"(self, callback)","Arguments":"[\"self\", \"callback\"]","Docstring":"Add a callback to the current list of [`~transformers.TrainerCallback`].\n\nArgs:\n   callback (`type` ","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.autocast_smart_context_manager","Module":"chronos.chronos2.trainer","Name":"autocast_smart_context_manager","Type":"method","Signature":"(self, cache_enabled: Optional[bool] = True)","Arguments":"[\"self\", \"cache_enabled\"]","Docstring":"A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the des","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.call_model_init","Module":"chronos.chronos2.trainer","Name":"call_model_init","Type":"method","Signature":"(self, trial=None)","Arguments":"[\"self\", \"trial\"]","Docstring":"","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.compare_trainer_and_checkpoint_args","Module":"chronos.chronos2.trainer","Name":"compare_trainer_and_checkpoint_args","Type":"method","Signature":"(self, training_args, trainer_state)","Arguments":"[\"self\", \"training_args\", \"trainer_state\"]","Docstring":"","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.compute_loss","Module":"chronos.chronos2.trainer","Name":"compute_loss","Type":"method","Signature":"(self, model: torch.nn.modules.module.Module, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]], return_outputs: bool = False, num_items_in_batch: Optional[torch.Tensor] = None)","Arguments":"[\"self\", \"model\", \"inputs\", \"return_outputs\", \"num_items_in_batch\"]","Docstring":"How the loss is computed by Trainer. By default, all models return the loss in the first element.\n\nA","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.compute_loss_context_manager","Module":"chronos.chronos2.trainer","Name":"compute_loss_context_manager","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A helper wrapper to group together context managers.","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.create_accelerator_and_postprocess","Module":"chronos.chronos2.trainer","Name":"create_accelerator_and_postprocess","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.create_model_card","Module":"chronos.chronos2.trainer","Name":"create_model_card","Type":"method","Signature":"(self, language: Optional[str] = None, license: Optional[str] = None, tags: Union[str, list[str], NoneType] = None, model_name: Optional[str] = None, finetuned_from: Optional[str] = None, tasks: Union[str, list[str], NoneType] = None, dataset_tags: Union[str, list[str], NoneType] = None, dataset: Union[str, list[str], NoneType] = None, dataset_args: Union[str, list[str], NoneType] = None)","Arguments":"[\"self\", \"language\", \"license\", \"tags\", \"model_name\", \"finetuned_from\", \"tasks\", \"dataset_tags\", \"dataset\", \"dataset_args\"]","Docstring":"Creates a draft of a model card using the information available to the `Trainer`.\n\nArgs:\n    languag","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.create_optimizer","Module":"chronos.chronos2.trainer","Name":"create_optimizer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Setup the optimizer.\n\nWe provide a reasonable default that works well. If you want to use something ","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.create_optimizer_and_scheduler","Module":"chronos.chronos2.trainer","Name":"create_optimizer_and_scheduler","Type":"method","Signature":"(self, num_training_steps: int)","Arguments":"[\"self\", \"num_training_steps\"]","Docstring":"Setup the optimizer and the learning rate scheduler.\n\nWe provide a reasonable default that works wel","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.create_scheduler","Module":"chronos.chronos2.trainer","Name":"create_scheduler","Type":"method","Signature":"(self, num_training_steps: int, optimizer: torch.optim.optimizer.Optimizer = None)","Arguments":"[\"self\", \"num_training_steps\", \"optimizer\"]","Docstring":"Setup the scheduler. The optimizer of the trainer must have been set up either before this method is","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.evaluate","Module":"chronos.chronos2.trainer","Name":"evaluate","Type":"method","Signature":"(self, eval_dataset: Union[torch.utils.data.dataset.Dataset, dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'eval') -> dict[str, float]","Arguments":"[\"self\", \"eval_dataset\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Run evaluation and returns metrics.\n\nThe calling script will be responsible for providing a method t","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.evaluation_loop","Module":"chronos.chronos2.trainer","Name":"evaluation_loop","Type":"method","Signature":"(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput","Arguments":"[\"self\", \"dataloader\", \"description\", \"prediction_loss_only\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Prediction\/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\nWorks both with","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.floating_point_ops","Module":"chronos.chronos2.trainer","Name":"floating_point_ops","Type":"method","Signature":"(self, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]])","Arguments":"[\"self\", \"inputs\"]","Docstring":"For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_batch_samples","Module":"chronos.chronos2.trainer","Name":"get_batch_samples","Type":"method","Signature":"(self, epoch_iterator: collections.abc.Iterator, num_batches: int, device: torch.device) -> tuple[list, typing.Union[torch.Tensor, int, NoneType]]","Arguments":"[\"self\", \"epoch_iterator\", \"num_batches\", \"device\"]","Docstring":"Collects a specified number of batches from the epoch iterator and optionally counts the number of i","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_decay_parameter_names","Module":"chronos.chronos2.trainer","Name":"get_decay_parameter_names","Type":"method","Signature":"(self, model) -> list[str]","Arguments":"[\"self\", \"model\"]","Docstring":"Get all parameter names that weight decay will be applied to.\n\nThis function filters out parameters ","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_eval_dataloader","Module":"chronos.chronos2.trainer","Name":"get_eval_dataloader","Type":"method","Signature":"(self, eval_dataset: Union[str, torch.utils.data.dataset.Dataset, NoneType] = None) -> torch.utils.data.dataloader.DataLoader","Arguments":"[\"self\", \"eval_dataset\"]","Docstring":"Returns the evaluation [`~torch.utils.data.DataLoader`].\n\nSubclass and override this method if you w","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_learning_rates","Module":"chronos.chronos2.trainer","Name":"get_learning_rates","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Returns the learning rate of each parameter from self.optimizer.","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_num_trainable_parameters","Module":"chronos.chronos2.trainer","Name":"get_num_trainable_parameters","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Get the number of trainable parameters.","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_optimizer_cls_and_kwargs","Module":"chronos.chronos2.trainer","Name":"get_optimizer_cls_and_kwargs","Type":"method","Signature":"(args: transformers.training_args.TrainingArguments, model: Optional[transformers.modeling_utils.PreTrainedModel] = None) -> tuple[typing.Any, typing.Any]","Arguments":"[\"args\", \"model\"]","Docstring":"Returns the optimizer class and optimizer parameters based on the training arguments.\n\nArgs:\n    arg","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_optimizer_group","Module":"chronos.chronos2.trainer","Name":"get_optimizer_group","Type":"method","Signature":"(self, param: Union[str, torch.nn.parameter.Parameter, NoneType] = None)","Arguments":"[\"self\", \"param\"]","Docstring":"Returns optimizer group for a parameter if given, else returns all optimizer groups for params.\n\nArg","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_test_dataloader","Module":"chronos.chronos2.trainer","Name":"get_test_dataloader","Type":"method","Signature":"(self, test_dataset: torch.utils.data.dataset.Dataset) -> torch.utils.data.dataloader.DataLoader","Arguments":"[\"self\", \"test_dataset\"]","Docstring":"Returns the test [`~torch.utils.data.DataLoader`].\n\nSubclass and override this method if you want to","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_total_train_batch_size","Module":"chronos.chronos2.trainer","Name":"get_total_train_batch_size","Type":"method","Signature":"(self, args) -> int","Arguments":"[\"self\", \"args\"]","Docstring":"Calculates total batch size (micro_batch * grad_accum * dp_world_size).\n\nNote: Only considers DP and","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_tp_size","Module":"chronos.chronos2.trainer","Name":"get_tp_size","Type":"method","Signature":"(self) -> int","Arguments":"[\"self\"]","Docstring":"Get the tensor parallel size from either the model or DeepSpeed config.","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.get_train_dataloader","Module":"chronos.chronos2.trainer","Name":"get_train_dataloader","Type":"method","Signature":"(self) -> torch.utils.data.dataloader.DataLoader","Arguments":"[\"self\"]","Docstring":"Returns the training [`~torch.utils.data.DataLoader`].\n\nWill use no sampler if `train_dataset` does ","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.hyperparameter_search","Module":"chronos.chronos2.trainer","Name":"hyperparameter_search","Type":"method","Signature":"(self, hp_space: Optional[Callable[[ForwardRef('optuna.Trial')], dict[str, float]]] = None, compute_objective: Optional[Callable[[dict[str, float]], float]] = None, n_trials: int = 20, direction: Union[str, list[str]] = 'minimize', backend: Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None, hp_name: Optional[Callable[[ForwardRef('optuna.Trial')], str]] = None, **kwargs) -> Union[transformers.trainer_utils.BestRun, list[transformers.trainer_utils.BestRun]]","Arguments":"[\"self\", \"hp_space\", \"compute_objective\", \"n_trials\", \"direction\", \"backend\", \"hp_name\", \"kwargs\"]","Docstring":"Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is ","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.init_hf_repo","Module":"chronos.chronos2.trainer","Name":"init_hf_repo","Type":"method","Signature":"(self, token: Optional[str] = None)","Arguments":"[\"self\", \"token\"]","Docstring":"Initializes a git repo in `self.args.hub_model_id`.","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.is_local_process_zero","Module":"chronos.chronos2.trainer","Name":"is_local_process_zero","Type":"method","Signature":"(self) -> bool","Arguments":"[\"self\"]","Docstring":"Whether or not this process is the local (e.g., on one machine if training in a distributed fashion ","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.is_world_process_zero","Module":"chronos.chronos2.trainer","Name":"is_world_process_zero","Type":"method","Signature":"(self) -> bool","Arguments":"[\"self\"]","Docstring":"Whether or not this process is the global main process (when training in a distributed fashion on se","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.log","Module":"chronos.chronos2.trainer","Name":"log","Type":"method","Signature":"(self, logs: dict[str, float], start_time: Optional[float] = None) -> None","Arguments":"[\"self\", \"logs\", \"start_time\"]","Docstring":"Log `logs` on the various objects watching training.\n\nSubclass and override this method to inject cu","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.log_metrics","Module":"chronos.chronos2.trainer","Name":"log_metrics","Type":"method","Signature":"(self, split, metrics)","Arguments":"[\"self\", \"split\", \"metrics\"]","Docstring":"Log metrics in a specially formatted way.\n\nUnder distributed environment this is done only for a pro","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.metrics_format","Module":"chronos.chronos2.trainer","Name":"metrics_format","Type":"method","Signature":"(metrics: dict[str, float]) -> dict[str, float]","Arguments":"[\"metrics\"]","Docstring":"Reformat Trainer metrics values to a human-readable format.\n\nArgs:\n    metrics (`dict[str, float]`):","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.num_examples","Module":"chronos.chronos2.trainer","Name":"num_examples","Type":"method","Signature":"(self, dataloader: torch.utils.data.dataloader.DataLoader) -> int","Arguments":"[\"self\", \"dataloader\"]","Docstring":"Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.num_tokens","Module":"chronos.chronos2.trainer","Name":"num_tokens","Type":"method","Signature":"(train_dl: torch.utils.data.dataloader.DataLoader, max_steps: Optional[int] = None) -> int","Arguments":"[\"train_dl\", \"max_steps\"]","Docstring":"Helper to get number of tokens in a [`~torch.utils.data.DataLoader`] by enumerating dataloader.","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.pop_callback","Module":"chronos.chronos2.trainer","Name":"pop_callback","Type":"method","Signature":"(self, callback)","Arguments":"[\"self\", \"callback\"]","Docstring":"Remove a callback from the current list of [`~transformers.TrainerCallback`] and returns it.\n\nIf the","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.predict","Module":"chronos.chronos2.trainer","Name":"predict","Type":"method","Signature":"(self, test_dataset: torch.utils.data.dataset.Dataset, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'test') -> transformers.trainer_utils.PredictionOutput","Arguments":"[\"self\", \"test_dataset\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Run prediction and returns predictions and potential metrics.\n\nDepending on the dataset and your use","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.prediction_loop","Module":"chronos.chronos2.trainer","Name":"prediction_loop","Type":"method","Signature":"(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput","Arguments":"[\"self\", \"dataloader\", \"description\", \"prediction_loss_only\", \"ignore_keys\", \"metric_key_prefix\"]","Docstring":"Prediction\/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\nWorks both with","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.prediction_step","Module":"chronos.chronos2.trainer","Name":"prediction_step","Type":"method","Signature":"(self, model: torch.nn.modules.module.Module, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]], prediction_loss_only: bool, ignore_keys: Optional[list[str]] = None) -> tuple[typing.Optional[torch.Tensor], typing.Optional[torch.Tensor], typing.Optional[torch.Tensor]]","Arguments":"[\"self\", \"model\", \"inputs\", \"prediction_loss_only\", \"ignore_keys\"]","Docstring":"Perform an evaluation step on `model` using `inputs`.\n\nSubclass and override to inject custom behavi","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.propagate_args_to_deepspeed","Module":"chronos.chronos2.trainer","Name":"propagate_args_to_deepspeed","Type":"method","Signature":"(self, auto_find_batch_size=False)","Arguments":"[\"self\", \"auto_find_batch_size\"]","Docstring":"Sets values in the deepspeed plugin based on the Trainer args","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.push_to_hub","Module":"chronos.chronos2.trainer","Name":"push_to_hub","Type":"method","Signature":"(self, commit_message: Optional[str] = 'End of training', blocking: bool = True, token: Optional[str] = None, revision: Optional[str] = None, **kwargs) -> str","Arguments":"[\"self\", \"commit_message\", \"blocking\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Upload `self.model` and `self.processing_class` to the \ud83e\udd17 model hub on the repo `self.args.hub_model_","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.remove_callback","Module":"chronos.chronos2.trainer","Name":"remove_callback","Type":"method","Signature":"(self, callback)","Arguments":"[\"self\", \"callback\"]","Docstring":"Remove a callback from the current list of [`~transformers.TrainerCallback`].\n\nArgs:\n   callback (`t","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.save_metrics","Module":"chronos.chronos2.trainer","Name":"save_metrics","Type":"method","Signature":"(self, split, metrics, combined=True)","Arguments":"[\"self\", \"split\", \"metrics\", \"combined\"]","Docstring":"Save metrics into a json file for that split, e.g. `train_results.json`.\n\nUnder distributed environm","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.save_model","Module":"chronos.chronos2.trainer","Name":"save_model","Type":"method","Signature":"(self, output_dir: Optional[str] = None, _internal_call: bool = False)","Arguments":"[\"self\", \"output_dir\", \"_internal_call\"]","Docstring":"Will save the model, so you can reload it using `from_pretrained()`.\n\nWill only save from the main p","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.save_state","Module":"chronos.chronos2.trainer","Name":"save_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model.\n\nUnder di","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.set_initial_training_values","Module":"chronos.chronos2.trainer","Name":"set_initial_training_values","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, dataloader: torch.utils.data.dataloader.DataLoader, total_train_batch_size: int)","Arguments":"[\"self\", \"args\", \"dataloader\", \"total_train_batch_size\"]","Docstring":"Calculates and returns the following values:\n- `num_train_epochs`\n- `num_update_steps_per_epoch`\n- `","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.store_flos","Module":"chronos.chronos2.trainer","Name":"store_flos","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.torch_jit_model_eval","Module":"chronos.chronos2.trainer","Name":"torch_jit_model_eval","Type":"method","Signature":"(self, model, dataloader, training=False)","Arguments":"[\"self\", \"model\", \"dataloader\", \"training\"]","Docstring":"","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.train","Module":"chronos.chronos2.trainer","Name":"train","Type":"method","Signature":"(self, resume_from_checkpoint: Union[str, bool, NoneType] = None, trial: Union[ForwardRef('optuna.Trial'), dict[str, Any], NoneType] = None, ignore_keys_for_eval: Optional[list[str]] = None, **kwargs: Any)","Arguments":"[\"self\", \"resume_from_checkpoint\", \"trial\", \"ignore_keys_for_eval\", \"kwargs\"]","Docstring":"Main training entry point.\n\nArgs:\n    resume_from_checkpoint (`str` or `bool`, *optional*):\n        ","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer.training_step","Module":"chronos.chronos2.trainer","Name":"training_step","Type":"method","Signature":"(self, model: torch.nn.modules.module.Module, inputs: dict[str, typing.Union[torch.Tensor, typing.Any]], num_items_in_batch: Optional[torch.Tensor] = None) -> torch.Tensor","Arguments":"[\"self\", \"model\", \"inputs\", \"num_items_in_batch\"]","Docstring":"Perform a training step on a batch of inputs.\n\nSubclass and override to inject custom behavior.\n\nArg","Parent":"Trainer"},{"Path":"chronos.chronos2.trainer.Trainer","Module":"chronos.chronos2.trainer","Name":"Trainer","Type":"class","Signature":"","Arguments":"[]","Docstring":"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \ud83e\udd17 Transfo","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_epoch_begin","Module":"chronos.chronos2.trainer","Name":"on_epoch_begin","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the beginning of an epoch.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_epoch_end","Module":"chronos.chronos2.trainer","Name":"on_epoch_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of an epoch.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_evaluate","Module":"chronos.chronos2.trainer","Name":"on_evaluate","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after an evaluation phase.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_init_end","Module":"chronos.chronos2.trainer","Name":"on_init_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of the initialization of the [`Trainer`].","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_log","Module":"chronos.chronos2.trainer","Name":"on_log","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after logging the last logs.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_optimizer_step","Module":"chronos.chronos2.trainer","Name":"on_optimizer_step","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after the optimizer step but before gradients are zeroed out. Useful for monitoring gra","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_pre_optimizer_step","Module":"chronos.chronos2.trainer","Name":"on_pre_optimizer_step","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called before the optimizer step but after gradient clipping. Useful for monitoring gradients.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_predict","Module":"chronos.chronos2.trainer","Name":"on_predict","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, metrics, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"metrics\", \"kwargs\"]","Docstring":"Event called after a successful prediction.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_prediction_step","Module":"chronos.chronos2.trainer","Name":"on_prediction_step","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after a prediction step.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_save","Module":"chronos.chronos2.trainer","Name":"on_save","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called after a checkpoint save.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_step_begin","Module":"chronos.chronos2.trainer","Name":"on_step_begin","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the beginning of a training step. If using gradient accumulation, one training step ","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_step_end","Module":"chronos.chronos2.trainer","Name":"on_step_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of a training step. If using gradient accumulation, one training step might ","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_substep_end","Module":"chronos.chronos2.trainer","Name":"on_substep_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of an substep during gradient accumulation.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_train_begin","Module":"chronos.chronos2.trainer","Name":"on_train_begin","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the beginning of training.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback.on_train_end","Module":"chronos.chronos2.trainer","Name":"on_train_end","Type":"method","Signature":"(self, args: transformers.training_args.TrainingArguments, state: transformers.trainer_callback.TrainerState, control: transformers.trainer_callback.TrainerControl, **kwargs)","Arguments":"[\"self\", \"args\", \"state\", \"control\", \"kwargs\"]","Docstring":"Event called at the end of training.","Parent":"TrainerCallback"},{"Path":"chronos.chronos2.trainer.TrainerCallback","Module":"chronos.chronos2.trainer","Name":"TrainerCallback","Type":"class","Signature":"","Arguments":"[]","Docstring":"A class for objects that will inspect the state of the training loop at some events and take some de","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.cast","Module":"chronos.chronos2.trainer","Name":"cast","Type":"function","Signature":"(typ, val)","Arguments":"[\"typ\", \"val\"]","Docstring":"Cast a value to a type.\n\nThis returns the value unchanged.  To the type checker this\nsignals that th","Parent":"chronos2"},{"Path":"chronos.chronos2.trainer.seed_worker","Module":"chronos.chronos2.trainer","Name":"seed_worker","Type":"function","Signature":"(worker_id: int)","Arguments":"[\"worker_id\"]","Docstring":"","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.AutoConfig.for_model","Module":"chronos.chronos2.pipeline","Name":"for_model","Type":"method","Signature":"(model_type: str, *args, **kwargs) -> transformers.configuration_utils.PretrainedConfig","Arguments":"[\"model_type\", \"args\", \"kwargs\"]","Docstring":"","Parent":"AutoConfig"},{"Path":"chronos.chronos2.pipeline.AutoConfig.from_pretrained","Module":"chronos.chronos2.pipeline","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike[str]], **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"Instantiate one of the configuration classes of the library from a pretrained model configuration.\n\n","Parent":"AutoConfig"},{"Path":"chronos.chronos2.pipeline.AutoConfig.register","Module":"chronos.chronos2.pipeline","Name":"register","Type":"method","Signature":"(model_type, config, exist_ok=False) -> None","Arguments":"[\"model_type\", \"config\", \"exist_ok\"]","Docstring":"Register a new configuration for this class.\n\nArgs:\n    model_type (`str`): The model type like \"ber","Parent":"AutoConfig"},{"Path":"chronos.chronos2.pipeline.AutoConfig","Module":"chronos.chronos2.pipeline","Name":"AutoConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"This is a generic configuration class that will be instantiated as one of the configuration classes ","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.BaseChronosPipeline.from_pretrained","Module":"chronos.chronos2.pipeline","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, pathlib.Path], *model_args, force_s3_download=False, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"force_s3_download\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix, or from the HuggingFace Hub.\nSupports the same ","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos2.pipeline.BaseChronosPipeline.predict","Module":"chronos.chronos2.pipeline","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None)","Arguments":"[\"self\", \"inputs\", \"prediction_length\"]","Docstring":"Get forecasts for the given time series. Predictions will be\nreturned in fp32 on the cpu.\n\nParameter","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos2.pipeline.BaseChronosPipeline.predict_df","Module":"chronos.chronos2.pipeline","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos2.pipeline.BaseChronosPipeline.predict_fev","Module":"chronos.chronos2.pipeline","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos2.pipeline.BaseChronosPipeline.predict_quantiles","Module":"chronos.chronos2.pipeline","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"kwargs\"]","Docstring":"Get quantile and mean forecasts for given time series.\nPredictions will be returned in fp32 on the c","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos2.pipeline.BaseChronosPipeline","Module":"chronos.chronos2.pipeline","Name":"BaseChronosPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Callable","Module":"chronos.chronos2.pipeline","Name":"Callable","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Deprecated alias to collections.abc.Callable.\n\nCallable[[int], str] signifies a function that takes ","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Chronos2Dataset.convert_inputs","Module":"chronos.chronos2.pipeline","Name":"convert_inputs","Type":"method","Signature":"(inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]]], context_length: int, prediction_length: int, batch_size: int, output_patch_size: int, min_past: int = 1, mode: str | chronos.chronos2.dataset.DatasetMode = <DatasetMode.TRAIN: 'train'>) -> 'Chronos2Dataset'","Arguments":"[\"inputs\", \"context_length\", \"prediction_length\", \"batch_size\", \"output_patch_size\", \"min_past\", \"mode\"]","Docstring":"Convert from different input formats to a Chronos2Dataset.","Parent":"Chronos2Dataset"},{"Path":"chronos.chronos2.pipeline.Chronos2Dataset","Module":"chronos.chronos2.pipeline","Name":"Chronos2Dataset","Type":"class","Signature":"","Arguments":"[]","Docstring":"A dataset wrapper for Chronos-2 models.\n\nArguments\n----------\ninputs\n    Time series data. Must be a","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.active_adapters","Module":"chronos.chronos2.pipeline","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.add_adapter","Module":"chronos.chronos2.pipeline","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.add_memory_hooks","Module":"chronos.chronos2.pipeline","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.add_model_tags","Module":"chronos.chronos2.pipeline","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.add_module","Module":"chronos.chronos2.pipeline","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.apply","Module":"chronos.chronos2.pipeline","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.bfloat16","Module":"chronos.chronos2.pipeline","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.buffers","Module":"chronos.chronos2.pipeline","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.can_generate","Module":"chronos.chronos2.pipeline","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.children","Module":"chronos.chronos2.pipeline","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.compile","Module":"chronos.chronos2.pipeline","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.cpu","Module":"chronos.chronos2.pipeline","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.create_extended_attention_mask_for_decoder","Module":"chronos.chronos2.pipeline","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.cuda","Module":"chronos.chronos2.pipeline","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.delete_adapter","Module":"chronos.chronos2.pipeline","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.dequantize","Module":"chronos.chronos2.pipeline","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.disable_adapters","Module":"chronos.chronos2.pipeline","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.disable_input_require_grads","Module":"chronos.chronos2.pipeline","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.double","Module":"chronos.chronos2.pipeline","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.enable_adapters","Module":"chronos.chronos2.pipeline","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.enable_input_require_grads","Module":"chronos.chronos2.pipeline","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.encode","Module":"chronos.chronos2.pipeline","Name":"encode","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False)","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.estimate_tokens","Module":"chronos.chronos2.pipeline","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.eval","Module":"chronos.chronos2.pipeline","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.extra_repr","Module":"chronos.chronos2.pipeline","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.float","Module":"chronos.chronos2.pipeline","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.floating_point_ops","Module":"chronos.chronos2.pipeline","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.forward","Module":"chronos.chronos2.pipeline","Name":"forward","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False) -> chronos.chronos2.model.Chronos2Output","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"Forward pass of the Chronos2 model.\n\nParameters\n----------\ncontext\n    Input tensor of shape (batch_","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.from_pretrained","Module":"chronos.chronos2.pipeline","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_adapter_state_dict","Module":"chronos.chronos2.pipeline","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_buffer","Module":"chronos.chronos2.pipeline","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_compiled_call","Module":"chronos.chronos2.pipeline","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_correct_attn_implementation","Module":"chronos.chronos2.pipeline","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_decoder","Module":"chronos.chronos2.pipeline","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_extended_attention_mask","Module":"chronos.chronos2.pipeline","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_extra_state","Module":"chronos.chronos2.pipeline","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_head_mask","Module":"chronos.chronos2.pipeline","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_init_context","Module":"chronos.chronos2.pipeline","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_input_embeddings","Module":"chronos.chronos2.pipeline","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_memory_footprint","Module":"chronos.chronos2.pipeline","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_output_embeddings","Module":"chronos.chronos2.pipeline","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_parameter","Module":"chronos.chronos2.pipeline","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_parameter_or_buffer","Module":"chronos.chronos2.pipeline","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_position_embeddings","Module":"chronos.chronos2.pipeline","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.get_submodule","Module":"chronos.chronos2.pipeline","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.gradient_checkpointing_disable","Module":"chronos.chronos2.pipeline","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.gradient_checkpointing_enable","Module":"chronos.chronos2.pipeline","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.half","Module":"chronos.chronos2.pipeline","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.init_weights","Module":"chronos.chronos2.pipeline","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.initialize_weights","Module":"chronos.chronos2.pipeline","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.invert_attention_mask","Module":"chronos.chronos2.pipeline","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.ipu","Module":"chronos.chronos2.pipeline","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.is_backend_compatible","Module":"chronos.chronos2.pipeline","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.kernelize","Module":"chronos.chronos2.pipeline","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.load_adapter","Module":"chronos.chronos2.pipeline","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.load_state_dict","Module":"chronos.chronos2.pipeline","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.modules","Module":"chronos.chronos2.pipeline","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.mtia","Module":"chronos.chronos2.pipeline","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.named_buffers","Module":"chronos.chronos2.pipeline","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.named_children","Module":"chronos.chronos2.pipeline","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.named_modules","Module":"chronos.chronos2.pipeline","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.named_parameters","Module":"chronos.chronos2.pipeline","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.num_parameters","Module":"chronos.chronos2.pipeline","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.parameters","Module":"chronos.chronos2.pipeline","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.post_init","Module":"chronos.chronos2.pipeline","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.prune_heads","Module":"chronos.chronos2.pipeline","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.push_to_hub","Module":"chronos.chronos2.pipeline","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_backward_hook","Module":"chronos.chronos2.pipeline","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_buffer","Module":"chronos.chronos2.pipeline","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_for_auto_class","Module":"chronos.chronos2.pipeline","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_forward_hook","Module":"chronos.chronos2.pipeline","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_forward_pre_hook","Module":"chronos.chronos2.pipeline","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_full_backward_hook","Module":"chronos.chronos2.pipeline","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_full_backward_pre_hook","Module":"chronos.chronos2.pipeline","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_load_state_dict_post_hook","Module":"chronos.chronos2.pipeline","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_load_state_dict_pre_hook","Module":"chronos.chronos2.pipeline","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_module","Module":"chronos.chronos2.pipeline","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_parameter","Module":"chronos.chronos2.pipeline","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_state_dict_post_hook","Module":"chronos.chronos2.pipeline","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.register_state_dict_pre_hook","Module":"chronos.chronos2.pipeline","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.requires_grad_","Module":"chronos.chronos2.pipeline","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.reset_memory_hooks_state","Module":"chronos.chronos2.pipeline","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.resize_position_embeddings","Module":"chronos.chronos2.pipeline","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.resize_token_embeddings","Module":"chronos.chronos2.pipeline","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.retrieve_modules_from_names","Module":"chronos.chronos2.pipeline","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.reverse_bettertransformer","Module":"chronos.chronos2.pipeline","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.save_pretrained","Module":"chronos.chronos2.pipeline","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.set_adapter","Module":"chronos.chronos2.pipeline","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.set_attn_implementation","Module":"chronos.chronos2.pipeline","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.set_decoder","Module":"chronos.chronos2.pipeline","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.set_extra_state","Module":"chronos.chronos2.pipeline","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.set_input_embeddings","Module":"chronos.chronos2.pipeline","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.set_output_embeddings","Module":"chronos.chronos2.pipeline","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.set_submodule","Module":"chronos.chronos2.pipeline","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.share_memory","Module":"chronos.chronos2.pipeline","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.state_dict","Module":"chronos.chronos2.pipeline","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos2.pipeline","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.tie_weights","Module":"chronos.chronos2.pipeline","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.to","Module":"chronos.chronos2.pipeline","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.to_bettertransformer","Module":"chronos.chronos2.pipeline","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.to_empty","Module":"chronos.chronos2.pipeline","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.train","Module":"chronos.chronos2.pipeline","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.type","Module":"chronos.chronos2.pipeline","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos2.pipeline","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.xpu","Module":"chronos.chronos2.pipeline","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model.zero_grad","Module":"chronos.chronos2.pipeline","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.pipeline.Chronos2Model","Module":"chronos.chronos2.pipeline","Name":"Chronos2Model","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all models.\n\n[`PreTrainedModel`] takes care of storing the configuration of the model","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.embed","Module":"chronos.chronos2.pipeline","Name":"embed","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray]], batch_size: int = 256, context_length: int | None = None) -> tuple[list[torch.Tensor], list[tuple[torch.Tensor, torch.Tensor]]]","Arguments":"[\"self\", \"inputs\", \"batch_size\", \"context_length\"]","Docstring":"Get encoder embeddings for the given time series.\n\nParameters\n----------\ninputs\n    The time series ","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.fit","Module":"chronos.chronos2.pipeline","Name":"fit","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]]], prediction_length: int, validation_inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]], NoneType] = None, finetune_mode: Literal['full', 'lora'] = 'full', lora_config: 'LoraConfig | dict | None' = None, context_length: int | None = None, learning_rate: float = 1e-06, num_steps: int = 1000, batch_size: int = 256, output_dir: pathlib.Path | str | None = None, min_past: int | None = None, finetuned_ckpt_name: str = 'finetuned-ckpt', callbacks: list['TrainerCallback'] | None = None, remove_printer_callback: bool = False, disable_data_parallel: bool = True, **extra_trainer_kwargs) -> 'Chronos2Pipeline'","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"validation_inputs\", \"finetune_mode\", \"lora_config\", \"context_length\", \"learning_rate\", \"num_steps\", \"batch_size\", \"output_dir\", \"min_past\", \"finetuned_ckpt_name\", \"callbacks\", \"remove_printer_callback\", \"disable_data_parallel\", \"extra_trainer_kwargs\"]","Docstring":"Fine-tune a copy of the current Chronos-2 model on the given inputs and return a new pipeline.\n\nPara","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.from_pretrained","Module":"chronos.chronos2.pipeline","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path, *args, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"args\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix or from the HuggingFace Hub.\nSupports the same a","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.predict","Module":"chronos.chronos2.pipeline","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]]]], prediction_length: int | None = None, batch_size: int = 256, context_length: int | None = None, cross_learning: bool = False, limit_prediction_length: bool = False, **kwargs) -> list[torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"batch_size\", \"context_length\", \"cross_learning\", \"limit_prediction_length\", \"kwargs\"]","Docstring":"Generate forecasts for the given time series.\n\nParameters\n----------\ninputs\n    The time series to g","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.predict_df","Module":"chronos.chronos2.pipeline","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', future_df: 'pd.DataFrame | None' = None, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str | list[str] = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], batch_size: int = 256, context_length: int | None = None, cross_learning: bool = False, validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"future_df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"batch_size\", \"context_length\", \"cross_learning\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.predict_fev","Module":"chronos.chronos2.pipeline","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 256, as_univariate: bool = False, finetune_kwargs: dict | None = None, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"as_univariate\", \"finetune_kwargs\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.predict_quantiles","Module":"chronos.chronos2.pipeline","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]]]], prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **predict_kwargs) -> tuple[list[torch.Tensor], list[torch.Tensor]]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"predict_kwargs\"]","Docstring":"Refer to ``Chronos2Pipeline.predict`` for shared parameters.\n\nAdditional parameters\n----------------","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline.save_pretrained","Module":"chronos.chronos2.pipeline","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: str | pathlib.Path, *args, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"args\", \"kwargs\"]","Docstring":"Save the underlying model to a local directory or to HuggingFace Hub.","Parent":"Chronos2Pipeline"},{"Path":"chronos.chronos2.pipeline.Chronos2Pipeline","Module":"chronos.chronos2.pipeline","Name":"Chronos2Pipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.DataLoader.check_worker_number_rationality","Module":"chronos.chronos2.pipeline","Name":"check_worker_number_rationality","Type":"method","Signature":"(self) -> 'None'","Arguments":"[\"self\"]","Docstring":"","Parent":"DataLoader"},{"Path":"chronos.chronos2.pipeline.DataLoader","Module":"chronos.chronos2.pipeline","Name":"DataLoader","Type":"class","Signature":"","Arguments":"[]","Docstring":"Data loader combines a dataset and a sampler, and provides an iterable over the given dataset.\n\nThe ","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.DatasetMode","Module":"chronos.chronos2.pipeline","Name":"DatasetMode","Type":"class","Signature":"","Arguments":"[]","Docstring":"str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object ","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.ForecastType","Module":"chronos.chronos2.pipeline","Name":"ForecastType","Type":"class","Signature":"","Arguments":"[]","Docstring":"Create a collection of name\/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED =","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Literal","Module":"chronos.chronos2.pipeline","Name":"Literal","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Special typing form to define literal types (a.k.a. value types).\n\nThis form can be used to indicate","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Mapping","Module":"chronos.chronos2.pipeline","Name":"Mapping","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of collections.abc.Mapping.","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Path.absolute","Module":"chronos.chronos2.pipeline","Name":"absolute","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return an absolute version of this path by prepending the current\nworking directory. No normalizatio","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.as_posix","Module":"chronos.chronos2.pipeline","Name":"as_posix","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the string representation of the path with forward (\/)\nslashes.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.as_uri","Module":"chronos.chronos2.pipeline","Name":"as_uri","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the path as a 'file' URI.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.chmod","Module":"chronos.chronos2.pipeline","Name":"chmod","Type":"method","Signature":"(self, mode, *, follow_symlinks=True)","Arguments":"[\"self\", \"mode\", \"follow_symlinks\"]","Docstring":"Change the permissions of the path, like os.chmod().","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.cwd","Module":"chronos.chronos2.pipeline","Name":"cwd","Type":"method","Signature":"()","Arguments":"[]","Docstring":"Return a new path pointing to the current working directory\n(as returned by os.getcwd()).","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.exists","Module":"chronos.chronos2.pipeline","Name":"exists","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path exists.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.expanduser","Module":"chronos.chronos2.pipeline","Name":"expanduser","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return a new path with expanded ~ and ~user constructs\n(as returned by os.path.expanduser)","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.glob","Module":"chronos.chronos2.pipeline","Name":"glob","Type":"method","Signature":"(self, pattern)","Arguments":"[\"self\", \"pattern\"]","Docstring":"Iterate over this subtree and yield all existing files (of any\nkind, including directories) matching","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.group","Module":"chronos.chronos2.pipeline","Name":"group","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the group name of the file gid.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.hardlink_to","Module":"chronos.chronos2.pipeline","Name":"hardlink_to","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Make this path a hard link pointing to the same file as *target*.\n\nNote the order of arguments (self","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.home","Module":"chronos.chronos2.pipeline","Name":"home","Type":"method","Signature":"()","Arguments":"[]","Docstring":"Return a new path pointing to the user's home directory (as\nreturned by os.path.expanduser('~')).","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_absolute","Module":"chronos.chronos2.pipeline","Name":"is_absolute","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"True if the path is absolute (has both a root and, if applicable,\na drive).","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_block_device","Module":"chronos.chronos2.pipeline","Name":"is_block_device","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a block device.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_char_device","Module":"chronos.chronos2.pipeline","Name":"is_char_device","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a character device.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_dir","Module":"chronos.chronos2.pipeline","Name":"is_dir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a directory.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_fifo","Module":"chronos.chronos2.pipeline","Name":"is_fifo","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a FIFO.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_file","Module":"chronos.chronos2.pipeline","Name":"is_file","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a regular file (also True for symlinks pointing\nto regular files).","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_mount","Module":"chronos.chronos2.pipeline","Name":"is_mount","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Check if this path is a POSIX mount point","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_relative_to","Module":"chronos.chronos2.pipeline","Name":"is_relative_to","Type":"method","Signature":"(self, *other)","Arguments":"[\"self\", \"other\"]","Docstring":"Return True if the path is relative to another path or False.\n        ","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_reserved","Module":"chronos.chronos2.pipeline","Name":"is_reserved","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return True if the path contains one of the special names reserved\nby the system, if any.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_socket","Module":"chronos.chronos2.pipeline","Name":"is_socket","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a socket.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.is_symlink","Module":"chronos.chronos2.pipeline","Name":"is_symlink","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a symbolic link.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.iterdir","Module":"chronos.chronos2.pipeline","Name":"iterdir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Iterate over the files in this directory.  Does not yield any\nresult for the special paths '.' and '","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.joinpath","Module":"chronos.chronos2.pipeline","Name":"joinpath","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Combine this path with one or several arguments, and return a\nnew path representing either a subpath","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.lchmod","Module":"chronos.chronos2.pipeline","Name":"lchmod","Type":"method","Signature":"(self, mode)","Arguments":"[\"self\", \"mode\"]","Docstring":"Like chmod(), except if the path points to a symlink, the symlink's\npermissions are changed, rather ","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.link_to","Module":"chronos.chronos2.pipeline","Name":"link_to","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Make the target path a hard link pointing to this path.\n\nNote this function does not make this path ","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.lstat","Module":"chronos.chronos2.pipeline","Name":"lstat","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Like stat(), except if the path points to a symlink, the symlink's\nstatus information is returned, r","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.match","Module":"chronos.chronos2.pipeline","Name":"match","Type":"method","Signature":"(self, path_pattern)","Arguments":"[\"self\", \"path_pattern\"]","Docstring":"Return True if this path matches the given pattern.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.mkdir","Module":"chronos.chronos2.pipeline","Name":"mkdir","Type":"method","Signature":"(self, mode=511, parents=False, exist_ok=False)","Arguments":"[\"self\", \"mode\", \"parents\", \"exist_ok\"]","Docstring":"Create a new directory at this given path.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.open","Module":"chronos.chronos2.pipeline","Name":"open","Type":"method","Signature":"(self, mode='r', buffering=-1, encoding=None, errors=None, newline=None)","Arguments":"[\"self\", \"mode\", \"buffering\", \"encoding\", \"errors\", \"newline\"]","Docstring":"Open the file pointed by this path and return a file object, as\nthe built-in open() function does.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.owner","Module":"chronos.chronos2.pipeline","Name":"owner","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the login name of the file owner.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.read_bytes","Module":"chronos.chronos2.pipeline","Name":"read_bytes","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Open the file in bytes mode, read it, and close the file.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.read_text","Module":"chronos.chronos2.pipeline","Name":"read_text","Type":"method","Signature":"(self, encoding=None, errors=None)","Arguments":"[\"self\", \"encoding\", \"errors\"]","Docstring":"Open the file in text mode, read it, and close the file.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.readlink","Module":"chronos.chronos2.pipeline","Name":"readlink","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the path to which the symbolic link points.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.relative_to","Module":"chronos.chronos2.pipeline","Name":"relative_to","Type":"method","Signature":"(self, *other)","Arguments":"[\"self\", \"other\"]","Docstring":"Return the relative path to another path identified by the passed\narguments.  If the operation is no","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.rename","Module":"chronos.chronos2.pipeline","Name":"rename","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Rename this path to the target path.\n\nThe target path may be absolute or relative. Relative paths ar","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.replace","Module":"chronos.chronos2.pipeline","Name":"replace","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Rename this path to the target path, overwriting if that path exists.\n\nThe target path may be absolu","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.resolve","Module":"chronos.chronos2.pipeline","Name":"resolve","Type":"method","Signature":"(self, strict=False)","Arguments":"[\"self\", \"strict\"]","Docstring":"Make the path absolute, resolving all symlinks on the way and also\nnormalizing it.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.rglob","Module":"chronos.chronos2.pipeline","Name":"rglob","Type":"method","Signature":"(self, pattern)","Arguments":"[\"self\", \"pattern\"]","Docstring":"Recursively yield all existing files (of any kind, including\ndirectories) matching the given relativ","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.rmdir","Module":"chronos.chronos2.pipeline","Name":"rmdir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Remove this directory.  The directory must be empty.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.samefile","Module":"chronos.chronos2.pipeline","Name":"samefile","Type":"method","Signature":"(self, other_path)","Arguments":"[\"self\", \"other_path\"]","Docstring":"Return whether other_path is the same or not as this file\n(as returned by os.path.samefile()).","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.stat","Module":"chronos.chronos2.pipeline","Name":"stat","Type":"method","Signature":"(self, *, follow_symlinks=True)","Arguments":"[\"self\", \"follow_symlinks\"]","Docstring":"Return the result of the stat() system call on this path, like\nos.stat() does.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.symlink_to","Module":"chronos.chronos2.pipeline","Name":"symlink_to","Type":"method","Signature":"(self, target, target_is_directory=False)","Arguments":"[\"self\", \"target\", \"target_is_directory\"]","Docstring":"Make this path a symlink pointing to the target path.\nNote the order of arguments (link, target) is ","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.touch","Module":"chronos.chronos2.pipeline","Name":"touch","Type":"method","Signature":"(self, mode=438, exist_ok=True)","Arguments":"[\"self\", \"mode\", \"exist_ok\"]","Docstring":"Create this file with the given access mode, if it doesn't exist.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.unlink","Module":"chronos.chronos2.pipeline","Name":"unlink","Type":"method","Signature":"(self, missing_ok=False)","Arguments":"[\"self\", \"missing_ok\"]","Docstring":"Remove this file or link.\nIf the path is a directory, use rmdir() instead.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.with_name","Module":"chronos.chronos2.pipeline","Name":"with_name","Type":"method","Signature":"(self, name)","Arguments":"[\"self\", \"name\"]","Docstring":"Return a new path with the file name changed.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.with_stem","Module":"chronos.chronos2.pipeline","Name":"with_stem","Type":"method","Signature":"(self, stem)","Arguments":"[\"self\", \"stem\"]","Docstring":"Return a new path with the stem changed.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.with_suffix","Module":"chronos.chronos2.pipeline","Name":"with_suffix","Type":"method","Signature":"(self, suffix)","Arguments":"[\"self\", \"suffix\"]","Docstring":"Return a new path with the file suffix changed.  If the path\nhas no suffix, add given suffix.  If th","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.write_bytes","Module":"chronos.chronos2.pipeline","Name":"write_bytes","Type":"method","Signature":"(self, data)","Arguments":"[\"self\", \"data\"]","Docstring":"Open the file in bytes mode, write to it, and close the file.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path.write_text","Module":"chronos.chronos2.pipeline","Name":"write_text","Type":"method","Signature":"(self, data, encoding=None, errors=None, newline=None)","Arguments":"[\"self\", \"data\", \"encoding\", \"errors\", \"newline\"]","Docstring":"Open the file in text mode, write to it, and close the file.","Parent":"Path"},{"Path":"chronos.chronos2.pipeline.Path","Module":"chronos.chronos2.pipeline","Name":"Path","Type":"class","Signature":"","Arguments":"[]","Docstring":"PurePath subclass that can make system calls.\n\nPath represents a filesystem path but unlike PurePath","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.Sequence","Module":"chronos.chronos2.pipeline","Name":"Sequence","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of collections.abc.Sequence.","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.TYPE_CHECKING","Module":"chronos.chronos2.pipeline","Name":"TYPE_CHECKING","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and Fa","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.TensorOrArray","Module":"chronos.chronos2.pipeline","Name":"TensorOrArray","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Represent a PEP 604 union type\n\nE.g. for int | str","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.convert_df_input_to_list_of_dicts_input","Module":"chronos.chronos2.pipeline","Name":"convert_df_input_to_list_of_dicts_input","Type":"function","Signature":"(df: 'pd.DataFrame', future_df: 'pd.DataFrame | None', target_columns: list[str], prediction_length: int, id_column: str = 'item_id', timestamp_column: str = 'timestamp', validate_inputs: bool = True, freq: str | None = None) -> tuple[list[dict[str, numpy.ndarray | dict[str, numpy.ndarray]]], numpy.ndarray, dict[str, 'pd.DatetimeIndex']]","Arguments":"[\"df\", \"future_df\", \"target_columns\", \"prediction_length\", \"id_column\", \"timestamp_column\", \"validate_inputs\", \"freq\"]","Docstring":"Convert from dataframe input format to a list of dictionaries input format.\n\nParameters\n----------\nd","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.deepcopy","Module":"chronos.chronos2.pipeline","Name":"deepcopy","Type":"function","Signature":"(x, memo=None, _nil=[])","Arguments":"[\"x\", \"memo\", \"_nil\"]","Docstring":"Deep copy operation on arbitrary Python objects.\n\nSee the module's __doc__ string for more info.","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.find_adapter_config_file","Module":"chronos.chronos2.pipeline","Name":"find_adapter_config_file","Type":"function","Signature":"(model_id: str, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, resume_download: Optional[bool] = None, proxies: Optional[dict[str, str]] = None, token: Union[bool, str, NoneType] = None, revision: Optional[str] = None, local_files_only: bool = False, subfolder: str = '', _commit_hash: Optional[str] = None) -> Optional[str]","Arguments":"[\"model_id\", \"cache_dir\", \"force_download\", \"resume_download\", \"proxies\", \"token\", \"revision\", \"local_files_only\", \"subfolder\", \"_commit_hash\"]","Docstring":"Simply checks if the model stored on the Hub or locally is an adapter model or not, return the path ","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.interpolate_quantiles","Module":"chronos.chronos2.pipeline","Name":"interpolate_quantiles","Type":"function","Signature":"(query_quantile_levels: torch.Tensor | list[float], original_quantile_levels: torch.Tensor | list[float], original_values: torch.Tensor) -> torch.Tensor","Arguments":"[\"query_quantile_levels\", \"original_quantile_levels\", \"original_values\"]","Docstring":"Interpolates quantile values at specified query levels using linear interpolation using original\nqua","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.is_peft_available","Module":"chronos.chronos2.pipeline","Name":"is_peft_available","Type":"function","Signature":"() -> Union[tuple[bool, str], bool]","Arguments":"[]","Docstring":"","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.logger","Module":"chronos.chronos2.pipeline","Name":"logger","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Instances of the Logger class represent a single logging channel. A\n\"logging channel\" indicates an a","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.rearrange","Module":"chronos.chronos2.pipeline","Name":"rearrange","Type":"function","Signature":"(tensor: Union[~Tensor, List[~Tensor]], pattern: str, **axes_lengths: Any) -> ~Tensor","Arguments":"[\"tensor\", \"pattern\", \"axes_lengths\"]","Docstring":"einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\nThis op","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.repeat","Module":"chronos.chronos2.pipeline","Name":"repeat","Type":"function","Signature":"(tensor: Union[~Tensor, List[~Tensor]], pattern: str, **axes_lengths: Any) -> ~Tensor","Arguments":"[\"tensor\", \"pattern\", \"axes_lengths\"]","Docstring":"einops.repeat allows reordering elements and repeating them in arbitrary combinations.\nThis operatio","Parent":"chronos2"},{"Path":"chronos.chronos2.pipeline.weighted_quantile","Module":"chronos.chronos2.pipeline","Name":"weighted_quantile","Type":"function","Signature":"(query_quantile_levels: torch.Tensor | list[float], sample_weights: torch.Tensor | list[float], samples: torch.Tensor)","Arguments":"[\"query_quantile_levels\", \"sample_weights\", \"samples\"]","Docstring":"Computes quantiles from a distribution specified by `samples` and their corresponding probability ma","Parent":"chronos2"},{"Path":"chronos.chronos2.model.AttentionOutput.pop","Module":"chronos.chronos2.model","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.model.AttentionOutput.setdefault","Module":"chronos.chronos2.model","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.model.AttentionOutput.to_tuple","Module":"chronos.chronos2.model","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.model.AttentionOutput.update","Module":"chronos.chronos2.model","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.model.AttentionOutput","Module":"chronos.chronos2.model","Name":"AttentionOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"AttentionOutput(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = None)","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.dict_dtype_to_str","Module":"chronos.chronos2.model","Name":"dict_dtype_to_str","Type":"method","Signature":"(self, d: dict[str, typing.Any]) -> None","Arguments":"[\"self\", \"d\"]","Docstring":"Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\nc","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.from_dict","Module":"chronos.chronos2.model","Name":"from_dict","Type":"method","Signature":"(config_dict: dict[str, typing.Any], **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"config_dict\", \"kwargs\"]","Docstring":"Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n\nArgs:\n    config_dict (","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.from_json_file","Module":"chronos.chronos2.model","Name":"from_json_file","Type":"method","Signature":"(json_file: Union[str, os.PathLike]) -> ~SpecificPretrainedConfigType","Arguments":"[\"json_file\"]","Docstring":"Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n\nArgs:\n    json_file","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.from_pretrained","Module":"chronos.chronos2.model","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"pretrained_model_name_or_path\", \"cache_dir\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n\nArgs","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.from_text_audio_configs","Module":"chronos.chronos2.model","Name":"from_text_audio_configs","Type":"method","Signature":"(text_config, audio_config, **kwargs)","Arguments":"[\"text_config\", \"audio_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and audio model\nconfig","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.from_text_vision_configs","Module":"chronos.chronos2.model","Name":"from_text_vision_configs","Type":"method","Signature":"(text_config, vision_config, **kwargs)","Arguments":"[\"text_config\", \"vision_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and vision model\nconfi","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.get_config_dict","Module":"chronos.chronos2.model","Name":"get_config_dict","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instan","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.get_text_config","Module":"chronos.chronos2.model","Name":"get_text_config","Type":"method","Signature":"(self, decoder=None, encoder=None) -> 'PretrainedConfig'","Arguments":"[\"self\", \"decoder\", \"encoder\"]","Docstring":"Returns the text config related to the text input (encoder) or text output (decoder) of the model. T","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.push_to_hub","Module":"chronos.chronos2.model","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the configuration file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.register_for_auto_class","Module":"chronos.chronos2.model","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoConfig')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom configurations as t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.save_pretrained","Module":"chronos.chronos2.model","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"push_to_hub\", \"kwargs\"]","Docstring":"Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.to_dict","Module":"chronos.chronos2.model","Name":"to_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Serializes this instance to a Python dictionary.\n\nReturns:\n    `dict[str, Any]`: Dictionary of all t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.to_diff_dict","Module":"chronos.chronos2.model","Name":"to_diff_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Removes all attributes from the configuration that correspond to the default config attributes for\nb","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.to_json_file","Module":"chronos.chronos2.model","Name":"to_json_file","Type":"method","Signature":"(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)","Arguments":"[\"self\", \"json_file_path\", \"use_diff\"]","Docstring":"Save this instance to a JSON file.\n\nArgs:\n    json_file_path (`str` or `os.PathLike`):\n        Path ","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.to_json_string","Module":"chronos.chronos2.model","Name":"to_json_string","Type":"method","Signature":"(self, use_diff: bool = True) -> str","Arguments":"[\"self\", \"use_diff\"]","Docstring":"Serializes this instance to a JSON string.\n\nArgs:\n    use_diff (`bool`, *optional*, defaults to `Tru","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.update","Module":"chronos.chronos2.model","Name":"update","Type":"method","Signature":"(self, config_dict: dict[str, typing.Any])","Arguments":"[\"self\", \"config_dict\"]","Docstring":"Updates attributes of this class with attributes from `config_dict`.\n\nArgs:\n    config_dict (`dict[s","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig.update_from_string","Module":"chronos.chronos2.model","Name":"update_from_string","Type":"method","Signature":"(self, update_str: str)","Arguments":"[\"self\", \"update_str\"]","Docstring":"Updates attributes of this class with attributes from `update_str`.\n\nThe expected format is ints, fl","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.model.Chronos2CoreConfig","Module":"chronos.chronos2.model","Name":"Chronos2CoreConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.\n\nArguments\n-------","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2Encoder.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, inputs_embeds: torch.Tensor, *, group_ids: torch.Tensor, attention_mask: torch.Tensor | None = None, position_ids: torch.Tensor | None = None, output_attentions: bool = False) -> chronos.chronos2.model.Chronos2EncoderOutput","Arguments":"[\"self\", \"inputs_embeds\", \"group_ids\", \"attention_mask\", \"position_ids\", \"output_attentions\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2Encoder"},{"Path":"chronos.chronos2.model.Chronos2Encoder","Module":"chronos.chronos2.model","Name":"Chronos2Encoder","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, *, position_ids: torch.Tensor, attention_mask: torch.Tensor, group_time_mask: torch.Tensor, output_attentions: bool = False) -> chronos.chronos2.model.Chronos2EncoderBlockOutput","Arguments":"[\"self\", \"hidden_states\", \"position_ids\", \"attention_mask\", \"group_time_mask\", \"output_attentions\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2EncoderBlock"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlock","Module":"chronos.chronos2.model","Name":"Chronos2EncoderBlock","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlockOutput.pop","Module":"chronos.chronos2.model","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"Chronos2EncoderBlockOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlockOutput.setdefault","Module":"chronos.chronos2.model","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"Chronos2EncoderBlockOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlockOutput.to_tuple","Module":"chronos.chronos2.model","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"Chronos2EncoderBlockOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlockOutput.update","Module":"chronos.chronos2.model","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"Chronos2EncoderBlockOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderBlockOutput","Module":"chronos.chronos2.model","Name":"Chronos2EncoderBlockOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"Chronos2EncoderBlockOutput(hidden_states: torch.Tensor | None = None, time_self_attn_weights: torch.","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2EncoderOutput.pop","Module":"chronos.chronos2.model","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"Chronos2EncoderOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderOutput.setdefault","Module":"chronos.chronos2.model","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"Chronos2EncoderOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderOutput.to_tuple","Module":"chronos.chronos2.model","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"Chronos2EncoderOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderOutput.update","Module":"chronos.chronos2.model","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"Chronos2EncoderOutput"},{"Path":"chronos.chronos2.model.Chronos2EncoderOutput","Module":"chronos.chronos2.model","Name":"Chronos2EncoderOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"Chronos2EncoderOutput(last_hidden_state: torch.Tensor | None = None, all_time_self_attn_weights: tup","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2ForecastingConfig.editable_fields","Module":"chronos.chronos2.model","Name":"editable_fields","Type":"method","Signature":"() -> list[str]","Arguments":"[]","Docstring":"Fields that maybe modified during the fine-tuning stage.","Parent":"Chronos2ForecastingConfig"},{"Path":"chronos.chronos2.model.Chronos2ForecastingConfig","Module":"chronos.chronos2.model","Name":"Chronos2ForecastingConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, input_","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, hidden_states)","Arguments":"[\"self\", \"hidden_states\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.model.Chronos2LayerNorm","Module":"chronos.chronos2.model","Name":"Chronos2LayerNorm","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2Model.active_adapters","Module":"chronos.chronos2.model","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.add_adapter","Module":"chronos.chronos2.model","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.add_memory_hooks","Module":"chronos.chronos2.model","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.add_model_tags","Module":"chronos.chronos2.model","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.can_generate","Module":"chronos.chronos2.model","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.create_extended_attention_mask_for_decoder","Module":"chronos.chronos2.model","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.delete_adapter","Module":"chronos.chronos2.model","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.dequantize","Module":"chronos.chronos2.model","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.disable_adapters","Module":"chronos.chronos2.model","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.disable_input_require_grads","Module":"chronos.chronos2.model","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.enable_adapters","Module":"chronos.chronos2.model","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.enable_input_require_grads","Module":"chronos.chronos2.model","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.encode","Module":"chronos.chronos2.model","Name":"encode","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False)","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.estimate_tokens","Module":"chronos.chronos2.model","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.floating_point_ops","Module":"chronos.chronos2.model","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, context: torch.Tensor, context_mask: torch.Tensor | None = None, group_ids: torch.Tensor | None = None, future_covariates: torch.Tensor | None = None, future_covariates_mask: torch.Tensor | None = None, num_output_patches: int = 1, future_target: torch.Tensor | None = None, future_target_mask: torch.Tensor | None = None, output_attentions: bool = False) -> chronos.chronos2.model.Chronos2Output","Arguments":"[\"self\", \"context\", \"context_mask\", \"group_ids\", \"future_covariates\", \"future_covariates_mask\", \"num_output_patches\", \"future_target\", \"future_target_mask\", \"output_attentions\"]","Docstring":"Forward pass of the Chronos2 model.\n\nParameters\n----------\ncontext\n    Input tensor of shape (batch_","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.from_pretrained","Module":"chronos.chronos2.model","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_adapter_state_dict","Module":"chronos.chronos2.model","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_compiled_call","Module":"chronos.chronos2.model","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_correct_attn_implementation","Module":"chronos.chronos2.model","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_decoder","Module":"chronos.chronos2.model","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_extended_attention_mask","Module":"chronos.chronos2.model","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_head_mask","Module":"chronos.chronos2.model","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_init_context","Module":"chronos.chronos2.model","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_input_embeddings","Module":"chronos.chronos2.model","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_memory_footprint","Module":"chronos.chronos2.model","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_output_embeddings","Module":"chronos.chronos2.model","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_parameter_or_buffer","Module":"chronos.chronos2.model","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_position_embeddings","Module":"chronos.chronos2.model","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.gradient_checkpointing_disable","Module":"chronos.chronos2.model","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.gradient_checkpointing_enable","Module":"chronos.chronos2.model","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.init_weights","Module":"chronos.chronos2.model","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.initialize_weights","Module":"chronos.chronos2.model","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.invert_attention_mask","Module":"chronos.chronos2.model","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.is_backend_compatible","Module":"chronos.chronos2.model","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.kernelize","Module":"chronos.chronos2.model","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.load_adapter","Module":"chronos.chronos2.model","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.num_parameters","Module":"chronos.chronos2.model","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.post_init","Module":"chronos.chronos2.model","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.prune_heads","Module":"chronos.chronos2.model","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.push_to_hub","Module":"chronos.chronos2.model","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_for_auto_class","Module":"chronos.chronos2.model","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.reset_memory_hooks_state","Module":"chronos.chronos2.model","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.resize_position_embeddings","Module":"chronos.chronos2.model","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.resize_token_embeddings","Module":"chronos.chronos2.model","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.retrieve_modules_from_names","Module":"chronos.chronos2.model","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.reverse_bettertransformer","Module":"chronos.chronos2.model","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.save_pretrained","Module":"chronos.chronos2.model","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.set_adapter","Module":"chronos.chronos2.model","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.set_attn_implementation","Module":"chronos.chronos2.model","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.set_decoder","Module":"chronos.chronos2.model","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.set_input_embeddings","Module":"chronos.chronos2.model","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.set_output_embeddings","Module":"chronos.chronos2.model","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos2.model","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.tie_weights","Module":"chronos.chronos2.model","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.to_bettertransformer","Module":"chronos.chronos2.model","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos2.model","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2Model"},{"Path":"chronos.chronos2.model.Chronos2Model","Module":"chronos.chronos2.model","Name":"Chronos2Model","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all models.\n\n[`PreTrainedModel`] takes care of storing the configuration of the model","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Chronos2Output.pop","Module":"chronos.chronos2.model","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"Chronos2Output"},{"Path":"chronos.chronos2.model.Chronos2Output.setdefault","Module":"chronos.chronos2.model","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"Chronos2Output"},{"Path":"chronos.chronos2.model.Chronos2Output.to_tuple","Module":"chronos.chronos2.model","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"Chronos2Output"},{"Path":"chronos.chronos2.model.Chronos2Output.update","Module":"chronos.chronos2.model","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"Chronos2Output"},{"Path":"chronos.chronos2.model.Chronos2Output","Module":"chronos.chronos2.model","Name":"Chronos2Output","Type":"class","Signature":"","Arguments":"[]","Docstring":"Chronos2Output(loss: torch.Tensor | None = None, quantile_preds: torch.Tensor | None = None, enc_tim","Parent":"chronos2"},{"Path":"chronos.chronos2.model.FeedForward.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"hidden_states\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"FeedForward"},{"Path":"chronos.chronos2.model.FeedForward","Module":"chronos.chronos2.model","Name":"FeedForward","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.model.GroupSelfAttention.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, output_attentions: bool = False) -> chronos.chronos2.layers.AttentionOutput","Arguments":"[\"self\", \"hidden_states\", \"attention_mask\", \"output_attentions\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.model.GroupSelfAttention","Module":"chronos.chronos2.model","Name":"GroupSelfAttention","Type":"class","Signature":"","Arguments":"[]","Docstring":"Self-attention applied along the batch axis masked by the group attention mask","Parent":"chronos2"},{"Path":"chronos.chronos2.model.InstanceNorm.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor, loc_scale: tuple[torch.Tensor, torch.Tensor] | None = None) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]","Arguments":"[\"self\", \"x\", \"loc_scale\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.inverse","Module":"chronos.chronos2.model","Name":"inverse","Type":"method","Signature":"(self, x: torch.Tensor, loc_scale: tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor","Arguments":"[\"self\", \"x\", \"loc_scale\"]","Docstring":"","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"InstanceNorm"},{"Path":"chronos.chronos2.model.InstanceNorm","Module":"chronos.chronos2.model","Name":"InstanceNorm","Type":"class","Signature":"","Arguments":"[]","Docstring":"Apply standardization along the last dimension and optionally apply arcsinh after standardization.","Parent":"chronos2"},{"Path":"chronos.chronos2.model.MHA.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, mask: torch.Tensor, encoder_states: torch.Tensor | None = None, position_ids: torch.Tensor | None = None, output_attentions: bool = False) -> chronos.chronos2.layers.AttentionOutput","Arguments":"[\"self\", \"hidden_states\", \"mask\", \"encoder_states\", \"position_ids\", \"output_attentions\"]","Docstring":"Multi-head attention forward pass.\n\nArgs:\n    hidden_states : Input tensor of shape [batch_size, seq","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"MHA"},{"Path":"chronos.chronos2.model.MHA","Module":"chronos.chronos2.model","Name":"MHA","Type":"class","Signature":"","Arguments":"[]","Docstring":"Multi-head Attention Layer","Parent":"chronos2"},{"Path":"chronos.chronos2.model.MLP.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"hidden_states\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"MLP"},{"Path":"chronos.chronos2.model.MLP","Module":"chronos.chronos2.model","Name":"MLP","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.model.ModelOutput.pop","Module":"chronos.chronos2.model","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"ModelOutput"},{"Path":"chronos.chronos2.model.ModelOutput.setdefault","Module":"chronos.chronos2.model","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"ModelOutput"},{"Path":"chronos.chronos2.model.ModelOutput.to_tuple","Module":"chronos.chronos2.model","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"ModelOutput"},{"Path":"chronos.chronos2.model.ModelOutput.update","Module":"chronos.chronos2.model","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"ModelOutput"},{"Path":"chronos.chronos2.model.ModelOutput","Module":"chronos.chronos2.model","Name":"ModelOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer o","Parent":"chronos2"},{"Path":"chronos.chronos2.model.Patch.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"x\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Patch"},{"Path":"chronos.chronos2.model.Patch","Module":"chronos.chronos2.model","Name":"Patch","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.model.PreTrainedModel.active_adapters","Module":"chronos.chronos2.model","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.add_adapter","Module":"chronos.chronos2.model","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.add_memory_hooks","Module":"chronos.chronos2.model","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.add_model_tags","Module":"chronos.chronos2.model","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.can_generate","Module":"chronos.chronos2.model","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.create_extended_attention_mask_for_decoder","Module":"chronos.chronos2.model","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.delete_adapter","Module":"chronos.chronos2.model","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.dequantize","Module":"chronos.chronos2.model","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.disable_adapters","Module":"chronos.chronos2.model","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.disable_input_require_grads","Module":"chronos.chronos2.model","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.enable_adapters","Module":"chronos.chronos2.model","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.enable_input_require_grads","Module":"chronos.chronos2.model","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.estimate_tokens","Module":"chronos.chronos2.model","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.floating_point_ops","Module":"chronos.chronos2.model","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, *input: Any) -> None","Arguments":"[\"self\", \"input\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.from_pretrained","Module":"chronos.chronos2.model","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_adapter_state_dict","Module":"chronos.chronos2.model","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_compiled_call","Module":"chronos.chronos2.model","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_correct_attn_implementation","Module":"chronos.chronos2.model","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_decoder","Module":"chronos.chronos2.model","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_extended_attention_mask","Module":"chronos.chronos2.model","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_head_mask","Module":"chronos.chronos2.model","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_init_context","Module":"chronos.chronos2.model","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_input_embeddings","Module":"chronos.chronos2.model","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_memory_footprint","Module":"chronos.chronos2.model","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_output_embeddings","Module":"chronos.chronos2.model","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_parameter_or_buffer","Module":"chronos.chronos2.model","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_position_embeddings","Module":"chronos.chronos2.model","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.gradient_checkpointing_disable","Module":"chronos.chronos2.model","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.gradient_checkpointing_enable","Module":"chronos.chronos2.model","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.init_weights","Module":"chronos.chronos2.model","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.initialize_weights","Module":"chronos.chronos2.model","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.invert_attention_mask","Module":"chronos.chronos2.model","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.is_backend_compatible","Module":"chronos.chronos2.model","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.kernelize","Module":"chronos.chronos2.model","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.load_adapter","Module":"chronos.chronos2.model","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.num_parameters","Module":"chronos.chronos2.model","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.post_init","Module":"chronos.chronos2.model","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.prune_heads","Module":"chronos.chronos2.model","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.push_to_hub","Module":"chronos.chronos2.model","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_for_auto_class","Module":"chronos.chronos2.model","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.reset_memory_hooks_state","Module":"chronos.chronos2.model","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.resize_position_embeddings","Module":"chronos.chronos2.model","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.resize_token_embeddings","Module":"chronos.chronos2.model","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.retrieve_modules_from_names","Module":"chronos.chronos2.model","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.reverse_bettertransformer","Module":"chronos.chronos2.model","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.save_pretrained","Module":"chronos.chronos2.model","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.set_adapter","Module":"chronos.chronos2.model","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.set_attn_implementation","Module":"chronos.chronos2.model","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.set_decoder","Module":"chronos.chronos2.model","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.set_input_embeddings","Module":"chronos.chronos2.model","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.set_output_embeddings","Module":"chronos.chronos2.model","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos2.model","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.tie_weights","Module":"chronos.chronos2.model","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.to_bettertransformer","Module":"chronos.chronos2.model","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos2.model","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos2.model.PreTrainedModel","Module":"chronos.chronos2.model","Name":"PreTrainedModel","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all models.\n\n[`PreTrainedModel`] takes care of storing the configuration of the model","Parent":"chronos2"},{"Path":"chronos.chronos2.model.ResidualBlock.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"x\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.model.ResidualBlock","Module":"chronos.chronos2.model","Name":"ResidualBlock","Type":"class","Signature":"","Arguments":"[]","Docstring":"A generic residual block which can be used for input and output embedding layers","Parent":"chronos2"},{"Path":"chronos.chronos2.model.TimeSelfAttention.add_module","Module":"chronos.chronos2.model","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.apply","Module":"chronos.chronos2.model","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.bfloat16","Module":"chronos.chronos2.model","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.buffers","Module":"chronos.chronos2.model","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.children","Module":"chronos.chronos2.model","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.compile","Module":"chronos.chronos2.model","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.cpu","Module":"chronos.chronos2.model","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.cuda","Module":"chronos.chronos2.model","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.double","Module":"chronos.chronos2.model","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.eval","Module":"chronos.chronos2.model","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.extra_repr","Module":"chronos.chronos2.model","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.float","Module":"chronos.chronos2.model","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.forward","Module":"chronos.chronos2.model","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.Tensor, output_attentions: bool = False) -> chronos.chronos2.layers.AttentionOutput","Arguments":"[\"self\", \"hidden_states\", \"attention_mask\", \"position_ids\", \"output_attentions\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.get_buffer","Module":"chronos.chronos2.model","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.get_extra_state","Module":"chronos.chronos2.model","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.get_parameter","Module":"chronos.chronos2.model","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.get_submodule","Module":"chronos.chronos2.model","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.half","Module":"chronos.chronos2.model","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.ipu","Module":"chronos.chronos2.model","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.load_state_dict","Module":"chronos.chronos2.model","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.modules","Module":"chronos.chronos2.model","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.mtia","Module":"chronos.chronos2.model","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.named_buffers","Module":"chronos.chronos2.model","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.named_children","Module":"chronos.chronos2.model","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.named_modules","Module":"chronos.chronos2.model","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.named_parameters","Module":"chronos.chronos2.model","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.parameters","Module":"chronos.chronos2.model","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_backward_hook","Module":"chronos.chronos2.model","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_buffer","Module":"chronos.chronos2.model","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_forward_hook","Module":"chronos.chronos2.model","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_forward_pre_hook","Module":"chronos.chronos2.model","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_full_backward_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_full_backward_pre_hook","Module":"chronos.chronos2.model","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_load_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_load_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_module","Module":"chronos.chronos2.model","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_parameter","Module":"chronos.chronos2.model","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_state_dict_post_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.register_state_dict_pre_hook","Module":"chronos.chronos2.model","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.requires_grad_","Module":"chronos.chronos2.model","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.set_extra_state","Module":"chronos.chronos2.model","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.set_submodule","Module":"chronos.chronos2.model","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.share_memory","Module":"chronos.chronos2.model","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.state_dict","Module":"chronos.chronos2.model","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.to","Module":"chronos.chronos2.model","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.to_empty","Module":"chronos.chronos2.model","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.train","Module":"chronos.chronos2.model","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.type","Module":"chronos.chronos2.model","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.xpu","Module":"chronos.chronos2.model","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention.zero_grad","Module":"chronos.chronos2.model","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.model.TimeSelfAttention","Module":"chronos.chronos2.model","Name":"TimeSelfAttention","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.model.cast","Module":"chronos.chronos2.model","Name":"cast","Type":"function","Signature":"(typ, val)","Arguments":"[\"typ\", \"val\"]","Docstring":"Cast a value to a type.\n\nThis returns the value unchanged.  To the type checker this\nsignals that th","Parent":"chronos2"},{"Path":"chronos.chronos2.model.dataclass","Module":"chronos.chronos2.model","Name":"dataclass","Type":"function","Signature":"(cls=None, \/, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)","Arguments":"[\"cls\", \"init\", \"repr\", \"eq\", \"order\", \"unsafe_hash\", \"frozen\", \"match_args\", \"kw_only\", \"slots\", \"weakref_slot\"]","Docstring":"Add dunder methods based on the fields defined in the class.\n\nExamines PEP 526 __annotations__ to de","Parent":"chronos2"},{"Path":"chronos.chronos2.model.rearrange","Module":"chronos.chronos2.model","Name":"rearrange","Type":"function","Signature":"(tensor: Union[~Tensor, List[~Tensor]], pattern: str, **axes_lengths: Any) -> ~Tensor","Arguments":"[\"tensor\", \"pattern\", \"axes_lengths\"]","Docstring":"einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\nThis op","Parent":"chronos2"},{"Path":"chronos.chronos2.model.repeat","Module":"chronos.chronos2.model","Name":"repeat","Type":"function","Signature":"(tensor: Union[~Tensor, List[~Tensor]], pattern: str, **axes_lengths: Any) -> ~Tensor","Arguments":"[\"tensor\", \"pattern\", \"axes_lengths\"]","Docstring":"einops.repeat allows reordering elements and repeating them in arbitrary combinations.\nThis operatio","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.ACT2FN","Module":"chronos.chronos2.layers","Name":"ACT2FN","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.ALL_LAYERNORM_LAYERS","Module":"chronos.chronos2.layers","Name":"ALL_LAYERNORM_LAYERS","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Built-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe a","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.AttentionOutput.pop","Module":"chronos.chronos2.layers","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.layers.AttentionOutput.setdefault","Module":"chronos.chronos2.layers","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.layers.AttentionOutput.to_tuple","Module":"chronos.chronos2.layers","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.layers.AttentionOutput.update","Module":"chronos.chronos2.layers","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"AttentionOutput"},{"Path":"chronos.chronos2.layers.AttentionOutput","Module":"chronos.chronos2.layers","Name":"AttentionOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"AttentionOutput(hidden_states: torch.Tensor | None = None, attn_weights: torch.Tensor | None = None)","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.dict_dtype_to_str","Module":"chronos.chronos2.layers","Name":"dict_dtype_to_str","Type":"method","Signature":"(self, d: dict[str, typing.Any]) -> None","Arguments":"[\"self\", \"d\"]","Docstring":"Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\nc","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.from_dict","Module":"chronos.chronos2.layers","Name":"from_dict","Type":"method","Signature":"(config_dict: dict[str, typing.Any], **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"config_dict\", \"kwargs\"]","Docstring":"Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n\nArgs:\n    config_dict (","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.from_json_file","Module":"chronos.chronos2.layers","Name":"from_json_file","Type":"method","Signature":"(json_file: Union[str, os.PathLike]) -> ~SpecificPretrainedConfigType","Arguments":"[\"json_file\"]","Docstring":"Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n\nArgs:\n    json_file","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.from_pretrained","Module":"chronos.chronos2.layers","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"pretrained_model_name_or_path\", \"cache_dir\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n\nArgs","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.from_text_audio_configs","Module":"chronos.chronos2.layers","Name":"from_text_audio_configs","Type":"method","Signature":"(text_config, audio_config, **kwargs)","Arguments":"[\"text_config\", \"audio_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and audio model\nconfig","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.from_text_vision_configs","Module":"chronos.chronos2.layers","Name":"from_text_vision_configs","Type":"method","Signature":"(text_config, vision_config, **kwargs)","Arguments":"[\"text_config\", \"vision_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and vision model\nconfi","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.get_config_dict","Module":"chronos.chronos2.layers","Name":"get_config_dict","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instan","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.get_text_config","Module":"chronos.chronos2.layers","Name":"get_text_config","Type":"method","Signature":"(self, decoder=None, encoder=None) -> 'PretrainedConfig'","Arguments":"[\"self\", \"decoder\", \"encoder\"]","Docstring":"Returns the text config related to the text input (encoder) or text output (decoder) of the model. T","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.push_to_hub","Module":"chronos.chronos2.layers","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the configuration file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.register_for_auto_class","Module":"chronos.chronos2.layers","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoConfig')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom configurations as t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.save_pretrained","Module":"chronos.chronos2.layers","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"push_to_hub\", \"kwargs\"]","Docstring":"Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.to_dict","Module":"chronos.chronos2.layers","Name":"to_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Serializes this instance to a Python dictionary.\n\nReturns:\n    `dict[str, Any]`: Dictionary of all t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.to_diff_dict","Module":"chronos.chronos2.layers","Name":"to_diff_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Removes all attributes from the configuration that correspond to the default config attributes for\nb","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.to_json_file","Module":"chronos.chronos2.layers","Name":"to_json_file","Type":"method","Signature":"(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)","Arguments":"[\"self\", \"json_file_path\", \"use_diff\"]","Docstring":"Save this instance to a JSON file.\n\nArgs:\n    json_file_path (`str` or `os.PathLike`):\n        Path ","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.to_json_string","Module":"chronos.chronos2.layers","Name":"to_json_string","Type":"method","Signature":"(self, use_diff: bool = True) -> str","Arguments":"[\"self\", \"use_diff\"]","Docstring":"Serializes this instance to a JSON string.\n\nArgs:\n    use_diff (`bool`, *optional*, defaults to `Tru","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.update","Module":"chronos.chronos2.layers","Name":"update","Type":"method","Signature":"(self, config_dict: dict[str, typing.Any])","Arguments":"[\"self\", \"config_dict\"]","Docstring":"Updates attributes of this class with attributes from `config_dict`.\n\nArgs:\n    config_dict (`dict[s","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig.update_from_string","Module":"chronos.chronos2.layers","Name":"update_from_string","Type":"method","Signature":"(self, update_str: str)","Arguments":"[\"self\", \"update_str\"]","Docstring":"Updates attributes of this class with attributes from `update_str`.\n\nThe expected format is ints, fl","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.layers.Chronos2CoreConfig","Module":"chronos.chronos2.layers","Name":"Chronos2CoreConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.\n\nArguments\n-------","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, hidden_states)","Arguments":"[\"self\", \"hidden_states\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"Chronos2LayerNorm"},{"Path":"chronos.chronos2.layers.Chronos2LayerNorm","Module":"chronos.chronos2.layers","Name":"Chronos2LayerNorm","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.FeedForward.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"hidden_states\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"FeedForward"},{"Path":"chronos.chronos2.layers.FeedForward","Module":"chronos.chronos2.layers","Name":"FeedForward","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, output_attentions: bool = False) -> chronos.chronos2.layers.AttentionOutput","Arguments":"[\"self\", \"hidden_states\", \"attention_mask\", \"output_attentions\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"GroupSelfAttention"},{"Path":"chronos.chronos2.layers.GroupSelfAttention","Module":"chronos.chronos2.layers","Name":"GroupSelfAttention","Type":"class","Signature":"","Arguments":"[]","Docstring":"Self-attention applied along the batch axis masked by the group attention mask","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.MHA.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, mask: torch.Tensor, encoder_states: torch.Tensor | None = None, position_ids: torch.Tensor | None = None, output_attentions: bool = False) -> chronos.chronos2.layers.AttentionOutput","Arguments":"[\"self\", \"hidden_states\", \"mask\", \"encoder_states\", \"position_ids\", \"output_attentions\"]","Docstring":"Multi-head attention forward pass.\n\nArgs:\n    hidden_states : Input tensor of shape [batch_size, seq","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"MHA"},{"Path":"chronos.chronos2.layers.MHA","Module":"chronos.chronos2.layers","Name":"MHA","Type":"class","Signature":"","Arguments":"[]","Docstring":"Multi-head Attention Layer","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.MLP.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"hidden_states\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"MLP"},{"Path":"chronos.chronos2.layers.MLP","Module":"chronos.chronos2.layers","Name":"MLP","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.ModelOutput.pop","Module":"chronos.chronos2.layers","Name":"pop","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n\nIf the key is ","Parent":"ModelOutput"},{"Path":"chronos.chronos2.layers.ModelOutput.setdefault","Module":"chronos.chronos2.layers","Name":"setdefault","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Insert key with a value of default if key is not in the dictionary.\n\nReturn the value for key if key","Parent":"ModelOutput"},{"Path":"chronos.chronos2.layers.ModelOutput.to_tuple","Module":"chronos.chronos2.layers","Name":"to_tuple","Type":"method","Signature":"(self) -> tuple","Arguments":"[\"self\"]","Docstring":"Convert self to a tuple containing all the attributes\/keys that are not `None`.","Parent":"ModelOutput"},{"Path":"chronos.chronos2.layers.ModelOutput.update","Module":"chronos.chronos2.layers","Name":"update","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"D.update([E, ]**F) -> None.  Update D from dict\/iterable E and F.\nIf E is present and has a .keys() ","Parent":"ModelOutput"},{"Path":"chronos.chronos2.layers.ModelOutput","Module":"chronos.chronos2.layers","Name":"ModelOutput","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer o","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.ResidualBlock.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"x\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"ResidualBlock"},{"Path":"chronos.chronos2.layers.ResidualBlock","Module":"chronos.chronos2.layers","Name":"ResidualBlock","Type":"class","Signature":"","Arguments":"[]","Docstring":"A generic residual block which can be used for input and output embedding layers","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.RoPE.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.apply_rotary_pos_emb","Module":"chronos.chronos2.layers","Name":"apply_rotary_pos_emb","Type":"method","Signature":"(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, unsqueeze_dim: int = 1) -> tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"q\", \"k\", \"cos\", \"sin\", \"unsqueeze_dim\"]","Docstring":"Applies Rotary Position Embedding to the query and key tensors.\n\nArgs:\n    q (`torch.Tensor`): The q","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, x: torch.Tensor, position_ids: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"x\", \"position_ids\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.rotate_half","Module":"chronos.chronos2.layers","Name":"rotate_half","Type":"method","Signature":"(x)","Arguments":"[\"x\"]","Docstring":"Rotates half the hidden dims of the input.","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"RoPE"},{"Path":"chronos.chronos2.layers.RoPE","Module":"chronos.chronos2.layers","Name":"RoPE","Type":"class","Signature":"","Arguments":"[]","Docstring":"Applies rotary position embeddings (RoPE) to input tensors.\n\nImplementation adapted from:\nhttps:\/\/gi","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, encoder_states: torch.Tensor, output_attentions: bool = False) -> chronos.chronos2.layers.AttentionOutput","Arguments":"[\"self\", \"hidden_states\", \"attention_mask\", \"encoder_states\", \"output_attentions\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"TimeCrossAttention"},{"Path":"chronos.chronos2.layers.TimeCrossAttention","Module":"chronos.chronos2.layers","Name":"TimeCrossAttention","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.add_module","Module":"chronos.chronos2.layers","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.apply","Module":"chronos.chronos2.layers","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.bfloat16","Module":"chronos.chronos2.layers","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.buffers","Module":"chronos.chronos2.layers","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.children","Module":"chronos.chronos2.layers","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.compile","Module":"chronos.chronos2.layers","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.cpu","Module":"chronos.chronos2.layers","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.cuda","Module":"chronos.chronos2.layers","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.double","Module":"chronos.chronos2.layers","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.eval","Module":"chronos.chronos2.layers","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.extra_repr","Module":"chronos.chronos2.layers","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.float","Module":"chronos.chronos2.layers","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.forward","Module":"chronos.chronos2.layers","Name":"forward","Type":"method","Signature":"(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, position_ids: torch.Tensor, output_attentions: bool = False) -> chronos.chronos2.layers.AttentionOutput","Arguments":"[\"self\", \"hidden_states\", \"attention_mask\", \"position_ids\", \"output_attentions\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.get_buffer","Module":"chronos.chronos2.layers","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.get_extra_state","Module":"chronos.chronos2.layers","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.get_parameter","Module":"chronos.chronos2.layers","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.get_submodule","Module":"chronos.chronos2.layers","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.half","Module":"chronos.chronos2.layers","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.ipu","Module":"chronos.chronos2.layers","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.load_state_dict","Module":"chronos.chronos2.layers","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.modules","Module":"chronos.chronos2.layers","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.mtia","Module":"chronos.chronos2.layers","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.named_buffers","Module":"chronos.chronos2.layers","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.named_children","Module":"chronos.chronos2.layers","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.named_modules","Module":"chronos.chronos2.layers","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.named_parameters","Module":"chronos.chronos2.layers","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.parameters","Module":"chronos.chronos2.layers","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_backward_hook","Module":"chronos.chronos2.layers","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_buffer","Module":"chronos.chronos2.layers","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_forward_hook","Module":"chronos.chronos2.layers","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_forward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_full_backward_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_full_backward_pre_hook","Module":"chronos.chronos2.layers","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_load_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_load_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_module","Module":"chronos.chronos2.layers","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_parameter","Module":"chronos.chronos2.layers","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_state_dict_post_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.register_state_dict_pre_hook","Module":"chronos.chronos2.layers","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.requires_grad_","Module":"chronos.chronos2.layers","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.set_extra_state","Module":"chronos.chronos2.layers","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.set_submodule","Module":"chronos.chronos2.layers","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.share_memory","Module":"chronos.chronos2.layers","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.state_dict","Module":"chronos.chronos2.layers","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.to","Module":"chronos.chronos2.layers","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.to_empty","Module":"chronos.chronos2.layers","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.train","Module":"chronos.chronos2.layers","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.type","Module":"chronos.chronos2.layers","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.xpu","Module":"chronos.chronos2.layers","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention.zero_grad","Module":"chronos.chronos2.layers","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"TimeSelfAttention"},{"Path":"chronos.chronos2.layers.TimeSelfAttention","Module":"chronos.chronos2.layers","Name":"TimeSelfAttention","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules ca","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.dataclass","Module":"chronos.chronos2.layers","Name":"dataclass","Type":"function","Signature":"(cls=None, \/, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)","Arguments":"[\"cls\", \"init\", \"repr\", \"eq\", \"order\", \"unsafe_hash\", \"frozen\", \"match_args\", \"kw_only\", \"slots\", \"weakref_slot\"]","Docstring":"Add dunder methods based on the fields defined in the class.\n\nExamines PEP 526 __annotations__ to de","Parent":"chronos2"},{"Path":"chronos.chronos2.layers.rearrange","Module":"chronos.chronos2.layers","Name":"rearrange","Type":"function","Signature":"(tensor: Union[~Tensor, List[~Tensor]], pattern: str, **axes_lengths: Any) -> ~Tensor","Arguments":"[\"tensor\", \"pattern\", \"axes_lengths\"]","Docstring":"einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\nThis op","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.Chronos2Dataset.convert_inputs","Module":"chronos.chronos2.dataset","Name":"convert_inputs","Type":"method","Signature":"(inputs: Union[torch.Tensor, numpy.ndarray, Sequence[torch.Tensor | numpy.ndarray], Sequence[Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray | None]]]]], context_length: int, prediction_length: int, batch_size: int, output_patch_size: int, min_past: int = 1, mode: str | chronos.chronos2.dataset.DatasetMode = <DatasetMode.TRAIN: 'train'>) -> 'Chronos2Dataset'","Arguments":"[\"inputs\", \"context_length\", \"prediction_length\", \"batch_size\", \"output_patch_size\", \"min_past\", \"mode\"]","Docstring":"Convert from different input formats to a Chronos2Dataset.","Parent":"Chronos2Dataset"},{"Path":"chronos.chronos2.dataset.Chronos2Dataset","Module":"chronos.chronos2.dataset","Name":"Chronos2Dataset","Type":"class","Signature":"","Arguments":"[]","Docstring":"A dataset wrapper for Chronos-2 models.\n\nArguments\n----------\ninputs\n    Time series data. Must be a","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.DatasetMode","Module":"chronos.chronos2.dataset","Name":"DatasetMode","Type":"class","Signature":"","Arguments":"[]","Docstring":"str(object='') -> str\nstr(bytes_or_buffer[, encoding[, errors]]) -> str\n\nCreate a new string object ","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.Enum","Module":"chronos.chronos2.dataset","Name":"Enum","Type":"class","Signature":"","Arguments":"[]","Docstring":"Create a collection of name\/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED =","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.IterableDataset","Module":"chronos.chronos2.dataset","Name":"IterableDataset","Type":"class","Signature":"","Arguments":"[]","Docstring":"An iterable Dataset.\n\nAll datasets that represent an iterable of data samples should subclass it.\nSu","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.Iterator","Module":"chronos.chronos2.dataset","Name":"Iterator","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of collections.abc.Iterator.","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.Mapping","Module":"chronos.chronos2.dataset","Name":"Mapping","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of collections.abc.Mapping.","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.fit","Module":"chronos.chronos2.dataset","Name":"fit","Type":"method","Signature":"(self, X, y=None)","Arguments":"[\"self\", \"X\", \"y\"]","Docstring":"Fit the OrdinalEncoder to X.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.fit_transform","Module":"chronos.chronos2.dataset","Name":"fit_transform","Type":"method","Signature":"(self, X, y=None, **fit_params)","Arguments":"[\"self\", \"X\", \"y\", \"fit_params\"]","Docstring":"Fit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_param","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.get_feature_names_out","Module":"chronos.chronos2.dataset","Name":"get_feature_names_out","Type":"method","Signature":"(self, input_features=None)","Arguments":"[\"self\", \"input_features\"]","Docstring":"Get output feature names for transformation.\n\nParameters\n----------\ninput_features : array-like of s","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.get_metadata_routing","Module":"chronos.chronos2.dataset","Name":"get_metadata_routing","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the r","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.get_params","Module":"chronos.chronos2.dataset","Name":"get_params","Type":"method","Signature":"(self, deep=True)","Arguments":"[\"self\", \"deep\"]","Docstring":"Get parameters for this estimator.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, wil","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.inverse_transform","Module":"chronos.chronos2.dataset","Name":"inverse_transform","Type":"method","Signature":"(self, X)","Arguments":"[\"self\", \"X\"]","Docstring":"Convert the data back to the original representation.\n\nParameters\n----------\nX : array-like of shape","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.set_output","Module":"chronos.chronos2.dataset","Name":"set_output","Type":"method","Signature":"(self, *, transform=None)","Arguments":"[\"self\", \"transform\"]","Docstring":"Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an exa","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.set_params","Module":"chronos.chronos2.dataset","Name":"set_params","Type":"method","Signature":"(self, **params)","Arguments":"[\"self\", \"params\"]","Docstring":"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested ob","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder.transform","Module":"chronos.chronos2.dataset","Name":"transform","Type":"method","Signature":"(self, X)","Arguments":"[\"self\", \"X\"]","Docstring":"Transform X to ordinal codes.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)","Parent":"OrdinalEncoder"},{"Path":"chronos.chronos2.dataset.OrdinalEncoder","Module":"chronos.chronos2.dataset","Name":"OrdinalEncoder","Type":"class","Signature":"","Arguments":"[]","Docstring":"Encode categorical features as an integer array.\n\nThe input to this transformer should be an array-l","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.Sequence","Module":"chronos.chronos2.dataset","Name":"Sequence","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of collections.abc.Sequence.","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.TYPE_CHECKING","Module":"chronos.chronos2.dataset","Name":"TYPE_CHECKING","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and Fa","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.TargetEncoder.fit","Module":"chronos.chronos2.dataset","Name":"fit","Type":"method","Signature":"(self, X, y)","Arguments":"[\"self\", \"X\", \"y\"]","Docstring":"Fit the :class:`TargetEncoder` to X and y.\n\nIt is discouraged to use this method because it can intr","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder.fit_transform","Module":"chronos.chronos2.dataset","Name":"fit_transform","Type":"method","Signature":"(self, X, y)","Arguments":"[\"self\", \"X\", \"y\"]","Docstring":"Fit :class:`TargetEncoder` and transform `X` with the target encoding.\n\nThis method uses a :term:`cr","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder.get_feature_names_out","Module":"chronos.chronos2.dataset","Name":"get_feature_names_out","Type":"method","Signature":"(self, input_features=None)","Arguments":"[\"self\", \"input_features\"]","Docstring":"Get output feature names for transformation.\n\nParameters\n----------\ninput_features : array-like of s","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder.get_metadata_routing","Module":"chronos.chronos2.dataset","Name":"get_metadata_routing","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Get metadata routing of this object.\n\nPlease check :ref:`User Guide <metadata_routing>` on how the r","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder.get_params","Module":"chronos.chronos2.dataset","Name":"get_params","Type":"method","Signature":"(self, deep=True)","Arguments":"[\"self\", \"deep\"]","Docstring":"Get parameters for this estimator.\n\nParameters\n----------\ndeep : bool, default=True\n    If True, wil","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder.set_output","Module":"chronos.chronos2.dataset","Name":"set_output","Type":"method","Signature":"(self, *, transform=None)","Arguments":"[\"self\", \"transform\"]","Docstring":"Set output container.\n\nSee :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\nfor an exa","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder.set_params","Module":"chronos.chronos2.dataset","Name":"set_params","Type":"method","Signature":"(self, **params)","Arguments":"[\"self\", \"params\"]","Docstring":"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested ob","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder.transform","Module":"chronos.chronos2.dataset","Name":"transform","Type":"method","Signature":"(self, X)","Arguments":"[\"self\", \"X\"]","Docstring":"Transform X with the target encoding.\n\nThis method internally uses the `encodings_` attribute learnt","Parent":"TargetEncoder"},{"Path":"chronos.chronos2.dataset.TargetEncoder","Module":"chronos.chronos2.dataset","Name":"TargetEncoder","Type":"class","Signature":"","Arguments":"[]","Docstring":"Target Encoder for regression and classification targets.\n\nEach category is encoded based on a shrun","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.TensorOrArray","Module":"chronos.chronos2.dataset","Name":"TensorOrArray","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Represent a PEP 604 union type\n\nE.g. for int | str","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.TypeAlias","Module":"chronos.chronos2.dataset","Name":"TypeAlias","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Special form for marking type aliases.\n\nUse TypeAlias to indicate that an assignment should\nbe recog","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.cast","Module":"chronos.chronos2.dataset","Name":"cast","Type":"function","Signature":"(typ, val)","Arguments":"[\"typ\", \"val\"]","Docstring":"Cast a value to a type.\n\nThis returns the value unchanged.  To the type checker this\nsignals that th","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.convert_fev_window_to_list_of_dicts_input","Module":"chronos.chronos2.dataset","Name":"convert_fev_window_to_list_of_dicts_input","Type":"function","Signature":"(window: 'fev.EvaluationWindow', as_univariate: bool) -> tuple[list[dict[str, numpy.ndarray | dict[str, numpy.ndarray]]], list[str], list[str], list[str]]","Arguments":"[\"window\", \"as_univariate\"]","Docstring":"","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.convert_list_of_tensors_input_to_list_of_dicts_input","Module":"chronos.chronos2.dataset","Name":"convert_list_of_tensors_input_to_list_of_dicts_input","Type":"function","Signature":"(list_of_tensors: Sequence[torch.Tensor | numpy.ndarray]) -> list[dict[str, torch.Tensor]]","Arguments":"[\"list_of_tensors\"]","Docstring":"Convert a list of tensors input format to a list of dictionaries input format.\n\n\nParameters\n--------","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.convert_tensor_input_to_list_of_dicts_input","Module":"chronos.chronos2.dataset","Name":"convert_tensor_input_to_list_of_dicts_input","Type":"function","Signature":"(tensor: torch.Tensor | numpy.ndarray) -> list[dict[str, torch.Tensor]]","Arguments":"[\"tensor\"]","Docstring":"Convert a tensor input format to a list of dictionaries input format.\n\nParameters\n----------\ntensor\n","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.left_pad_and_cat_2D","Module":"chronos.chronos2.dataset","Name":"left_pad_and_cat_2D","Type":"function","Signature":"(tensors: list[torch.Tensor]) -> torch.Tensor","Arguments":"[\"tensors\"]","Docstring":"Left pads tensors in the list to the length of the longest tensor along the second axis, then concat","Parent":"chronos2"},{"Path":"chronos.chronos2.dataset.validate_and_prepare_single_dict_task","Module":"chronos.chronos2.dataset","Name":"validate_and_prepare_single_dict_task","Type":"function","Signature":"(task: Mapping[str, Union[torch.Tensor, numpy.ndarray, Mapping[str, torch.Tensor | numpy.ndarray]]], idx: int, prediction_length: int) -> tuple[torch.Tensor, torch.Tensor, int, int, int]","Arguments":"[\"task\", \"idx\", \"prediction_length\"]","Docstring":"Validates and prepares a single dictionary task for Chronos2Model.\n\nParameters\n----------\ntask\n    A","Parent":"chronos2"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.dict_dtype_to_str","Module":"chronos.chronos2.config","Name":"dict_dtype_to_str","Type":"method","Signature":"(self, d: dict[str, typing.Any]) -> None","Arguments":"[\"self\", \"d\"]","Docstring":"Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\nc","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.from_dict","Module":"chronos.chronos2.config","Name":"from_dict","Type":"method","Signature":"(config_dict: dict[str, typing.Any], **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"config_dict\", \"kwargs\"]","Docstring":"Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n\nArgs:\n    config_dict (","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.from_json_file","Module":"chronos.chronos2.config","Name":"from_json_file","Type":"method","Signature":"(json_file: Union[str, os.PathLike]) -> ~SpecificPretrainedConfigType","Arguments":"[\"json_file\"]","Docstring":"Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n\nArgs:\n    json_file","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.from_pretrained","Module":"chronos.chronos2.config","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"pretrained_model_name_or_path\", \"cache_dir\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n\nArgs","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.from_text_audio_configs","Module":"chronos.chronos2.config","Name":"from_text_audio_configs","Type":"method","Signature":"(text_config, audio_config, **kwargs)","Arguments":"[\"text_config\", \"audio_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and audio model\nconfig","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.from_text_vision_configs","Module":"chronos.chronos2.config","Name":"from_text_vision_configs","Type":"method","Signature":"(text_config, vision_config, **kwargs)","Arguments":"[\"text_config\", \"vision_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and vision model\nconfi","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.get_config_dict","Module":"chronos.chronos2.config","Name":"get_config_dict","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instan","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.get_text_config","Module":"chronos.chronos2.config","Name":"get_text_config","Type":"method","Signature":"(self, decoder=None, encoder=None) -> 'PretrainedConfig'","Arguments":"[\"self\", \"decoder\", \"encoder\"]","Docstring":"Returns the text config related to the text input (encoder) or text output (decoder) of the model. T","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.push_to_hub","Module":"chronos.chronos2.config","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the configuration file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.register_for_auto_class","Module":"chronos.chronos2.config","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoConfig')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom configurations as t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.save_pretrained","Module":"chronos.chronos2.config","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"push_to_hub\", \"kwargs\"]","Docstring":"Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.to_dict","Module":"chronos.chronos2.config","Name":"to_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Serializes this instance to a Python dictionary.\n\nReturns:\n    `dict[str, Any]`: Dictionary of all t","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.to_diff_dict","Module":"chronos.chronos2.config","Name":"to_diff_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Removes all attributes from the configuration that correspond to the default config attributes for\nb","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.to_json_file","Module":"chronos.chronos2.config","Name":"to_json_file","Type":"method","Signature":"(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)","Arguments":"[\"self\", \"json_file_path\", \"use_diff\"]","Docstring":"Save this instance to a JSON file.\n\nArgs:\n    json_file_path (`str` or `os.PathLike`):\n        Path ","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.to_json_string","Module":"chronos.chronos2.config","Name":"to_json_string","Type":"method","Signature":"(self, use_diff: bool = True) -> str","Arguments":"[\"self\", \"use_diff\"]","Docstring":"Serializes this instance to a JSON string.\n\nArgs:\n    use_diff (`bool`, *optional*, defaults to `Tru","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.update","Module":"chronos.chronos2.config","Name":"update","Type":"method","Signature":"(self, config_dict: dict[str, typing.Any])","Arguments":"[\"self\", \"config_dict\"]","Docstring":"Updates attributes of this class with attributes from `config_dict`.\n\nArgs:\n    config_dict (`dict[s","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig.update_from_string","Module":"chronos.chronos2.config","Name":"update_from_string","Type":"method","Signature":"(self, update_str: str)","Arguments":"[\"self\", \"update_str\"]","Docstring":"Updates attributes of this class with attributes from `update_str`.\n\nThe expected format is ints, fl","Parent":"Chronos2CoreConfig"},{"Path":"chronos.chronos2.config.Chronos2CoreConfig","Module":"chronos.chronos2.config","Name":"Chronos2CoreConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"HF transformers-style pretrained model config for Chronos-2.0, based on T5Config.\n\nArguments\n-------","Parent":"chronos2"},{"Path":"chronos.chronos2.config.Chronos2ForecastingConfig.editable_fields","Module":"chronos.chronos2.config","Name":"editable_fields","Type":"method","Signature":"() -> list[str]","Arguments":"[]","Docstring":"Fields that maybe modified during the fine-tuning stage.","Parent":"Chronos2ForecastingConfig"},{"Path":"chronos.chronos2.config.Chronos2ForecastingConfig","Module":"chronos.chronos2.config","Name":"Chronos2ForecastingConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"Chronos2ForecastingConfig(context_length: int, output_patch_size: int, input_patch_size: int, input_","Parent":"chronos2"},{"Path":"chronos.chronos2.config.List","Module":"chronos.chronos2.config","Name":"List","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of list.","Parent":"chronos2"},{"Path":"chronos.chronos2.config.Literal","Module":"chronos.chronos2.config","Name":"Literal","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Special typing form to define literal types (a.k.a. value types).\n\nThis form can be used to indicate","Parent":"chronos2"},{"Path":"chronos.chronos2.config.PretrainedConfig.dict_dtype_to_str","Module":"chronos.chronos2.config","Name":"dict_dtype_to_str","Type":"method","Signature":"(self, d: dict[str, typing.Any]) -> None","Arguments":"[\"self\", \"d\"]","Docstring":"Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\nc","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.from_dict","Module":"chronos.chronos2.config","Name":"from_dict","Type":"method","Signature":"(config_dict: dict[str, typing.Any], **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"config_dict\", \"kwargs\"]","Docstring":"Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n\nArgs:\n    config_dict (","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.from_json_file","Module":"chronos.chronos2.config","Name":"from_json_file","Type":"method","Signature":"(json_file: Union[str, os.PathLike]) -> ~SpecificPretrainedConfigType","Arguments":"[\"json_file\"]","Docstring":"Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.\n\nArgs:\n    json_file","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.from_pretrained","Module":"chronos.chronos2.config","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> ~SpecificPretrainedConfigType","Arguments":"[\"pretrained_model_name_or_path\", \"cache_dir\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\n\nArgs","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.from_text_audio_configs","Module":"chronos.chronos2.config","Name":"from_text_audio_configs","Type":"method","Signature":"(text_config, audio_config, **kwargs)","Arguments":"[\"text_config\", \"audio_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and audio model\nconfig","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.from_text_vision_configs","Module":"chronos.chronos2.config","Name":"from_text_vision_configs","Type":"method","Signature":"(text_config, vision_config, **kwargs)","Arguments":"[\"text_config\", \"vision_config\", \"kwargs\"]","Docstring":"Instantiate a model config (or a derived class) from text model configuration and vision model\nconfi","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.get_config_dict","Module":"chronos.chronos2.config","Name":"get_config_dict","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> tuple[dict[str, typing.Any], dict[str, typing.Any]]","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instan","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.get_text_config","Module":"chronos.chronos2.config","Name":"get_text_config","Type":"method","Signature":"(self, decoder=None, encoder=None) -> 'PretrainedConfig'","Arguments":"[\"self\", \"decoder\", \"encoder\"]","Docstring":"Returns the text config related to the text input (encoder) or text output (decoder) of the model. T","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.push_to_hub","Module":"chronos.chronos2.config","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the configuration file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.register_for_auto_class","Module":"chronos.chronos2.config","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoConfig')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom configurations as t","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.save_pretrained","Module":"chronos.chronos2.config","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"push_to_hub\", \"kwargs\"]","Docstring":"Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.to_dict","Module":"chronos.chronos2.config","Name":"to_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Serializes this instance to a Python dictionary.\n\nReturns:\n    `dict[str, Any]`: Dictionary of all t","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.to_diff_dict","Module":"chronos.chronos2.config","Name":"to_diff_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Removes all attributes from the configuration that correspond to the default config attributes for\nb","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.to_json_file","Module":"chronos.chronos2.config","Name":"to_json_file","Type":"method","Signature":"(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)","Arguments":"[\"self\", \"json_file_path\", \"use_diff\"]","Docstring":"Save this instance to a JSON file.\n\nArgs:\n    json_file_path (`str` or `os.PathLike`):\n        Path ","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.to_json_string","Module":"chronos.chronos2.config","Name":"to_json_string","Type":"method","Signature":"(self, use_diff: bool = True) -> str","Arguments":"[\"self\", \"use_diff\"]","Docstring":"Serializes this instance to a JSON string.\n\nArgs:\n    use_diff (`bool`, *optional*, defaults to `Tru","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.update","Module":"chronos.chronos2.config","Name":"update","Type":"method","Signature":"(self, config_dict: dict[str, typing.Any])","Arguments":"[\"self\", \"config_dict\"]","Docstring":"Updates attributes of this class with attributes from `config_dict`.\n\nArgs:\n    config_dict (`dict[s","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig.update_from_string","Module":"chronos.chronos2.config","Name":"update_from_string","Type":"method","Signature":"(self, update_str: str)","Arguments":"[\"self\", \"update_str\"]","Docstring":"Updates attributes of this class with attributes from `update_str`.\n\nThe expected format is ints, fl","Parent":"PretrainedConfig"},{"Path":"chronos.chronos2.config.PretrainedConfig","Module":"chronos.chronos2.config","Name":"PretrainedConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all configuration classes. Handles a few parameters common to all models' configurati","Parent":"chronos2"},{"Path":"chronos.chronos2.config.dataclass","Module":"chronos.chronos2.config","Name":"dataclass","Type":"function","Signature":"(cls=None, \/, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)","Arguments":"[\"cls\", \"init\", \"repr\", \"eq\", \"order\", \"unsafe_hash\", \"frozen\", \"match_args\", \"kw_only\", \"slots\", \"weakref_slot\"]","Docstring":"Add dunder methods based on the fields defined in the class.\n\nExamines PEP 526 __annotations__ to de","Parent":"chronos2"},{"Path":"chronos.chronos.Any","Module":"chronos.chronos","Name":"Any","Type":"class","Signature":"","Arguments":"[]","Docstring":"Special type indicating an unconstrained type.\n\n- Any is compatible with every type.\n- Any assumed t","Parent":"chronos"},{"Path":"chronos.chronos.AutoConfig.for_model","Module":"chronos.chronos","Name":"for_model","Type":"method","Signature":"(model_type: str, *args, **kwargs) -> transformers.configuration_utils.PretrainedConfig","Arguments":"[\"model_type\", \"args\", \"kwargs\"]","Docstring":"","Parent":"AutoConfig"},{"Path":"chronos.chronos.AutoConfig.from_pretrained","Module":"chronos.chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike[str]], **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"kwargs\"]","Docstring":"Instantiate one of the configuration classes of the library from a pretrained model configuration.\n\n","Parent":"AutoConfig"},{"Path":"chronos.chronos.AutoConfig.register","Module":"chronos.chronos","Name":"register","Type":"method","Signature":"(model_type, config, exist_ok=False) -> None","Arguments":"[\"model_type\", \"config\", \"exist_ok\"]","Docstring":"Register a new configuration for this class.\n\nArgs:\n    model_type (`str`): The model type like \"ber","Parent":"AutoConfig"},{"Path":"chronos.chronos.AutoConfig","Module":"chronos.chronos","Name":"AutoConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"This is a generic configuration class that will be instantiated as one of the configuration classes ","Parent":"chronos"},{"Path":"chronos.chronos.AutoModelForCausalLM.from_config","Module":"chronos.chronos","Name":"from_config","Type":"method","Signature":"(**kwargs)","Arguments":"[\"kwargs\"]","Docstring":"Instantiates one of the model classes of the library (with a causal language modeling head) from a c","Parent":"AutoModelForCausalLM"},{"Path":"chronos.chronos.AutoModelForCausalLM.from_pretrained","Module":"chronos.chronos","Name":"from_pretrained","Type":"method","Signature":"(*model_args, **kwargs)","Arguments":"[\"model_args\", \"kwargs\"]","Docstring":"Instantiate one of the model classes of the library (with a causal language modeling head) from a pr","Parent":"AutoModelForCausalLM"},{"Path":"chronos.chronos.AutoModelForCausalLM.register","Module":"chronos.chronos","Name":"register","Type":"method","Signature":"(config_class, model_class, exist_ok=False) -> None","Arguments":"[\"config_class\", \"model_class\", \"exist_ok\"]","Docstring":"Register a new model for this class.\n\nArgs:\n    config_class ([`PretrainedConfig`]):\n        The con","Parent":"AutoModelForCausalLM"},{"Path":"chronos.chronos.AutoModelForCausalLM","Module":"chronos.chronos","Name":"AutoModelForCausalLM","Type":"class","Signature":"","Arguments":"[]","Docstring":"This is a generic model class that will be instantiated as one of the model classes of the library (","Parent":"chronos"},{"Path":"chronos.chronos.AutoModelForSeq2SeqLM.from_config","Module":"chronos.chronos","Name":"from_config","Type":"method","Signature":"(**kwargs)","Arguments":"[\"kwargs\"]","Docstring":"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling ","Parent":"AutoModelForSeq2SeqLM"},{"Path":"chronos.chronos.AutoModelForSeq2SeqLM.from_pretrained","Module":"chronos.chronos","Name":"from_pretrained","Type":"method","Signature":"(*model_args, **kwargs)","Arguments":"[\"model_args\", \"kwargs\"]","Docstring":"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling h","Parent":"AutoModelForSeq2SeqLM"},{"Path":"chronos.chronos.AutoModelForSeq2SeqLM.register","Module":"chronos.chronos","Name":"register","Type":"method","Signature":"(config_class, model_class, exist_ok=False) -> None","Arguments":"[\"config_class\", \"model_class\", \"exist_ok\"]","Docstring":"Register a new model for this class.\n\nArgs:\n    config_class ([`PretrainedConfig`]):\n        The con","Parent":"AutoModelForSeq2SeqLM"},{"Path":"chronos.chronos.AutoModelForSeq2SeqLM","Module":"chronos.chronos","Name":"AutoModelForSeq2SeqLM","Type":"class","Signature":"","Arguments":"[]","Docstring":"This is a generic model class that will be instantiated as one of the model classes of the library (","Parent":"chronos"},{"Path":"chronos.chronos.BaseChronosPipeline.from_pretrained","Module":"chronos.chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, pathlib.Path], *model_args, force_s3_download=False, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"force_s3_download\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix, or from the HuggingFace Hub.\nSupports the same ","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos.BaseChronosPipeline.predict","Module":"chronos.chronos","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None)","Arguments":"[\"self\", \"inputs\", \"prediction_length\"]","Docstring":"Get forecasts for the given time series. Predictions will be\nreturned in fp32 on the cpu.\n\nParameter","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos.BaseChronosPipeline.predict_df","Module":"chronos.chronos","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos.BaseChronosPipeline.predict_fev","Module":"chronos.chronos","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos.BaseChronosPipeline.predict_quantiles","Module":"chronos.chronos","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"kwargs\"]","Docstring":"Get quantile and mean forecasts for given time series.\nPredictions will be returned in fp32 on the c","Parent":"BaseChronosPipeline"},{"Path":"chronos.chronos.BaseChronosPipeline","Module":"chronos.chronos","Name":"BaseChronosPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos"},{"Path":"chronos.chronos.ChronosConfig.create_tokenizer","Module":"chronos.chronos","Name":"create_tokenizer","Type":"method","Signature":"(self) -> 'ChronosTokenizer'","Arguments":"[\"self\"]","Docstring":"","Parent":"ChronosConfig"},{"Path":"chronos.chronos.ChronosConfig","Module":"chronos.chronos","Name":"ChronosConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"This class holds all the configuration parameters to be used\nby ``ChronosTokenizer`` and ``ChronosMo","Parent":"chronos"},{"Path":"chronos.chronos.ChronosModel.add_module","Module":"chronos.chronos","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.apply","Module":"chronos.chronos","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.bfloat16","Module":"chronos.chronos","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.buffers","Module":"chronos.chronos","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.children","Module":"chronos.chronos","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.compile","Module":"chronos.chronos","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.cpu","Module":"chronos.chronos","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.cuda","Module":"chronos.chronos","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.double","Module":"chronos.chronos","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.encode","Module":"chronos.chronos","Name":"encode","Type":"method","Signature":"(self, input_ids: torch.Tensor, attention_mask: torch.Tensor)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Extract the encoder embedding for the given token sequences.\n\nParameters\n----------\ninput_ids\n    Te","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.eval","Module":"chronos.chronos","Name":"eval","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.extra_repr","Module":"chronos.chronos","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.float","Module":"chronos.chronos","Name":"float","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.forward","Module":"chronos.chronos","Name":"forward","Type":"method","Signature":"(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, prediction_length: Optional[int] = None, num_samples: Optional[int] = None, temperature: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None) -> torch.Tensor","Arguments":"[\"self\", \"input_ids\", \"attention_mask\", \"prediction_length\", \"num_samples\", \"temperature\", \"top_k\", \"top_p\"]","Docstring":"Predict future sample tokens for the given token sequences.\n\nArguments ``prediction_length``, ``num_","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.get_buffer","Module":"chronos.chronos","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.get_extra_state","Module":"chronos.chronos","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.get_parameter","Module":"chronos.chronos","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.get_submodule","Module":"chronos.chronos","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.half","Module":"chronos.chronos","Name":"half","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.ipu","Module":"chronos.chronos","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.load_state_dict","Module":"chronos.chronos","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.modules","Module":"chronos.chronos","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.mtia","Module":"chronos.chronos","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.named_buffers","Module":"chronos.chronos","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.named_children","Module":"chronos.chronos","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.named_modules","Module":"chronos.chronos","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.named_parameters","Module":"chronos.chronos","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.parameters","Module":"chronos.chronos","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_backward_hook","Module":"chronos.chronos","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_buffer","Module":"chronos.chronos","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_forward_hook","Module":"chronos.chronos","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_forward_pre_hook","Module":"chronos.chronos","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_full_backward_hook","Module":"chronos.chronos","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_full_backward_pre_hook","Module":"chronos.chronos","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_load_state_dict_post_hook","Module":"chronos.chronos","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_load_state_dict_pre_hook","Module":"chronos.chronos","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_module","Module":"chronos.chronos","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_parameter","Module":"chronos.chronos","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_state_dict_post_hook","Module":"chronos.chronos","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.register_state_dict_pre_hook","Module":"chronos.chronos","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.requires_grad_","Module":"chronos.chronos","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.set_extra_state","Module":"chronos.chronos","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.set_submodule","Module":"chronos.chronos","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.share_memory","Module":"chronos.chronos","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.state_dict","Module":"chronos.chronos","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.to","Module":"chronos.chronos","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.to_empty","Module":"chronos.chronos","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.train","Module":"chronos.chronos","Name":"train","Type":"method","Signature":"(self, mode: bool = True) -> Self","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.type","Module":"chronos.chronos","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.xpu","Module":"chronos.chronos","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel.zero_grad","Module":"chronos.chronos","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"ChronosModel"},{"Path":"chronos.chronos.ChronosModel","Module":"chronos.chronos","Name":"ChronosModel","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosModel`` wraps a ``PreTrainedModel`` object from ``transformers``\nand uses it to predict s","Parent":"chronos"},{"Path":"chronos.chronos.ChronosPipeline.embed","Module":"chronos.chronos","Name":"embed","Type":"method","Signature":"(self, context: Union[torch.Tensor, List[torch.Tensor]]) -> Tuple[torch.Tensor, Any]","Arguments":"[\"self\", \"context\"]","Docstring":"Get encoder embeddings for the given time series.\n\nParameters\n----------\ncontext\n    Input series. T","Parent":"ChronosPipeline"},{"Path":"chronos.chronos.ChronosPipeline.from_pretrained","Module":"chronos.chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path, *args, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"args\", \"kwargs\"]","Docstring":"Load the model, either from a local path S3 prefix or from the HuggingFace Hub.\nSupports the same ar","Parent":"ChronosPipeline"},{"Path":"chronos.chronos.ChronosPipeline.predict","Module":"chronos.chronos","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, num_samples: Optional[int] = None, temperature: Optional[float] = None, top_k: Optional[int] = None, top_p: Optional[float] = None, limit_prediction_length: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"num_samples\", \"temperature\", \"top_k\", \"top_p\", \"limit_prediction_length\"]","Docstring":"Get forecasts for the given time series.\n\nRefer to the base method (``BaseChronosPipeline.predict``)","Parent":"ChronosPipeline"},{"Path":"chronos.chronos.ChronosPipeline.predict_df","Module":"chronos.chronos","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"ChronosPipeline"},{"Path":"chronos.chronos.ChronosPipeline.predict_fev","Module":"chronos.chronos","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"ChronosPipeline"},{"Path":"chronos.chronos.ChronosPipeline.predict_quantiles","Module":"chronos.chronos","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **predict_kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"predict_kwargs\"]","Docstring":"Refer to the base method (``BaseChronosPipeline.predict_quantiles``).","Parent":"ChronosPipeline"},{"Path":"chronos.chronos.ChronosPipeline","Module":"chronos.chronos","Name":"ChronosPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosPipeline`` uses the given tokenizer and model to forecast\ninput time series.\n\nUse the ``f","Parent":"chronos"},{"Path":"chronos.chronos.ChronosTokenizer.context_input_transform","Module":"chronos.chronos","Name":"context_input_transform","Type":"method","Signature":"(self, context: torch.Tensor) -> Tuple","Arguments":"[\"self\", \"context\"]","Docstring":"Turn a batch of time series into token IDs, attention map, and tokenizer_state.\n\nParameters\n--------","Parent":"ChronosTokenizer"},{"Path":"chronos.chronos.ChronosTokenizer.label_input_transform","Module":"chronos.chronos","Name":"label_input_transform","Type":"method","Signature":"(self, label: torch.Tensor, tokenizer_state: Any) -> Tuple","Arguments":"[\"self\", \"label\", \"tokenizer_state\"]","Docstring":"Turn a batch of label slices of time series into token IDs and attention map\nusing the ``tokenizer_s","Parent":"ChronosTokenizer"},{"Path":"chronos.chronos.ChronosTokenizer.output_transform","Module":"chronos.chronos","Name":"output_transform","Type":"method","Signature":"(self, samples: torch.Tensor, tokenizer_state: Any) -> torch.Tensor","Arguments":"[\"self\", \"samples\", \"tokenizer_state\"]","Docstring":"Turn a batch of sample token IDs into real values.\n\nParameters\n----------\nsamples\n    A tensor of in","Parent":"ChronosTokenizer"},{"Path":"chronos.chronos.ChronosTokenizer","Module":"chronos.chronos","Name":"ChronosTokenizer","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosTokenizer`` defines how time series are mapped into token IDs\nand back.\n\nFor details, see","Parent":"chronos"},{"Path":"chronos.chronos.Dict","Module":"chronos.chronos","Name":"Dict","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of dict.","Parent":"chronos"},{"Path":"chronos.chronos.ForecastType","Module":"chronos.chronos","Name":"ForecastType","Type":"class","Signature":"","Arguments":"[]","Docstring":"Create a collection of name\/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED =","Parent":"chronos"},{"Path":"chronos.chronos.GenerationConfig.dict_dtype_to_str","Module":"chronos.chronos","Name":"dict_dtype_to_str","Type":"method","Signature":"(self, d: dict[str, typing.Any]) -> None","Arguments":"[\"self\", \"d\"]","Docstring":"Checks whether the passed dictionary and its nested dicts have a *dtype* key and if it's not None,\nc","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.from_dict","Module":"chronos.chronos","Name":"from_dict","Type":"method","Signature":"(config_dict: dict[str, typing.Any], **kwargs) -> 'GenerationConfig'","Arguments":"[\"config_dict\", \"kwargs\"]","Docstring":"Instantiates a [`GenerationConfig`] from a Python dictionary of parameters.\n\nArgs:\n    config_dict (","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.from_model_config","Module":"chronos.chronos","Name":"from_model_config","Type":"method","Signature":"(model_config: transformers.configuration_utils.PretrainedConfig) -> 'GenerationConfig'","Arguments":"[\"model_config\"]","Docstring":"Instantiates a [`GenerationConfig`] from a [`PretrainedConfig`]. This function is useful to convert ","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.from_pretrained","Module":"chronos.chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name: Union[str, os.PathLike], config_file_name: Union[str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', **kwargs) -> 'GenerationConfig'","Arguments":"[\"pretrained_model_name\", \"config_file_name\", \"cache_dir\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"kwargs\"]","Docstring":"Instantiate a [`GenerationConfig`] from a generation configuration file.\n\nArgs:\n    pretrained_model","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.get_generation_mode","Module":"chronos.chronos","Name":"get_generation_mode","Type":"method","Signature":"(self, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None) -> transformers.generation.configuration_utils.GenerationMode","Arguments":"[\"self\", \"assistant_model\"]","Docstring":"Returns the generation mode triggered by the [`GenerationConfig`] instance.\n\nArg:\n    assistant_mode","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.push_to_hub","Module":"chronos.chronos","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the {object_files} to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of ","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.save_pretrained","Module":"chronos.chronos","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], config_file_name: Union[str, os.PathLike, NoneType] = None, push_to_hub: bool = False, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"config_file_name\", \"push_to_hub\", \"kwargs\"]","Docstring":"Save a generation configuration object to the directory `save_directory`, so that it can be re-loade","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.to_dict","Module":"chronos.chronos","Name":"to_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Serializes this instance to a Python dictionary.\n\nReturns:\n    `dict[str, Any]`: Dictionary of all t","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.to_diff_dict","Module":"chronos.chronos","Name":"to_diff_dict","Type":"method","Signature":"(self) -> dict[str, typing.Any]","Arguments":"[\"self\"]","Docstring":"Removes all attributes from config which correspond to the default config attributes for better read","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.to_json_file","Module":"chronos.chronos","Name":"to_json_file","Type":"method","Signature":"(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)","Arguments":"[\"self\", \"json_file_path\", \"use_diff\"]","Docstring":"Save this instance to a JSON file.\n\nArgs:\n    json_file_path (`str` or `os.PathLike`):\n        Path ","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.to_json_string","Module":"chronos.chronos","Name":"to_json_string","Type":"method","Signature":"(self, use_diff: bool = True, ignore_metadata: bool = False) -> str","Arguments":"[\"self\", \"use_diff\", \"ignore_metadata\"]","Docstring":"Serializes this instance to a JSON string.\n\nArgs:\n    use_diff (`bool`, *optional*, defaults to `Tru","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.update","Module":"chronos.chronos","Name":"update","Type":"method","Signature":"(self, **kwargs)","Arguments":"[\"self\", \"kwargs\"]","Docstring":"Updates attributes of this class instance with attributes from `kwargs` if they match existing attri","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig.validate","Module":"chronos.chronos","Name":"validate","Type":"method","Signature":"(self, strict=False)","Arguments":"[\"self\", \"strict\"]","Docstring":"Validates the values of the attributes of the [`GenerationConfig`] instance. Raises exceptions in th","Parent":"GenerationConfig"},{"Path":"chronos.chronos.GenerationConfig","Module":"chronos.chronos","Name":"GenerationConfig","Type":"class","Signature":"","Arguments":"[]","Docstring":"Class that holds a configuration for a generation task. A `generate` call supports the following gen","Parent":"chronos"},{"Path":"chronos.chronos.List","Module":"chronos.chronos","Name":"List","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of list.","Parent":"chronos"},{"Path":"chronos.chronos.Literal","Module":"chronos.chronos","Name":"Literal","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Special typing form to define literal types (a.k.a. value types).\n\nThis form can be used to indicate","Parent":"chronos"},{"Path":"chronos.chronos.MeanScaleUniformBins.context_input_transform","Module":"chronos.chronos","Name":"context_input_transform","Type":"method","Signature":"(self, context: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"context\"]","Docstring":"Turn a batch of time series into token IDs, attention map, and tokenizer_state.\n\nParameters\n--------","Parent":"MeanScaleUniformBins"},{"Path":"chronos.chronos.MeanScaleUniformBins.label_input_transform","Module":"chronos.chronos","Name":"label_input_transform","Type":"method","Signature":"(self, label: torch.Tensor, scale: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"label\", \"scale\"]","Docstring":"Turn a batch of label slices of time series into token IDs and attention map\nusing the ``tokenizer_s","Parent":"MeanScaleUniformBins"},{"Path":"chronos.chronos.MeanScaleUniformBins.output_transform","Module":"chronos.chronos","Name":"output_transform","Type":"method","Signature":"(self, samples: torch.Tensor, scale: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"samples\", \"scale\"]","Docstring":"Turn a batch of sample token IDs into real values.\n\nParameters\n----------\nsamples\n    A tensor of in","Parent":"MeanScaleUniformBins"},{"Path":"chronos.chronos.MeanScaleUniformBins","Module":"chronos.chronos","Name":"MeanScaleUniformBins","Type":"class","Signature":"","Arguments":"[]","Docstring":"A ``ChronosTokenizer`` defines how time series are mapped into token IDs\nand back.\n\nFor details, see","Parent":"chronos"},{"Path":"chronos.chronos.Optional","Module":"chronos.chronos","Name":"Optional","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Optional[X] is equivalent to Union[X, None].","Parent":"chronos"},{"Path":"chronos.chronos.PreTrainedModel.active_adapters","Module":"chronos.chronos","Name":"active_adapters","Type":"method","Signature":"(self) -> list[str]","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.add_adapter","Module":"chronos.chronos","Name":"add_adapter","Type":"method","Signature":"(self, adapter_config, adapter_name: Optional[str] = None) -> None","Arguments":"[\"self\", \"adapter_config\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.add_memory_hooks","Module":"chronos.chronos","Name":"add_memory_hooks","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Add a memory hook before and after each sub-module forward pass to record increase in memory consump","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.add_model_tags","Module":"chronos.chronos","Name":"add_model_tags","Type":"method","Signature":"(self, tags: Union[list[str], str]) -> None","Arguments":"[\"self\", \"tags\"]","Docstring":"Add custom tags into the model that gets pushed to the Hugging Face Hub. Will\nnot overwrite existing","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.add_module","Module":"chronos.chronos","Name":"add_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Add a child module to the current module.\n\nThe module can be accessed as an attribute using the give","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.apply","Module":"chronos.chronos","Name":"apply","Type":"method","Signature":"(self, fn: collections.abc.Callable[['Module'], None]) -> Self","Arguments":"[\"self\", \"fn\"]","Docstring":"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n\nTypic","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.bfloat16","Module":"chronos.chronos","Name":"bfloat16","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n\n.. note::\n    This method","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.buffers","Module":"chronos.chronos","Name":"buffers","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module buffers.\n\nArgs:\n    recurse (bool): if True, then yields buffers of t","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.can_generate","Module":"chronos.chronos","Name":"can_generate","Type":"method","Signature":"() -> bool","Arguments":"[]","Docstring":"Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n\nUn","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.children","Module":"chronos.chronos","Name":"children","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules.\n\nYields:\n    Module: a child module","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.compile","Module":"chronos.chronos","Name":"compile","Type":"method","Signature":"(self, *args, **kwargs) -> None","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Compile this Module's forward using :func:`torch.compile`.\n\nThis Module's `__call__` method is compi","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.cpu","Module":"chronos.chronos","Name":"cpu","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Move all model parameters and buffers to the CPU.\n\n.. note::\n    This method modifies the module in-","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.create_extended_attention_mask_for_decoder","Module":"chronos.chronos","Name":"create_extended_attention_mask_for_decoder","Type":"method","Signature":"(input_shape, attention_mask, device=None)","Arguments":"[\"input_shape\", \"attention_mask\", \"device\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.cuda","Module":"chronos.chronos","Name":"cuda","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.delete_adapter","Module":"chronos.chronos","Name":"delete_adapter","Type":"method","Signature":"(self, adapter_names: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_names\"]","Docstring":"Delete a PEFT adapter from the underlying model.\n\nArgs:\n    adapter_names (`Union[list[str], str]`):","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.dequantize","Module":"chronos.chronos","Name":"dequantize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Potentially dequantize the model in case it has been quantized by a quantization method that support","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.disable_adapters","Module":"chronos.chronos","Name":"disable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.disable_input_require_grads","Module":"chronos.chronos","Name":"disable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Removes the `_require_grads_hook`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.double","Module":"chronos.chronos","Name":"double","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"Casts all floating point parameters and buffers to ``double`` datatype.\n\n.. note::\n    This method m","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.enable_adapters","Module":"chronos.chronos","Name":"enable_adapters","Type":"method","Signature":"(self) -> None","Arguments":"[\"self\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.enable_input_require_grads","Module":"chronos.chronos","Name":"enable_input_require_grads","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.estimate_tokens","Module":"chronos.chronos","Name":"estimate_tokens","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]]) -> int","Arguments":"[\"self\", \"input_dict\"]","Docstring":"Helper function to estimate the total number of tokens from the model inputs.\n\nArgs:\n    inputs (`di","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.eval","Module":"chronos.chronos","Name":"eval","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Set the module in evaluation mode.\n\nThis has an effect only on certain modules. See the documentatio","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.extra_repr","Module":"chronos.chronos","Name":"extra_repr","Type":"method","Signature":"(self) -> str","Arguments":"[\"self\"]","Docstring":"Return the extra representation of the module.\n\nTo print customized extra information, you should re","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.float","Module":"chronos.chronos","Name":"float","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``float`` datatype.\n\n.. note::\n    This method mo","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.floating_point_ops","Module":"chronos.chronos","Name":"floating_point_ops","Type":"method","Signature":"(self, input_dict: dict[str, typing.Union[torch.Tensor, typing.Any]], exclude_embeddings: bool = True) -> int","Arguments":"[\"self\", \"input_dict\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, non-embeddings) floating-point operations for the forward and backward pa","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.forward","Module":"chronos.chronos","Name":"forward","Type":"method","Signature":"(self, *input: Any) -> None","Arguments":"[\"self\", \"input\"]","Docstring":"Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.from_pretrained","Module":"chronos.chronos","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], *model_args, config: Union[transformers.configuration_utils.PretrainedConfig, str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, ignore_mismatched_sizes: bool = False, force_download: bool = False, local_files_only: bool = False, token: Union[str, bool, NoneType] = None, revision: str = 'main', use_safetensors: Optional[bool] = None, weights_only: bool = True, **kwargs) -> ~SpecificPreTrainedModelType","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"config\", \"cache_dir\", \"ignore_mismatched_sizes\", \"force_download\", \"local_files_only\", \"token\", \"revision\", \"use_safetensors\", \"weights_only\", \"kwargs\"]","Docstring":"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\nThe model is set in ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_adapter_state_dict","Module":"chronos.chronos","Name":"get_adapter_state_dict","Type":"method","Signature":"(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict","Arguments":"[\"self\", \"adapter_name\", \"state_dict\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_buffer","Module":"chronos.chronos","Name":"get_buffer","Type":"method","Signature":"(self, target: str) -> 'Tensor'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring for","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_compiled_call","Module":"chronos.chronos","Name":"get_compiled_call","Type":"method","Signature":"(self, compile_config: Optional[transformers.generation.configuration_utils.CompileConfig]) -> Callable","Arguments":"[\"self\", \"compile_config\"]","Docstring":"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_correct_attn_implementation","Module":"chronos.chronos","Name":"get_correct_attn_implementation","Type":"method","Signature":"(self, requested_attention: Optional[str], is_init_check: bool = False) -> str","Arguments":"[\"self\", \"requested_attention\", \"is_init_check\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_decoder","Module":"chronos.chronos","Name":"get_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Best-effort lookup of the *decoder* module.\n\nOrder of attempts (covers ~85 % of current usages):\n\n1.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_extended_attention_mask","Module":"chronos.chronos","Name":"get_extended_attention_mask","Type":"method","Signature":"(self, attention_mask: torch.Tensor, input_shape: tuple[int, ...], device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor","Arguments":"[\"self\", \"attention_mask\", \"input_shape\", \"device\", \"dtype\"]","Docstring":"Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\nArgume","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_extra_state","Module":"chronos.chronos","Name":"get_extra_state","Type":"method","Signature":"(self) -> Any","Arguments":"[\"self\"]","Docstring":"Return any extra state to include in the module's state_dict.\n\nImplement this and a corresponding :f","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_head_mask","Module":"chronos.chronos","Name":"get_head_mask","Type":"method","Signature":"(self, head_mask: Optional[torch.Tensor], num_hidden_layers: int, is_attention_chunked: bool = False) -> torch.Tensor","Arguments":"[\"self\", \"head_mask\", \"num_hidden_layers\", \"is_attention_chunked\"]","Docstring":"Prepare the head mask if needed.\n\nArgs:\n    head_mask (`torch.Tensor` with shape `[num_heads]` or `[","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_init_context","Module":"chronos.chronos","Name":"get_init_context","Type":"method","Signature":"(is_quantized: bool, _is_ds_init_called: bool)","Arguments":"[\"is_quantized\", \"_is_ds_init_called\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_input_embeddings","Module":"chronos.chronos","Name":"get_input_embeddings","Type":"method","Signature":"(self) -> torch.nn.modules.module.Module","Arguments":"[\"self\"]","Docstring":"Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary t","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_memory_footprint","Module":"chronos.chronos","Name":"get_memory_footprint","Type":"method","Signature":"(self, return_buffers=True)","Arguments":"[\"self\", \"return_buffers\"]","Docstring":"Get the memory footprint of a model. This will return the memory footprint of the current model in b","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_output_embeddings","Module":"chronos.chronos","Name":"get_output_embeddings","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_parameter","Module":"chronos.chronos","Name":"get_parameter","Type":"method","Signature":"(self, target: str) -> 'Parameter'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter given by ``target`` if it exists, otherwise throw an error.\n\nSee the docstring ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_parameter_or_buffer","Module":"chronos.chronos","Name":"get_parameter_or_buffer","Type":"method","Signature":"(self, target: str)","Arguments":"[\"self\", \"target\"]","Docstring":"Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combin","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_position_embeddings","Module":"chronos.chronos","Name":"get_position_embeddings","Type":"method","Signature":"(self) -> Union[torch.nn.modules.sparse.Embedding, tuple[torch.nn.modules.sparse.Embedding]]","Arguments":"[\"self\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.get_submodule","Module":"chronos.chronos","Name":"get_submodule","Type":"method","Signature":"(self, target: str) -> 'Module'","Arguments":"[\"self\", \"target\"]","Docstring":"Return the submodule given by ``target`` if it exists, otherwise throw an error.\n\nFor example, let's","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.gradient_checkpointing_disable","Module":"chronos.chronos","Name":"gradient_checkpointing_disable","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Deactivates gradient checkpointing for the current model.\n\nNote that in other frameworks this featur","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.gradient_checkpointing_enable","Module":"chronos.chronos","Name":"gradient_checkpointing_enable","Type":"method","Signature":"(self, gradient_checkpointing_kwargs=None)","Arguments":"[\"self\", \"gradient_checkpointing_kwargs\"]","Docstring":"Activates gradient checkpointing for the current model.\n\nNote that in other frameworks this feature ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.half","Module":"chronos.chronos","Name":"half","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Casts all floating point parameters and buffers to ``half`` datatype.\n\n.. note::\n    This method mod","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.init_weights","Module":"chronos.chronos","Name":"init_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to imp","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.initialize_weights","Module":"chronos.chronos","Name":"initialize_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"This is equivalent to calling `self.apply(self._initialize_weights)`, but correctly handles composit","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.invert_attention_mask","Module":"chronos.chronos","Name":"invert_attention_mask","Type":"method","Signature":"(self, encoder_attention_mask: torch.Tensor) -> torch.Tensor","Arguments":"[\"self\", \"encoder_attention_mask\"]","Docstring":"Invert an attention mask (e.g., switches 0. and 1.).\n\nArgs:\n    encoder_attention_mask (`torch.Tenso","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.ipu","Module":"chronos.chronos","Name":"ipu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the IPU.\n\nThis also makes associated parameters and buffers","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.is_backend_compatible","Module":"chronos.chronos","Name":"is_backend_compatible","Type":"method","Signature":"()","Arguments":"[]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.kernelize","Module":"chronos.chronos","Name":"kernelize","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.load_adapter","Module":"chronos.chronos","Name":"load_adapter","Type":"method","Signature":"(self, peft_model_id: Optional[str] = None, adapter_name: Optional[str] = None, revision: Optional[str] = None, token: Optional[str] = None, device_map: str = 'auto', max_memory: Optional[str] = None, offload_folder: Optional[str] = None, offload_index: Optional[int] = None, peft_config: Optional[dict[str, Any]] = None, adapter_state_dict: Optional[dict[str, 'torch.Tensor']] = None, low_cpu_mem_usage: bool = False, is_trainable: bool = False, adapter_kwargs: Optional[dict[str, Any]] = None) -> None","Arguments":"[\"self\", \"peft_model_id\", \"adapter_name\", \"revision\", \"token\", \"device_map\", \"max_memory\", \"offload_folder\", \"offload_index\", \"peft_config\", \"adapter_state_dict\", \"low_cpu_mem_usage\", \"is_trainable\", \"adapter_kwargs\"]","Docstring":"Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.load_state_dict","Module":"chronos.chronos","Name":"load_state_dict","Type":"method","Signature":"(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)","Arguments":"[\"self\", \"state_dict\", \"strict\", \"assign\"]","Docstring":"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n\nIf :attr:","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.modules","Module":"chronos.chronos","Name":"modules","Type":"method","Signature":"(self) -> collections.abc.Iterator['Module']","Arguments":"[\"self\"]","Docstring":"Return an iterator over all modules in the network.\n\nYields:\n    Module: a module in the network\n\nNo","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.mtia","Module":"chronos.chronos","Name":"mtia","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the MTIA.\n\nThis also makes associated parameters and buffer","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.named_buffers","Module":"chronos.chronos","Name":"named_buffers","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer i","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.named_children","Module":"chronos.chronos","Name":"named_children","Type":"method","Signature":"(self) -> collections.abc.Iterator[tuple[str, 'Module']]","Arguments":"[\"self\"]","Docstring":"Return an iterator over immediate children modules, yielding both the name of the module as well as ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.named_modules","Module":"chronos.chronos","Name":"named_modules","Type":"method","Signature":"(self, memo: set['Module'] | None = None, prefix: str = '', remove_duplicate: bool = True)","Arguments":"[\"self\", \"memo\", \"prefix\", \"remove_duplicate\"]","Docstring":"Return an iterator over all modules in the network, yielding both the name of the module as well as ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.named_parameters","Module":"chronos.chronos","Name":"named_parameters","Type":"method","Signature":"(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]","Arguments":"[\"self\", \"prefix\", \"recurse\", \"remove_duplicate\"]","Docstring":"Return an iterator over module parameters, yielding both the name of the parameter as well as the pa","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.num_parameters","Module":"chronos.chronos","Name":"num_parameters","Type":"method","Signature":"(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int","Arguments":"[\"self\", \"only_trainable\", \"exclude_embeddings\"]","Docstring":"Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\nArgs:\n    only_tr","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.parameters","Module":"chronos.chronos","Name":"parameters","Type":"method","Signature":"(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]","Arguments":"[\"self\", \"recurse\"]","Docstring":"Return an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nArgs:\n    rec","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.post_init","Module":"chronos.chronos","Name":"post_init","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"A method executed at the end of each Transformer model initialization, to execute code that needs th","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.prune_heads","Module":"chronos.chronos","Name":"prune_heads","Type":"method","Signature":"(self, heads_to_prune: dict[int, list[int]])","Arguments":"[\"self\", \"heads_to_prune\"]","Docstring":"Prunes heads of the base model.\n\nArguments:\n    heads_to_prune (`dict[int, list[int]]`):\n        Dic","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.push_to_hub","Module":"chronos.chronos","Name":"push_to_hub","Type":"method","Signature":"(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str","Arguments":"[\"self\", \"repo_id\", \"use_temp_dir\", \"commit_message\", \"private\", \"token\", \"max_shard_size\", \"create_pr\", \"safe_serialization\", \"revision\", \"commit_description\", \"tags\", \"deprecated_kwargs\"]","Docstring":"Upload the model file to the \ud83e\udd17 Model Hub.\n\nParameters:\n    repo_id (`str`):\n        The name of the ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_backward_hook","Module":"chronos.chronos","Name":"register_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a backward hook on the module.\n\nThis function is deprecated in favor of :meth:`~torch.nn.Mo","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_buffer","Module":"chronos.chronos","Name":"register_buffer","Type":"method","Signature":"(self, name: str, tensor: torch.Tensor | None, persistent: bool = True) -> None","Arguments":"[\"self\", \"name\", \"tensor\", \"persistent\"]","Docstring":"Add a buffer to the module.\n\nThis is typically used to register a buffer that should not be\nconsider","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_for_auto_class","Module":"chronos.chronos","Name":"register_for_auto_class","Type":"method","Signature":"(auto_class='AutoModel')","Arguments":"[\"auto_class\"]","Docstring":"Register this class with a given auto class. This should only be used for custom models as the ones ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_forward_hook","Module":"chronos.chronos","Name":"register_forward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...], typing.Any], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any], typing.Any], typing.Any | None], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\", \"always_call\"]","Docstring":"Register a forward hook on the module.\n\nThe hook will be called every time after :func:`forward` has","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_forward_pre_hook","Module":"chronos.chronos","Name":"register_forward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[[~T, tuple[typing.Any, ...]], typing.Any | None] | collections.abc.Callable[[~T, tuple[typing.Any, ...], dict[str, typing.Any]], tuple[typing.Any, dict[str, typing.Any]] | None], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\", \"with_kwargs\"]","Docstring":"Register a forward pre-hook on the module.\n\nThe hook will be called every time before :func:`forward","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_full_backward_hook","Module":"chronos.chronos","Name":"register_full_backward_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor], typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward hook on the module.\n\nThe hook will be called every time the gradients with respe","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_full_backward_pre_hook","Module":"chronos.chronos","Name":"register_full_backward_pre_hook","Type":"method","Signature":"(self, hook: collections.abc.Callable[['Module', typing.Union[tuple[torch.Tensor, ...], torch.Tensor]], typing.Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle","Arguments":"[\"self\", \"hook\", \"prepend\"]","Docstring":"Register a backward pre-hook on the module.\n\nThe hook will be called every time the gradients for th","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_load_state_dict_post_hook","Module":"chronos.chronos","Name":"register_load_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_load_state_dict_pre_hook","Module":"chronos.chronos","Name":"register_load_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n\nIt shou","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_module","Module":"chronos.chronos","Name":"register_module","Type":"method","Signature":"(self, name: str, module: Optional[ForwardRef('Module')]) -> None","Arguments":"[\"self\", \"name\", \"module\"]","Docstring":"Alias for :func:`add_module`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_parameter","Module":"chronos.chronos","Name":"register_parameter","Type":"method","Signature":"(self, name: str, param: torch.nn.parameter.Parameter | None) -> None","Arguments":"[\"self\", \"name\", \"param\"]","Docstring":"Add a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nArg","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_state_dict_post_hook","Module":"chronos.chronos","Name":"register_state_dict_post_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followi","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.register_state_dict_pre_hook","Module":"chronos.chronos","Name":"register_state_dict_pre_hook","Type":"method","Signature":"(self, hook)","Arguments":"[\"self\", \"hook\"]","Docstring":"Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n\nIt should have the followin","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.requires_grad_","Module":"chronos.chronos","Name":"requires_grad_","Type":"method","Signature":"(self, requires_grad: bool = True) -> Self","Arguments":"[\"self\", \"requires_grad\"]","Docstring":"Change if autograd should record operations on parameters in this module.\n\nThis method sets the para","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.reset_memory_hooks_state","Module":"chronos.chronos","Name":"reset_memory_hooks_state","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reset the `mem_rss_diff` attribute of each module (see [`~modeling_utils.ModuleUtilsMixin.add_memory","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.resize_position_embeddings","Module":"chronos.chronos","Name":"resize_position_embeddings","Type":"method","Signature":"(self, new_num_position_embeddings: int)","Arguments":"[\"self\", \"new_num_position_embeddings\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.resize_token_embeddings","Module":"chronos.chronos","Name":"resize_token_embeddings","Type":"method","Signature":"(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True) -> torch.nn.modules.sparse.Embedding","Arguments":"[\"self\", \"new_num_tokens\", \"pad_to_multiple_of\", \"mean_resizing\"]","Docstring":"Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n\nTakes ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.retrieve_modules_from_names","Module":"chronos.chronos","Name":"retrieve_modules_from_names","Type":"method","Signature":"(self, names, add_prefix=False, remove_prefix=False)","Arguments":"[\"self\", \"names\", \"add_prefix\", \"remove_prefix\"]","Docstring":"","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.reverse_bettertransformer","Module":"chronos.chronos","Name":"reverse_bettertransformer","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Reverts the transformation from [`~PreTrainedModel.to_bettertransformer`] so that the original model","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.save_pretrained","Module":"chronos.chronos","Name":"save_pretrained","Type":"method","Signature":"(self, save_directory: Union[str, os.PathLike], is_main_process: bool = True, state_dict: Optional[dict] = None, save_function: Callable = <function save at 0x0000020857EE1260>, push_to_hub: bool = False, max_shard_size: Union[int, str] = '5GB', safe_serialization: bool = True, variant: Optional[str] = None, token: Union[str, bool, NoneType] = None, save_peft_format: bool = True, **kwargs)","Arguments":"[\"self\", \"save_directory\", \"is_main_process\", \"state_dict\", \"save_function\", \"push_to_hub\", \"max_shard_size\", \"safe_serialization\", \"variant\", \"token\", \"save_peft_format\", \"kwargs\"]","Docstring":"Save a model and its configuration file to a directory, so that it can be re-loaded using the\n[`~Pre","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.set_adapter","Module":"chronos.chronos","Name":"set_adapter","Type":"method","Signature":"(self, adapter_name: Union[list[str], str]) -> None","Arguments":"[\"self\", \"adapter_name\"]","Docstring":"If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.set_attn_implementation","Module":"chronos.chronos","Name":"set_attn_implementation","Type":"method","Signature":"(self, attn_implementation: Union[str, dict])","Arguments":"[\"self\", \"attn_implementation\"]","Docstring":"Set the requested `attn_implementation` for this model.\n\nArgs:\n    attn_implementation (`str` or `di","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.set_decoder","Module":"chronos.chronos","Name":"set_decoder","Type":"method","Signature":"(self, decoder)","Arguments":"[\"self\", \"decoder\"]","Docstring":"Symmetric setter. Mirrors the lookup logic used in `get_decoder`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.set_extra_state","Module":"chronos.chronos","Name":"set_extra_state","Type":"method","Signature":"(self, state: Any) -> None","Arguments":"[\"self\", \"state\"]","Docstring":"Set extra state contained in the loaded `state_dict`.\n\nThis function is called from :func:`load_stat","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.set_input_embeddings","Module":"chronos.chronos","Name":"set_input_embeddings","Type":"method","Signature":"(self, value: torch.nn.modules.module.Module)","Arguments":"[\"self\", \"value\"]","Docstring":"Fallback setter that handles **~70%** of models in the code-base.\n\nOrder of attempts:\n1. `self.model","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.set_output_embeddings","Module":"chronos.chronos","Name":"set_output_embeddings","Type":"method","Signature":"(self, new_embeddings)","Arguments":"[\"self\", \"new_embeddings\"]","Docstring":"Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.set_submodule","Module":"chronos.chronos","Name":"set_submodule","Type":"method","Signature":"(self, target: str, module: 'Module', strict: bool = False) -> None","Arguments":"[\"self\", \"target\", \"module\", \"strict\"]","Docstring":"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n.. note::\n    If ``st","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.share_memory","Module":"chronos.chronos","Name":"share_memory","Type":"method","Signature":"(self) -> Self","Arguments":"[\"self\"]","Docstring":"See :meth:`torch.Tensor.share_memory_`.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.state_dict","Module":"chronos.chronos","Name":"state_dict","Type":"method","Signature":"(self, *args, destination=None, prefix='', keep_vars=False)","Arguments":"[\"self\", \"args\", \"destination\", \"prefix\", \"keep_vars\"]","Docstring":"Return a dictionary containing references to the whole state of the module.\n\nBoth parameters and per","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.tie_embeddings_and_encoder_decoder","Module":"chronos.chronos","Name":"tie_embeddings_and_encoder_decoder","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"If set in the config, tie the weights between the input embeddings and the output embeddings,\nand th","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.tie_weights","Module":"chronos.chronos","Name":"tie_weights","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Recursively (for all submodels) tie all the weights of the model.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.to","Module":"chronos.chronos","Name":"to","Type":"method","Signature":"(self, *args, **kwargs)","Arguments":"[\"self\", \"args\", \"kwargs\"]","Docstring":"Move and\/or cast the parameters and buffers.\n\nThis can be called as\n\n.. function:: to(device=None, d","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.to_bettertransformer","Module":"chronos.chronos","Name":"to_bettertransformer","Type":"method","Signature":"(self) -> 'PreTrainedModel'","Arguments":"[\"self\"]","Docstring":"Converts the model to use [PyTorch's native attention\nimplementation](https:\/\/pytorch.org\/docs\/stabl","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.to_empty","Module":"chronos.chronos","Name":"to_empty","Type":"method","Signature":"(self, *, device: Union[str, torch.device, int, NoneType], recurse: bool = True) -> Self","Arguments":"[\"self\", \"device\", \"recurse\"]","Docstring":"Move the parameters and buffers to the specified device without copying storage.\n\nArgs:\n    device (","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.train","Module":"chronos.chronos","Name":"train","Type":"method","Signature":"(self, mode: bool = True)","Arguments":"[\"self\", \"mode\"]","Docstring":"Set the module in training mode.\n\nThis has an effect only on certain modules. See the documentation ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.type","Module":"chronos.chronos","Name":"type","Type":"method","Signature":"(self, dst_type: torch.dtype | str) -> Self","Arguments":"[\"self\", \"dst_type\"]","Docstring":"Casts all parameters and buffers to :attr:`dst_type`.\n\n.. note::\n    This method modifies the module","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.warn_if_padding_and_no_attention_mask","Module":"chronos.chronos","Name":"warn_if_padding_and_no_attention_mask","Type":"method","Signature":"(self, input_ids, attention_mask)","Arguments":"[\"self\", \"input_ids\", \"attention_mask\"]","Docstring":"Shows a one-time warning if the input_ids appear to contain padding and no attention mask was given.","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.xpu","Module":"chronos.chronos","Name":"xpu","Type":"method","Signature":"(self, device: int | torch.device | None = None) -> Self","Arguments":"[\"self\", \"device\"]","Docstring":"Move all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel.zero_grad","Module":"chronos.chronos","Name":"zero_grad","Type":"method","Signature":"(self, set_to_none: bool = True) -> None","Arguments":"[\"self\", \"set_to_none\"]","Docstring":"Reset gradients of all model parameters.\n\nSee similar function under :class:`torch.optim.Optimizer` ","Parent":"PreTrainedModel"},{"Path":"chronos.chronos.PreTrainedModel","Module":"chronos.chronos","Name":"PreTrainedModel","Type":"class","Signature":"","Arguments":"[]","Docstring":"Base class for all models.\n\n[`PreTrainedModel`] takes care of storing the configuration of the model","Parent":"chronos"},{"Path":"chronos.chronos.Tuple","Module":"chronos.chronos","Name":"Tuple","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Deprecated alias to builtins.tuple.\n\nTuple[X, Y] is the cross-product type of X and Y.\n\nExample: Tup","Parent":"chronos"},{"Path":"chronos.chronos.Union","Module":"chronos.chronos","Name":"Union","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Union type; Union[X, Y] means either X or Y.\n\nOn Python 3.10 and higher, the | operator\ncan also be ","Parent":"chronos"},{"Path":"chronos.chronos.dataclass","Module":"chronos.chronos","Name":"dataclass","Type":"function","Signature":"(cls=None, \/, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, match_args=True, kw_only=False, slots=False, weakref_slot=False)","Arguments":"[\"cls\", \"init\", \"repr\", \"eq\", \"order\", \"unsafe_hash\", \"frozen\", \"match_args\", \"kw_only\", \"slots\", \"weakref_slot\"]","Docstring":"Add dunder methods based on the fields defined in the class.\n\nExamines PEP 526 __annotations__ to de","Parent":"chronos"},{"Path":"chronos.chronos.left_pad_and_stack_1D","Module":"chronos.chronos","Name":"left_pad_and_stack_1D","Type":"function","Signature":"(tensors: List[torch.Tensor]) -> torch.Tensor","Arguments":"[\"tensors\"]","Docstring":"","Parent":"chronos"},{"Path":"chronos.chronos.logger","Module":"chronos.chronos","Name":"logger","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Instances of the Logger class represent a single logging channel. A\n\"logging channel\" indicates an a","Parent":"chronos"},{"Path":"chronos.boto_utils.CHUNK_SIZE","Module":"chronos.boto_utils","Name":"CHUNK_SIZE","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"int([x]) -> integer\nint(x, base=10) -> integer\n\nConvert a number or string to an integer, or return ","Parent":"chronos"},{"Path":"chronos.boto_utils.CLOUDFRONT_MAPPING","Module":"chronos.boto_utils","Name":"CLOUDFRONT_MAPPING","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"dict() -> new empty dictionary\ndict(mapping) -> new dictionary initialized from a mapping object's\n ","Parent":"chronos"},{"Path":"chronos.boto_utils.ClientError","Module":"chronos.boto_utils","Name":"ClientError","Type":"class","Signature":"","Arguments":"[]","Docstring":"Common base class for all non-exit exceptions.","Parent":"chronos"},{"Path":"chronos.boto_utils.Config.merge","Module":"chronos.boto_utils","Name":"merge","Type":"method","Signature":"(self, other_config)","Arguments":"[\"self\", \"other_config\"]","Docstring":"Merges the config object with another config object\n\nThis will merge in all non-default values from ","Parent":"Config"},{"Path":"chronos.boto_utils.Config","Module":"chronos.boto_utils","Name":"Config","Type":"class","Signature":"","Arguments":"[]","Docstring":"Advanced configuration for Botocore clients.\n\n:type region_name: str\n:param region_name: The region ","Parent":"chronos"},{"Path":"chronos.boto_utils.MODEL_FILENAMES","Module":"chronos.boto_utils","Name":"MODEL_FILENAMES","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Built-in mutable sequence.\n\nIf no argument is given, the constructor creates a new empty list.\nThe a","Parent":"chronos"},{"Path":"chronos.boto_utils.NoCredentialsError","Module":"chronos.boto_utils","Name":"NoCredentialsError","Type":"class","Signature":"","Arguments":"[]","Docstring":"No credentials could be found.","Parent":"chronos"},{"Path":"chronos.boto_utils.Path.absolute","Module":"chronos.boto_utils","Name":"absolute","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return an absolute version of this path by prepending the current\nworking directory. No normalizatio","Parent":"Path"},{"Path":"chronos.boto_utils.Path.as_posix","Module":"chronos.boto_utils","Name":"as_posix","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the string representation of the path with forward (\/)\nslashes.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.as_uri","Module":"chronos.boto_utils","Name":"as_uri","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the path as a 'file' URI.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.chmod","Module":"chronos.boto_utils","Name":"chmod","Type":"method","Signature":"(self, mode, *, follow_symlinks=True)","Arguments":"[\"self\", \"mode\", \"follow_symlinks\"]","Docstring":"Change the permissions of the path, like os.chmod().","Parent":"Path"},{"Path":"chronos.boto_utils.Path.cwd","Module":"chronos.boto_utils","Name":"cwd","Type":"method","Signature":"()","Arguments":"[]","Docstring":"Return a new path pointing to the current working directory\n(as returned by os.getcwd()).","Parent":"Path"},{"Path":"chronos.boto_utils.Path.exists","Module":"chronos.boto_utils","Name":"exists","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path exists.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.expanduser","Module":"chronos.boto_utils","Name":"expanduser","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return a new path with expanded ~ and ~user constructs\n(as returned by os.path.expanduser)","Parent":"Path"},{"Path":"chronos.boto_utils.Path.glob","Module":"chronos.boto_utils","Name":"glob","Type":"method","Signature":"(self, pattern)","Arguments":"[\"self\", \"pattern\"]","Docstring":"Iterate over this subtree and yield all existing files (of any\nkind, including directories) matching","Parent":"Path"},{"Path":"chronos.boto_utils.Path.group","Module":"chronos.boto_utils","Name":"group","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the group name of the file gid.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.hardlink_to","Module":"chronos.boto_utils","Name":"hardlink_to","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Make this path a hard link pointing to the same file as *target*.\n\nNote the order of arguments (self","Parent":"Path"},{"Path":"chronos.boto_utils.Path.home","Module":"chronos.boto_utils","Name":"home","Type":"method","Signature":"()","Arguments":"[]","Docstring":"Return a new path pointing to the user's home directory (as\nreturned by os.path.expanduser('~')).","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_absolute","Module":"chronos.boto_utils","Name":"is_absolute","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"True if the path is absolute (has both a root and, if applicable,\na drive).","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_block_device","Module":"chronos.boto_utils","Name":"is_block_device","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a block device.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_char_device","Module":"chronos.boto_utils","Name":"is_char_device","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a character device.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_dir","Module":"chronos.boto_utils","Name":"is_dir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a directory.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_fifo","Module":"chronos.boto_utils","Name":"is_fifo","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a FIFO.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_file","Module":"chronos.boto_utils","Name":"is_file","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a regular file (also True for symlinks pointing\nto regular files).","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_mount","Module":"chronos.boto_utils","Name":"is_mount","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Check if this path is a POSIX mount point","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_relative_to","Module":"chronos.boto_utils","Name":"is_relative_to","Type":"method","Signature":"(self, *other)","Arguments":"[\"self\", \"other\"]","Docstring":"Return True if the path is relative to another path or False.\n        ","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_reserved","Module":"chronos.boto_utils","Name":"is_reserved","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return True if the path contains one of the special names reserved\nby the system, if any.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_socket","Module":"chronos.boto_utils","Name":"is_socket","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a socket.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.is_symlink","Module":"chronos.boto_utils","Name":"is_symlink","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a symbolic link.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.iterdir","Module":"chronos.boto_utils","Name":"iterdir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Iterate over the files in this directory.  Does not yield any\nresult for the special paths '.' and '","Parent":"Path"},{"Path":"chronos.boto_utils.Path.joinpath","Module":"chronos.boto_utils","Name":"joinpath","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Combine this path with one or several arguments, and return a\nnew path representing either a subpath","Parent":"Path"},{"Path":"chronos.boto_utils.Path.lchmod","Module":"chronos.boto_utils","Name":"lchmod","Type":"method","Signature":"(self, mode)","Arguments":"[\"self\", \"mode\"]","Docstring":"Like chmod(), except if the path points to a symlink, the symlink's\npermissions are changed, rather ","Parent":"Path"},{"Path":"chronos.boto_utils.Path.link_to","Module":"chronos.boto_utils","Name":"link_to","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Make the target path a hard link pointing to this path.\n\nNote this function does not make this path ","Parent":"Path"},{"Path":"chronos.boto_utils.Path.lstat","Module":"chronos.boto_utils","Name":"lstat","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Like stat(), except if the path points to a symlink, the symlink's\nstatus information is returned, r","Parent":"Path"},{"Path":"chronos.boto_utils.Path.match","Module":"chronos.boto_utils","Name":"match","Type":"method","Signature":"(self, path_pattern)","Arguments":"[\"self\", \"path_pattern\"]","Docstring":"Return True if this path matches the given pattern.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.mkdir","Module":"chronos.boto_utils","Name":"mkdir","Type":"method","Signature":"(self, mode=511, parents=False, exist_ok=False)","Arguments":"[\"self\", \"mode\", \"parents\", \"exist_ok\"]","Docstring":"Create a new directory at this given path.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.open","Module":"chronos.boto_utils","Name":"open","Type":"method","Signature":"(self, mode='r', buffering=-1, encoding=None, errors=None, newline=None)","Arguments":"[\"self\", \"mode\", \"buffering\", \"encoding\", \"errors\", \"newline\"]","Docstring":"Open the file pointed by this path and return a file object, as\nthe built-in open() function does.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.owner","Module":"chronos.boto_utils","Name":"owner","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the login name of the file owner.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.read_bytes","Module":"chronos.boto_utils","Name":"read_bytes","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Open the file in bytes mode, read it, and close the file.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.read_text","Module":"chronos.boto_utils","Name":"read_text","Type":"method","Signature":"(self, encoding=None, errors=None)","Arguments":"[\"self\", \"encoding\", \"errors\"]","Docstring":"Open the file in text mode, read it, and close the file.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.readlink","Module":"chronos.boto_utils","Name":"readlink","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the path to which the symbolic link points.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.relative_to","Module":"chronos.boto_utils","Name":"relative_to","Type":"method","Signature":"(self, *other)","Arguments":"[\"self\", \"other\"]","Docstring":"Return the relative path to another path identified by the passed\narguments.  If the operation is no","Parent":"Path"},{"Path":"chronos.boto_utils.Path.rename","Module":"chronos.boto_utils","Name":"rename","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Rename this path to the target path.\n\nThe target path may be absolute or relative. Relative paths ar","Parent":"Path"},{"Path":"chronos.boto_utils.Path.replace","Module":"chronos.boto_utils","Name":"replace","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Rename this path to the target path, overwriting if that path exists.\n\nThe target path may be absolu","Parent":"Path"},{"Path":"chronos.boto_utils.Path.resolve","Module":"chronos.boto_utils","Name":"resolve","Type":"method","Signature":"(self, strict=False)","Arguments":"[\"self\", \"strict\"]","Docstring":"Make the path absolute, resolving all symlinks on the way and also\nnormalizing it.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.rglob","Module":"chronos.boto_utils","Name":"rglob","Type":"method","Signature":"(self, pattern)","Arguments":"[\"self\", \"pattern\"]","Docstring":"Recursively yield all existing files (of any kind, including\ndirectories) matching the given relativ","Parent":"Path"},{"Path":"chronos.boto_utils.Path.rmdir","Module":"chronos.boto_utils","Name":"rmdir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Remove this directory.  The directory must be empty.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.samefile","Module":"chronos.boto_utils","Name":"samefile","Type":"method","Signature":"(self, other_path)","Arguments":"[\"self\", \"other_path\"]","Docstring":"Return whether other_path is the same or not as this file\n(as returned by os.path.samefile()).","Parent":"Path"},{"Path":"chronos.boto_utils.Path.stat","Module":"chronos.boto_utils","Name":"stat","Type":"method","Signature":"(self, *, follow_symlinks=True)","Arguments":"[\"self\", \"follow_symlinks\"]","Docstring":"Return the result of the stat() system call on this path, like\nos.stat() does.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.symlink_to","Module":"chronos.boto_utils","Name":"symlink_to","Type":"method","Signature":"(self, target, target_is_directory=False)","Arguments":"[\"self\", \"target\", \"target_is_directory\"]","Docstring":"Make this path a symlink pointing to the target path.\nNote the order of arguments (link, target) is ","Parent":"Path"},{"Path":"chronos.boto_utils.Path.touch","Module":"chronos.boto_utils","Name":"touch","Type":"method","Signature":"(self, mode=438, exist_ok=True)","Arguments":"[\"self\", \"mode\", \"exist_ok\"]","Docstring":"Create this file with the given access mode, if it doesn't exist.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.unlink","Module":"chronos.boto_utils","Name":"unlink","Type":"method","Signature":"(self, missing_ok=False)","Arguments":"[\"self\", \"missing_ok\"]","Docstring":"Remove this file or link.\nIf the path is a directory, use rmdir() instead.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.with_name","Module":"chronos.boto_utils","Name":"with_name","Type":"method","Signature":"(self, name)","Arguments":"[\"self\", \"name\"]","Docstring":"Return a new path with the file name changed.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.with_stem","Module":"chronos.boto_utils","Name":"with_stem","Type":"method","Signature":"(self, stem)","Arguments":"[\"self\", \"stem\"]","Docstring":"Return a new path with the stem changed.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.with_suffix","Module":"chronos.boto_utils","Name":"with_suffix","Type":"method","Signature":"(self, suffix)","Arguments":"[\"self\", \"suffix\"]","Docstring":"Return a new path with the file suffix changed.  If the path\nhas no suffix, add given suffix.  If th","Parent":"Path"},{"Path":"chronos.boto_utils.Path.write_bytes","Module":"chronos.boto_utils","Name":"write_bytes","Type":"method","Signature":"(self, data)","Arguments":"[\"self\", \"data\"]","Docstring":"Open the file in bytes mode, write to it, and close the file.","Parent":"Path"},{"Path":"chronos.boto_utils.Path.write_text","Module":"chronos.boto_utils","Name":"write_text","Type":"method","Signature":"(self, data, encoding=None, errors=None, newline=None)","Arguments":"[\"self\", \"data\", \"encoding\", \"errors\", \"newline\"]","Docstring":"Open the file in text mode, write to it, and close the file.","Parent":"Path"},{"Path":"chronos.boto_utils.Path","Module":"chronos.boto_utils","Name":"Path","Type":"class","Signature":"","Arguments":"[]","Docstring":"PurePath subclass that can make system calls.\n\nPath represents a filesystem path but unlike PurePath","Parent":"chronos"},{"Path":"chronos.boto_utils.UNSIGNED","Module":"chronos.boto_utils","Name":"UNSIGNED","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos"},{"Path":"chronos.boto_utils.cache_model_from_s3","Module":"chronos.boto_utils","Name":"cache_model_from_s3","Type":"function","Signature":"(s3_uri: str, force_download: bool = False, boto3_session: boto3.session.Session | None = None)","Arguments":"[\"s3_uri\", \"force_download\", \"boto3_session\"]","Docstring":"","Parent":"chronos"},{"Path":"chronos.boto_utils.download_model_files_from_cloudfront","Module":"chronos.boto_utils","Name":"download_model_files_from_cloudfront","Type":"function","Signature":"(cloudfront_url: str, bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool = False) -> None","Arguments":"[\"cloudfront_url\", \"bucket\", \"prefix\", \"local_path\", \"force_download\"]","Docstring":"","Parent":"chronos"},{"Path":"chronos.boto_utils.download_model_files_from_s3","Module":"chronos.boto_utils","Name":"download_model_files_from_s3","Type":"function","Signature":"(bucket: str, prefix: str, local_path: pathlib.Path, force_download: bool = False, boto3_session: boto3.session.Session | None = None) -> None","Arguments":"[\"bucket\", \"prefix\", \"local_path\", \"force_download\", \"boto3_session\"]","Docstring":"","Parent":"chronos"},{"Path":"chronos.boto_utils.logger","Module":"chronos.boto_utils","Name":"logger","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Instances of the Logger class represent a single logging channel. A\n\"logging channel\" indicates an a","Parent":"chronos"},{"Path":"chronos.base.BaseChronosPipeline.from_pretrained","Module":"chronos.base","Name":"from_pretrained","Type":"method","Signature":"(pretrained_model_name_or_path: Union[str, pathlib.Path], *model_args, force_s3_download=False, **kwargs)","Arguments":"[\"pretrained_model_name_or_path\", \"model_args\", \"force_s3_download\", \"kwargs\"]","Docstring":"Load the model, either from a local path, S3 prefix, or from the HuggingFace Hub.\nSupports the same ","Parent":"BaseChronosPipeline"},{"Path":"chronos.base.BaseChronosPipeline.predict","Module":"chronos.base","Name":"predict","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None)","Arguments":"[\"self\", \"inputs\", \"prediction_length\"]","Docstring":"Get forecasts for the given time series. Predictions will be\nreturned in fp32 on the cpu.\n\nParameter","Parent":"BaseChronosPipeline"},{"Path":"chronos.base.BaseChronosPipeline.predict_df","Module":"chronos.base","Name":"predict_df","Type":"method","Signature":"(self, df: 'pd.DataFrame', *, id_column: str = 'item_id', timestamp_column: str = 'timestamp', target: str = 'target', prediction_length: int | None = None, quantile_levels: list[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], validate_inputs: bool = True, freq: str | None = None, **predict_kwargs) -> 'pd.DataFrame'","Arguments":"[\"self\", \"df\", \"id_column\", \"timestamp_column\", \"target\", \"prediction_length\", \"quantile_levels\", \"validate_inputs\", \"freq\", \"predict_kwargs\"]","Docstring":"Perform forecasting on time series data in a long-format pandas DataFrame.\n\nParameters\n----------\ndf","Parent":"BaseChronosPipeline"},{"Path":"chronos.base.BaseChronosPipeline.predict_fev","Module":"chronos.base","Name":"predict_fev","Type":"method","Signature":"(self, task: 'fev.Task', batch_size: int = 32, **kwargs) -> tuple[list['datasets.DatasetDict'], float]","Arguments":"[\"self\", \"task\", \"batch_size\", \"kwargs\"]","Docstring":"Make predictions for evaluation on a fev.Task.\n\nParameters\n----------\ntask\n    Benchmark task on whi","Parent":"BaseChronosPipeline"},{"Path":"chronos.base.BaseChronosPipeline.predict_quantiles","Module":"chronos.base","Name":"predict_quantiles","Type":"method","Signature":"(self, inputs: Union[torch.Tensor, List[torch.Tensor]], prediction_length: Optional[int] = None, quantile_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], **kwargs) -> Tuple[torch.Tensor, torch.Tensor]","Arguments":"[\"self\", \"inputs\", \"prediction_length\", \"quantile_levels\", \"kwargs\"]","Docstring":"Get quantile and mean forecasts for given time series.\nPredictions will be returned in fp32 on the c","Parent":"BaseChronosPipeline"},{"Path":"chronos.base.BaseChronosPipeline","Module":"chronos.base","Name":"BaseChronosPipeline","Type":"class","Signature":"","Arguments":"[]","Docstring":"","Parent":"chronos"},{"Path":"chronos.base.Dict","Module":"chronos.base","Name":"Dict","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of dict.","Parent":"chronos"},{"Path":"chronos.base.Enum","Module":"chronos.base","Name":"Enum","Type":"class","Signature":"","Arguments":"[]","Docstring":"Create a collection of name\/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED =","Parent":"chronos"},{"Path":"chronos.base.ForecastType","Module":"chronos.base","Name":"ForecastType","Type":"class","Signature":"","Arguments":"[]","Docstring":"Create a collection of name\/value pairs.\n\nExample enumeration:\n\n>>> class Color(Enum):\n...     RED =","Parent":"chronos"},{"Path":"chronos.base.List","Module":"chronos.base","Name":"List","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"A generic version of list.","Parent":"chronos"},{"Path":"chronos.base.Optional","Module":"chronos.base","Name":"Optional","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Optional[X] is equivalent to Union[X, None].","Parent":"chronos"},{"Path":"chronos.base.Path.absolute","Module":"chronos.base","Name":"absolute","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return an absolute version of this path by prepending the current\nworking directory. No normalizatio","Parent":"Path"},{"Path":"chronos.base.Path.as_posix","Module":"chronos.base","Name":"as_posix","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the string representation of the path with forward (\/)\nslashes.","Parent":"Path"},{"Path":"chronos.base.Path.as_uri","Module":"chronos.base","Name":"as_uri","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the path as a 'file' URI.","Parent":"Path"},{"Path":"chronos.base.Path.chmod","Module":"chronos.base","Name":"chmod","Type":"method","Signature":"(self, mode, *, follow_symlinks=True)","Arguments":"[\"self\", \"mode\", \"follow_symlinks\"]","Docstring":"Change the permissions of the path, like os.chmod().","Parent":"Path"},{"Path":"chronos.base.Path.cwd","Module":"chronos.base","Name":"cwd","Type":"method","Signature":"()","Arguments":"[]","Docstring":"Return a new path pointing to the current working directory\n(as returned by os.getcwd()).","Parent":"Path"},{"Path":"chronos.base.Path.exists","Module":"chronos.base","Name":"exists","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path exists.","Parent":"Path"},{"Path":"chronos.base.Path.expanduser","Module":"chronos.base","Name":"expanduser","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return a new path with expanded ~ and ~user constructs\n(as returned by os.path.expanduser)","Parent":"Path"},{"Path":"chronos.base.Path.glob","Module":"chronos.base","Name":"glob","Type":"method","Signature":"(self, pattern)","Arguments":"[\"self\", \"pattern\"]","Docstring":"Iterate over this subtree and yield all existing files (of any\nkind, including directories) matching","Parent":"Path"},{"Path":"chronos.base.Path.group","Module":"chronos.base","Name":"group","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the group name of the file gid.","Parent":"Path"},{"Path":"chronos.base.Path.hardlink_to","Module":"chronos.base","Name":"hardlink_to","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Make this path a hard link pointing to the same file as *target*.\n\nNote the order of arguments (self","Parent":"Path"},{"Path":"chronos.base.Path.home","Module":"chronos.base","Name":"home","Type":"method","Signature":"()","Arguments":"[]","Docstring":"Return a new path pointing to the user's home directory (as\nreturned by os.path.expanduser('~')).","Parent":"Path"},{"Path":"chronos.base.Path.is_absolute","Module":"chronos.base","Name":"is_absolute","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"True if the path is absolute (has both a root and, if applicable,\na drive).","Parent":"Path"},{"Path":"chronos.base.Path.is_block_device","Module":"chronos.base","Name":"is_block_device","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a block device.","Parent":"Path"},{"Path":"chronos.base.Path.is_char_device","Module":"chronos.base","Name":"is_char_device","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a character device.","Parent":"Path"},{"Path":"chronos.base.Path.is_dir","Module":"chronos.base","Name":"is_dir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a directory.","Parent":"Path"},{"Path":"chronos.base.Path.is_fifo","Module":"chronos.base","Name":"is_fifo","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a FIFO.","Parent":"Path"},{"Path":"chronos.base.Path.is_file","Module":"chronos.base","Name":"is_file","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a regular file (also True for symlinks pointing\nto regular files).","Parent":"Path"},{"Path":"chronos.base.Path.is_mount","Module":"chronos.base","Name":"is_mount","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Check if this path is a POSIX mount point","Parent":"Path"},{"Path":"chronos.base.Path.is_relative_to","Module":"chronos.base","Name":"is_relative_to","Type":"method","Signature":"(self, *other)","Arguments":"[\"self\", \"other\"]","Docstring":"Return True if the path is relative to another path or False.\n        ","Parent":"Path"},{"Path":"chronos.base.Path.is_reserved","Module":"chronos.base","Name":"is_reserved","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return True if the path contains one of the special names reserved\nby the system, if any.","Parent":"Path"},{"Path":"chronos.base.Path.is_socket","Module":"chronos.base","Name":"is_socket","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a socket.","Parent":"Path"},{"Path":"chronos.base.Path.is_symlink","Module":"chronos.base","Name":"is_symlink","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Whether this path is a symbolic link.","Parent":"Path"},{"Path":"chronos.base.Path.iterdir","Module":"chronos.base","Name":"iterdir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Iterate over the files in this directory.  Does not yield any\nresult for the special paths '.' and '","Parent":"Path"},{"Path":"chronos.base.Path.joinpath","Module":"chronos.base","Name":"joinpath","Type":"method","Signature":"(self, *args)","Arguments":"[\"self\", \"args\"]","Docstring":"Combine this path with one or several arguments, and return a\nnew path representing either a subpath","Parent":"Path"},{"Path":"chronos.base.Path.lchmod","Module":"chronos.base","Name":"lchmod","Type":"method","Signature":"(self, mode)","Arguments":"[\"self\", \"mode\"]","Docstring":"Like chmod(), except if the path points to a symlink, the symlink's\npermissions are changed, rather ","Parent":"Path"},{"Path":"chronos.base.Path.link_to","Module":"chronos.base","Name":"link_to","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Make the target path a hard link pointing to this path.\n\nNote this function does not make this path ","Parent":"Path"},{"Path":"chronos.base.Path.lstat","Module":"chronos.base","Name":"lstat","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Like stat(), except if the path points to a symlink, the symlink's\nstatus information is returned, r","Parent":"Path"},{"Path":"chronos.base.Path.match","Module":"chronos.base","Name":"match","Type":"method","Signature":"(self, path_pattern)","Arguments":"[\"self\", \"path_pattern\"]","Docstring":"Return True if this path matches the given pattern.","Parent":"Path"},{"Path":"chronos.base.Path.mkdir","Module":"chronos.base","Name":"mkdir","Type":"method","Signature":"(self, mode=511, parents=False, exist_ok=False)","Arguments":"[\"self\", \"mode\", \"parents\", \"exist_ok\"]","Docstring":"Create a new directory at this given path.","Parent":"Path"},{"Path":"chronos.base.Path.open","Module":"chronos.base","Name":"open","Type":"method","Signature":"(self, mode='r', buffering=-1, encoding=None, errors=None, newline=None)","Arguments":"[\"self\", \"mode\", \"buffering\", \"encoding\", \"errors\", \"newline\"]","Docstring":"Open the file pointed by this path and return a file object, as\nthe built-in open() function does.","Parent":"Path"},{"Path":"chronos.base.Path.owner","Module":"chronos.base","Name":"owner","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the login name of the file owner.","Parent":"Path"},{"Path":"chronos.base.Path.read_bytes","Module":"chronos.base","Name":"read_bytes","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Open the file in bytes mode, read it, and close the file.","Parent":"Path"},{"Path":"chronos.base.Path.read_text","Module":"chronos.base","Name":"read_text","Type":"method","Signature":"(self, encoding=None, errors=None)","Arguments":"[\"self\", \"encoding\", \"errors\"]","Docstring":"Open the file in text mode, read it, and close the file.","Parent":"Path"},{"Path":"chronos.base.Path.readlink","Module":"chronos.base","Name":"readlink","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Return the path to which the symbolic link points.","Parent":"Path"},{"Path":"chronos.base.Path.relative_to","Module":"chronos.base","Name":"relative_to","Type":"method","Signature":"(self, *other)","Arguments":"[\"self\", \"other\"]","Docstring":"Return the relative path to another path identified by the passed\narguments.  If the operation is no","Parent":"Path"},{"Path":"chronos.base.Path.rename","Module":"chronos.base","Name":"rename","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Rename this path to the target path.\n\nThe target path may be absolute or relative. Relative paths ar","Parent":"Path"},{"Path":"chronos.base.Path.replace","Module":"chronos.base","Name":"replace","Type":"method","Signature":"(self, target)","Arguments":"[\"self\", \"target\"]","Docstring":"Rename this path to the target path, overwriting if that path exists.\n\nThe target path may be absolu","Parent":"Path"},{"Path":"chronos.base.Path.resolve","Module":"chronos.base","Name":"resolve","Type":"method","Signature":"(self, strict=False)","Arguments":"[\"self\", \"strict\"]","Docstring":"Make the path absolute, resolving all symlinks on the way and also\nnormalizing it.","Parent":"Path"},{"Path":"chronos.base.Path.rglob","Module":"chronos.base","Name":"rglob","Type":"method","Signature":"(self, pattern)","Arguments":"[\"self\", \"pattern\"]","Docstring":"Recursively yield all existing files (of any kind, including\ndirectories) matching the given relativ","Parent":"Path"},{"Path":"chronos.base.Path.rmdir","Module":"chronos.base","Name":"rmdir","Type":"method","Signature":"(self)","Arguments":"[\"self\"]","Docstring":"Remove this directory.  The directory must be empty.","Parent":"Path"},{"Path":"chronos.base.Path.samefile","Module":"chronos.base","Name":"samefile","Type":"method","Signature":"(self, other_path)","Arguments":"[\"self\", \"other_path\"]","Docstring":"Return whether other_path is the same or not as this file\n(as returned by os.path.samefile()).","Parent":"Path"},{"Path":"chronos.base.Path.stat","Module":"chronos.base","Name":"stat","Type":"method","Signature":"(self, *, follow_symlinks=True)","Arguments":"[\"self\", \"follow_symlinks\"]","Docstring":"Return the result of the stat() system call on this path, like\nos.stat() does.","Parent":"Path"},{"Path":"chronos.base.Path.symlink_to","Module":"chronos.base","Name":"symlink_to","Type":"method","Signature":"(self, target, target_is_directory=False)","Arguments":"[\"self\", \"target\", \"target_is_directory\"]","Docstring":"Make this path a symlink pointing to the target path.\nNote the order of arguments (link, target) is ","Parent":"Path"},{"Path":"chronos.base.Path.touch","Module":"chronos.base","Name":"touch","Type":"method","Signature":"(self, mode=438, exist_ok=True)","Arguments":"[\"self\", \"mode\", \"exist_ok\"]","Docstring":"Create this file with the given access mode, if it doesn't exist.","Parent":"Path"},{"Path":"chronos.base.Path.unlink","Module":"chronos.base","Name":"unlink","Type":"method","Signature":"(self, missing_ok=False)","Arguments":"[\"self\", \"missing_ok\"]","Docstring":"Remove this file or link.\nIf the path is a directory, use rmdir() instead.","Parent":"Path"},{"Path":"chronos.base.Path.with_name","Module":"chronos.base","Name":"with_name","Type":"method","Signature":"(self, name)","Arguments":"[\"self\", \"name\"]","Docstring":"Return a new path with the file name changed.","Parent":"Path"},{"Path":"chronos.base.Path.with_stem","Module":"chronos.base","Name":"with_stem","Type":"method","Signature":"(self, stem)","Arguments":"[\"self\", \"stem\"]","Docstring":"Return a new path with the stem changed.","Parent":"Path"},{"Path":"chronos.base.Path.with_suffix","Module":"chronos.base","Name":"with_suffix","Type":"method","Signature":"(self, suffix)","Arguments":"[\"self\", \"suffix\"]","Docstring":"Return a new path with the file suffix changed.  If the path\nhas no suffix, add given suffix.  If th","Parent":"Path"},{"Path":"chronos.base.Path.write_bytes","Module":"chronos.base","Name":"write_bytes","Type":"method","Signature":"(self, data)","Arguments":"[\"self\", \"data\"]","Docstring":"Open the file in bytes mode, write to it, and close the file.","Parent":"Path"},{"Path":"chronos.base.Path.write_text","Module":"chronos.base","Name":"write_text","Type":"method","Signature":"(self, data, encoding=None, errors=None, newline=None)","Arguments":"[\"self\", \"data\", \"encoding\", \"errors\", \"newline\"]","Docstring":"Open the file in text mode, write to it, and close the file.","Parent":"Path"},{"Path":"chronos.base.Path","Module":"chronos.base","Name":"Path","Type":"class","Signature":"","Arguments":"[]","Docstring":"PurePath subclass that can make system calls.\n\nPath represents a filesystem path but unlike PurePath","Parent":"chronos"},{"Path":"chronos.base.PipelineRegistry","Module":"chronos.base","Name":"PipelineRegistry","Type":"class","Signature":"","Arguments":"[]","Docstring":"type(object) -> the object's type\ntype(name, bases, dict, **kwds) -> a new type","Parent":"chronos"},{"Path":"chronos.base.TYPE_CHECKING","Module":"chronos.base","Name":"TYPE_CHECKING","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"bool(x) -> bool\n\nReturns True when the argument x is true, False otherwise.\nThe builtins True and Fa","Parent":"chronos"},{"Path":"chronos.base.Tuple","Module":"chronos.base","Name":"Tuple","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Deprecated alias to builtins.tuple.\n\nTuple[X, Y] is the cross-product type of X and Y.\n\nExample: Tup","Parent":"chronos"},{"Path":"chronos.base.Union","Module":"chronos.base","Name":"Union","Type":"unknown","Signature":"","Arguments":"[]","Docstring":"Union type; Union[X, Y] means either X or Y.\n\nOn Python 3.10 and higher, the | operator\ncan also be ","Parent":"chronos"},{"Path":"chronos.base.left_pad_and_stack_1D","Module":"chronos.base","Name":"left_pad_and_stack_1D","Type":"function","Signature":"(tensors: List[torch.Tensor]) -> torch.Tensor","Arguments":"[\"tensors\"]","Docstring":"","Parent":"chronos"}]