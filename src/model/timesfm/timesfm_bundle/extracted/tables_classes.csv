ID,Parent,Type,Name,Path,Module,Docstring,Params,ReturnType
2,1,class,TimeSeriesdata,timesfm.data_loader.TimeSeriesdata,timesfm.data_loader,Data loader class.,,
17,15,class,TimeCovariates,timesfm.time_features.TimeCovariates,timesfm.time_features,Extract all time covariates except for holidays.,,
38,36,class,TimesFMConfig,timesfm.pytorch_patched_decoder.TimesFMConfig,timesfm.pytorch_patched_decoder,Config for initializing timesfm patched_decoder class.,,
46,36,class,ResidualBlock,timesfm.pytorch_patched_decoder.ResidualBlock,timesfm.pytorch_patched_decoder,TimesFM residual block.,,
49,36,class,RMSNorm,timesfm.pytorch_patched_decoder.RMSNorm,timesfm.pytorch_patched_decoder,Pax rms norm in pytorch.,,
53,36,class,TransformerMLP,timesfm.pytorch_patched_decoder.TransformerMLP,timesfm.pytorch_patched_decoder,Pax transformer MLP in pytorch.,,
56,36,class,TimesFMAttention,timesfm.pytorch_patched_decoder.TimesFMAttention,timesfm.pytorch_patched_decoder,Implements the attention used in TimesFM.,,
60,36,class,TimesFMDecoderLayer,timesfm.pytorch_patched_decoder.TimesFMDecoderLayer,timesfm.pytorch_patched_decoder,Transformer layer.,,
63,36,class,StackedDecoder,timesfm.pytorch_patched_decoder.StackedDecoder,timesfm.pytorch_patched_decoder,Stacked transformer layer.,,
66,36,class,PositionalEmbedding,timesfm.pytorch_patched_decoder.PositionalEmbedding,timesfm.pytorch_patched_decoder,Generates position embedding for a given 1-d sequence.\n\nAttributes:\n min_timescale: Start of the geometric index. Determines the periodicity of\n the added signal.\n max_timescale: End of the geometric index. Determines the frequency of the\n added signal.\n embedding_dims: Dimension of the embedding to be generated.,,
69,36,class,PatchedTimeSeriesDecoder,timesfm.pytorch_patched_decoder.PatchedTimeSeriesDecoder,timesfm.pytorch_patched_decoder,Patched time-series decoder.,,
83,82,class,TimesFmJax,timesfm.timesfm_jax.TimesFmJax,timesfm.timesfm_jax,"TimesFM forecast API for inference.\n\nThis class is the scaffolding for calling TimesFM forecast. To properly use:\n 1. Create an instance with the correct hyperparameters of a TimesFM model.\n 2. Call `load_from_checkpoint` to load a compatible checkpoint.\n 3. Call `forecast` for inference.\n\nGiven the model size, this API does not shard the model weights for SPMD. All\nparallelism happens on the data dimension.\n\nCompilation happens during the first time `forecast` is called and uses the\n`per_core_batch_size` to set and freeze the input signature. Subsequent calls\nto `forecast` reflect the actual inference latency.",,
106,102,class,BatchedInContextXRegBase,timesfm.xreg_lib.BatchedInContextXRegBase,timesfm.xreg_lib,Helper class for in-context regression covariate formatting.\n\nAttributes:\n targets: List of targets (responses) of the in-context regression.\n train_lens: List of lengths of each target vector from the context.\n test_lens: List of lengths of each forecast horizon.\n train_dynamic_numerical_covariates: Dict of covariate names mapping to the\n dynamic numerical covariates of each forecast task on the context. Their\n lengths should match the corresponding lengths in `train_lens`.\n train_dynamic_categorical_covariates: Dict of covariate names mapping to the\n dynamic categorical covariates of each forecast task on the context. Their\n lengths should match the corresponding lengths in `train_lens`.\n test_dynamic_numerical_covariates: Dict of covariate names mapping to the\n dynamic numerical covariates of each forecast task on the horizon. Their\n lengths should match the corresponding lengths in `test_lens`.\n test_dynamic_categorical_covariates: Dict of covariate names mapping to the\n dynamic categorical covariates of each forecast task on the horizon. Their\n lengths should match the corresponding lengths in `test_lens`.\n static_numerical_covariates: Dict of covariate names mapping to the static\n numerical covariates of each forecast task.\n static_categorical_covariates: Dict of covariate names mapping to the static\n categorical covariates of each forecast task.,,
111,102,class,BatchedInContextXRegLinear,timesfm.xreg_lib.BatchedInContextXRegLinear,timesfm.xreg_lib,Linear in-context regression model.,,
122,120,class,ResidualBlock,timesfm.patched_decoder.ResidualBlock,timesfm.patched_decoder,Simple feedforward block with residual connection.\n\nAttributes:\n input_dims: input dimension.\n hidden_dims: hidden dimension.\n output_dims: output dimension.\n dropout_prob: dropout probability.\n layer_norm: whether to use layer norm or not.\n dropout_tpl: config for dropout.\n ln_tpl: config for layer norm.\n act_tpl: config for activation in hidden layer.,,
127,120,class,PatchedTimeSeriesDecoder,timesfm.patched_decoder.PatchedTimeSeriesDecoder,timesfm.patched_decoder,"Patch decoder layer for time-series foundation model.\n\nAttributes:\n patch_len: length of input patches.\n horizon_len: length of output patches. Referred to as `output_patch_len`\n during inference.\n model_dims: model dimension of stacked transformer layer.\n hidden_dims: hidden dimensions in fully connected layers.\n quantiles: list of quantiles for non prob model.\n residual_block_tpl: config for residual block.\n stacked_transformer_params_tpl: config for stacked transformer.\n use_freq: whether to use frequency encoding.\n\nIn all of what followed, except specified otherwise, B is batch size, T is\nsequence length of time-series. N is the number of input patches that can be\nobtained from T. P is the input patch length and H is the horizon length. Q is\nnumber of output logits. D is model dimension.",,
136,120,class,PatchedDecoderFinetuneModel,timesfm.patched_decoder.PatchedDecoderFinetuneModel,timesfm.patched_decoder,Model class for finetuning patched time-series decoder.\n\nAttributes:\n core_layer_tpl: config for core layer.\n freq: freq to finetune on.,,
149,148,class,TimesFmTorch,timesfm.timesfm_torch.TimesFmTorch,timesfm.timesfm_torch,TimesFM forecast API for inference.,,
167,159,class,TimesFmHparams,timesfm.timesfm_base.TimesFmHparams,timesfm.timesfm_base,"Hparams used to initialize a TimesFM model for inference.\n\nThese are the sufficient subset of hparams to configure TimesFM inference\nagnostic to the checkpoint version, and are not necessarily the same as the\nhparams used to train the checkpoint.\n\nAttributes:\n context_len: Largest context length the model allows for each decode call.\n This technically can be any large, but practically should set to the\n context length the checkpoint was trained with.\n horizon_len: Forecast horizon.\n input_patch_len: Input patch len.\n output_patch_len: Output patch len. How many timepoints is taken from a\n single step of autoregressive decoding. Can be set as the training horizon\n of the checkpoint.\n num_layers: Number of transformer layers in the model.\n model_dims: Model dimension.\n per_core_batch_size: Batch size on each core for data parallelism.\n backend: One of ""cpu"", ""gpu"" or ""tpu"".\n quantiles: Which quantiles are output by the model.",,
168,159,class,TimesFmCheckpoint,timesfm.timesfm_base.TimesFmCheckpoint,timesfm.timesfm_base,"Checkpoint used to initialize a TimesFM model for inference.\n\nAttributes:\n version: Version of the checkpoint, e.g. ""jax"", ""torch"", ""tensorflow"", etc.\n The factory will create the corresponding TimesFm inference class based on\n this version.\n path: Path to the checkpoint.\n type: If provided, type of the checkpoint used by the specific checkpoint\n loader per version.\n step: If provided, step of the checkpoint.",,
169,159,class,TimesFmBase,timesfm.timesfm_base.TimesFmBase,timesfm.timesfm_base,Base TimesFM forecast API for inference.\n\nThis class is the scaffolding for calling TimesFM forecast. To properly use:\n 1. Create an instance with the correct hyperparameters of a TimesFM model.\n 2. Call `load_from_checkpoint` to load a compatible checkpoint.\n 3. Call `forecast` for inference.,,
