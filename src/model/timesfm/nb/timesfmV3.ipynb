{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4325785e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad716962",
   "metadata": {},
   "source": [
    "| 順序 | モジュール名 (Name) | パス (Path) | 機能・実装理由 (Description) |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | configs | `timesfm.configs` | 【基盤】設定定義\n",
    "\n",
    "<br>モデル設定（`ForecastConfig`等）を定義します。後続のほぼ全てのモジュールで型定義として参照されるため、最初に実装が必要です。 |\n",
    "| 2 | util | `timesfm.torch.util`<br>\n",
    "\n",
    "<br>`timesfm.flax.util` | 【基盤】ユーティリティ<br>\n",
    "\n",
    "<br>計算補助関数、キャッシュ機構（`DecodeCache`）など、レイヤー実装に必要な基本的ツール群です。 |\n",
    "| 3 | normalization | `timesfm.torch.normalization`<br>\n",
    "\n",
    "<br>`timesfm.flax.normalization` | 【部品】正規化層<br>\n",
    "\n",
    "<br>`RMSNorm` などの基本的な正規化レイヤーです。依存関係が少なく、早期に実装可能です。 |\n",
    "| 4 | dense | `timesfm.torch.dense`<br>\n",
    "\n",
    "<br>`timesfm.flax.dense` | 【部品】全結合層<br>\n",
    "\n",
    "<br>残差ブロック（`ResidualBlock`）やフーリエ特徴量埋め込みなど、Transformerの構成要素となります。 |\n",
    "| 5 | transformer | `timesfm.torch.transformer`<br>\n",
    "\n",
    "<br>`timesfm.flax.transformer` | 【部品】Transformer層<br>\n",
    "\n",
    "<br>Attention機構やTransformerブロック本体です。これまでの `dense`, `normalization`, `util` を組み合わせて構築します。 |\n",
    "| 6 | xreg_lib | `timesfm.utils.xreg_lib` | 【機能】共変量ライブラリ<br>\n",
    "\n",
    "<br>外部共変量（外生変数）を処理するための独立したロジックです。モデル本体の推論ロジックで使用されます。 |\n",
    "| 7 | timesfm_2p5_base | `timesfm.timesfm_2p5.timesfm_2p5_base` | 【骨格】モデル基底クラス<br>\n",
    "\n",
    "<br>モデルの共通インターフェース、前処理、推論のワークフローを定義する抽象クラスです。 |\n",
    "| 8 | timesfm_2p5_torch<br>\n",
    "\n",
    "<br>(or _flax) | `timesfm.timesfm_2p5.timesfm_2p5_torch`<br>\n",
    "\n",
    "<br>`timesfm.timesfm_2p5.timesfm_2p5_flax` | 【統合】モデル実装<br>\n",
    "\n",
    "<br>これまでに作成した部品（config, layers, base）を統合し、具体的な `TimesFM` モデル（ロード、推論処理）を完成させます。 |\n",
    "| 9 | timesfm | `timesfm` | 【I/F】APIエンドポイント<br>\n",
    "\n",
    "<br>ユーザーがライブラリをインポートして利用するためのトップレベルAPIです。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6e8bd",
   "metadata": {},
   "source": [
    "# timesfm.configs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ac850",
   "metadata": {},
   "source": [
    "## 予測設定 (ForecastConfig) の定義\n",
    "推論時の最大コンテキスト長、予測期間（ホライズン）、バッチサイズなどの詳細設定を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566a39f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForecastConfig created: context=512, horizon=96\n"
     ]
    }
   ],
   "source": [
    "from timesfm.configs import ForecastConfig\n",
    "\n",
    "# 予測設定の作成\n",
    "# TODO: 用途に合わせて各パラメータを調整してください\n",
    "obj = ForecastConfig(\n",
    "    max_context=512,                 # モデルに入力する最大過去データ点数 (例: 512)\n",
    "    max_horizon=96,                  # 一度に予測する最大ステップ数 (例: 96)\n",
    "    normalize_inputs=True,           # 入力データを正規化するか (推奨: True)\n",
    "    window_size=None,                # 分解予測時のウィンドウサイズ (通常は None)\n",
    "    per_core_batch_size=32,          # コアごとのバッチサイズ\n",
    "    use_continuous_quantile_head=False, # 連続分位点ヘッドを使用するか\n",
    "    force_flip_invariance=False,     # 反転不変性 (符号反転への対応) を強制するか\n",
    "    infer_is_positive=False,         # 出力が非負であることを保証するか\n",
    "    fix_quantile_crossing=True,      # 分位点の交差を修正するか\n",
    "    return_backcast=False,           # 過去データの再構成 (backcast) を返すか\n",
    ")\n",
    "\n",
    "print(f\"ForecastConfig created: context={obj.max_context}, horizon={obj.max_horizon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca311d9",
   "metadata": {},
   "source": [
    "## ランダムフーリエ特徴量設定 (RandomFourierFeaturesConfig) の定義\n",
    "時系列データの周波数成分を捉えるためのランダムフーリエ特徴量レイヤーの設定を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335f9ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFF Config created: input=64, output=64\n"
     ]
    }
   ],
   "source": [
    "from timesfm.configs import RandomFourierFeaturesConfig\n",
    "\n",
    "# ランダムフーリエ特徴量レイヤーの設定作成\n",
    "# TODO: モデルのアーキテクチャに合わせて値を調整してください\n",
    "obj = RandomFourierFeaturesConfig(\n",
    "    input_dims=64,           # 入力次元数 (int)\n",
    "    output_dims=64,          # 出力次元数 (int)\n",
    "    projection_stddev=0.01,  # 投影重みの初期化標準偏差 (float)\n",
    "    use_bias=False,          # バイアス項を使用するか (bool)\n",
    ")\n",
    "\n",
    "print(f\"RFF Config created: input={obj.input_dims}, output={obj.output_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca57611",
   "metadata": {},
   "source": [
    "## 残差ブロック設定 (ResidualBlockConfig) の定義\n",
    "モデル内の各層で使用される残差ブロック（Residual Block）の次元数や活性化関数などの構成を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e31de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock Config created: 64 -> 128 -> 64 (act=swish)\n"
     ]
    }
   ],
   "source": [
    "from timesfm.configs import ResidualBlockConfig\n",
    "\n",
    "# 残差ブロックの設定作成\n",
    "# TODO: モデルの各層の設計に合わせて値を調整してください\n",
    "obj = ResidualBlockConfig(\n",
    "    input_dims=64,           # 入力次元数 (int)\n",
    "    hidden_dims=128,         # 隠れ層の次元数 (int)\n",
    "    output_dims=64,          # 出力次元数 (int)\n",
    "    use_bias=True,           # バイアス項を使用するか (bool)\n",
    "    activation=\"swish\",      # 活性化関数 (Literal[\"relu\", \"swish\", \"none\"])\n",
    ")\n",
    "\n",
    "print(f\"ResidualBlock Config created: {obj.input_dims} -> {obj.hidden_dims} -> {obj.output_dims} (act={obj.activation})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10140c1",
   "metadata": {},
   "source": [
    "## 積み上げ型Transformer設定 (StackedTransformersConfig) の定義\n",
    "モデルの核となるTransformerブロックを何層積み上げるか、および各層の詳細な構成（TransformerConfig）を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd927fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Transformers: 20 layers of 1024 dims\n"
     ]
    }
   ],
   "source": [
    "from timesfm.configs import TransformerConfig, StackedTransformersConfig\n",
    "\n",
    "# 1. 個別のTransformer層の詳細設定\n",
    "# TODO: モデルの規模（200M等）に合わせて調整してください\n",
    "transformer_config = TransformerConfig(\n",
    "    model_dims=1024,\n",
    "    hidden_dims=4096,\n",
    "    num_heads=16,\n",
    "    attention_norm=\"rms\",\n",
    "    feedforward_norm=\"rms\",\n",
    "    qk_norm=\"rms\",\n",
    "    use_bias=False,\n",
    "    use_rotary_position_embeddings=True,\n",
    "    ff_activation=\"swish\",\n",
    "    fuse_qkv=True\n",
    ")\n",
    "\n",
    "# 2. Transformerを積み上げる設定の作成\n",
    "# TODO: レイヤー数を指定してください\n",
    "obj = StackedTransformersConfig(\n",
    "    num_layers=20,            # 積み上げるTransformerの層数 (int)\n",
    "    transformer=transformer_config  # 上記で定義したTransformerConfigオブジェクト\n",
    ")\n",
    "\n",
    "print(f\"Stacked Transformers: {obj.num_layers} layers of {obj.transformer.model_dims} dims\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a236eb",
   "metadata": {},
   "source": [
    "## Transformer構成設定 (TransformerConfig) の定義\n",
    "\n",
    "TimesFMモデルの核となるTransformerブロックの内部アーキテクチャを詳細に定義します。この設定は、アテンション機構の挙動、正規化の手法、およびフィードフォワードネットワークの構成を決定します。\n",
    "\n",
    "### 主な設定項目:\n",
    "* **次元数設定**: モデルの基底次元 (`model_dims`) とフィードフォワード層の隠れ次元 (`hidden_dims`) を指定します。\n",
    "* **正規化 (Normalization)**: `attention_norm` や `feedforward_norm` に `rms` (Root Mean Square Layer Normalization) を指定し、学習の安定化を図ります。\n",
    "* **Q/K 正規化 (QK Norm)**: クエリ(Q)とキー(K)に対して正規化を行うことで、アテンションスコアの極端な増大を抑制します。\n",
    "* **埋め込み方式**: `use_rotary_position_embeddings` (RoPE) を有効にし、相対的な位置情報を効率的に扱います。\n",
    "* **効率化**: `fuse_qkv` を True にすることで、Q, K, V の計算を 1 つの行列演算に統合し、計算速度を向上させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d4ce2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Config created: 1024 dims, 16 heads, act=swish\n"
     ]
    }
   ],
   "source": [
    "from timesfm.configs import TransformerConfig\n",
    "\n",
    "# Transformerの詳細なアーキテクチャ設定を作成\n",
    "# TODO: 構築するモデルのパラメータ数や計算リソースに応じて値を調整してください\n",
    "obj = TransformerConfig(\n",
    "    model_dims=1024,                      # モデルの基底次元数 (int)\n",
    "    hidden_dims=4096,                     # フィードフォワード層の隠れ次元数 (int)\n",
    "    num_heads=16,                         # マルチヘッドアテンションのヘッド数 (int)\n",
    "    attention_norm=\"rms\",                 # アテンション層の正規化手法 (Literal[\"rms\"])\n",
    "    feedforward_norm=\"rms\",               # フィードフォワード層の正規化手法 (Literal[\"rms\"])\n",
    "    qk_norm=\"rms\",                        # Query/Keyへの正規化適用 (Literal[\"rms\", \"none\"])\n",
    "    use_bias=False,                       # 線形層でバイアス項を使用するか (bool)\n",
    "    use_rotary_position_embeddings=True,  # Rotary Positional Embeddingsを使用するか (bool)\n",
    "    ff_activation=\"swish\",                # フィードフォワード層の活性化関数 (Literal[\"relu\", \"swish\", \"none\"])\n",
    "    fuse_qkv=True                         # Q, K, Vの計算を統合するか (bool)\n",
    ")\n",
    "\n",
    "print(f\"Transformer Config created: {obj.model_dims} dims, {obj.num_heads} heads, act={obj.ff_activation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a99d7c",
   "metadata": {},
   "source": [
    "# timesfm.flax.dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ecc3d",
   "metadata": {},
   "source": [
    "## デコードキャッシュ設定 (DecodeCache) の定義\n",
    "\n",
    "TimesFM 2.5 の Flax 実装において、自己回帰的（Autoregressive）なデコードを効率化するためのキャッシュ機構を定義します。このキャッシュは、トランスフォーマーのアテンション計算における過去の Key と Value を保持し、推論速度を大幅に向上させます。\n",
    "\n",
    "### 構成要素の詳細:\n",
    "* **next_index**: 次に書き込むキャッシュのインデックスを保持する配列です。バッチごとに管理されます。\n",
    "* **num_masked**: マスクされているトークンの数を管理します。\n",
    "* **key / value**: アテンション機構で再利用される Key と Value のテンソルです。形状は `[batch, length, heads, dims]` となります。\n",
    "* **イミュータブルな設計**: このクラスは `dataclasses.dataclass` かつ `frozen=True` で定義されており、関数型プログラミングのパラダイムに沿った安全な状態管理を可能にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c8f5270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2026-02-11 22:27:42,182:jax._src.xla_bridge:791: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flax DecodeCache initialized: Key shape (32, 512, 16, 64)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from timesfm.flax.util import DecodeCache\n",
    "\n",
    "# 1. キャッシュの次元定義 (例: batch_size=32, seq_len=512, num_heads=16, head_dim=64)\n",
    "batch_size = 32\n",
    "max_len = 512\n",
    "num_heads = 16\n",
    "head_dim = 64\n",
    "\n",
    "# 2. デコードキャッシュのインスタンス化\n",
    "# TODO: モデルの推論状態に合わせて各配列の値を初期化してください\n",
    "obj = DecodeCache(\n",
    "    next_index=jnp.zeros((batch_size,), dtype=jnp.int32),  # 次の書き込み位置\n",
    "    num_masked=jnp.zeros((batch_size,), dtype=jnp.int32),  # マスクされたトークン数\n",
    "    key=jnp.zeros((batch_size, max_len, num_heads, head_dim), dtype=jnp.float32),   # Keyキャッシュ\n",
    "    value=jnp.zeros((batch_size, max_len, num_heads, head_dim), dtype=jnp.float32)  # Valueキャッシュ\n",
    ")\n",
    "\n",
    "print(f\"Flax DecodeCache initialized: Key shape {obj.key.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53934902",
   "metadata": {},
   "source": [
    "## RandomFourierFeatures（ランダムフーリエ特徴量）で入力を周期特徴へ写像する\n",
    "\n",
    "`RandomFourierFeatures` は、入力 `x`（例：時刻特徴や連続値特徴）を、ランダムな線形射影→ `sin` / `cos` 変換で高次元の周期特徴へ変換する層です（Random Fourier Features：カーネル近似の定番テクニック）。:contentReference[oaicite:0]{index=0}\n",
    "\n",
    "### 典型的な引数（実装差があるので実際は signature を確認）\n",
    "- `scale`（スケール）：周波数の大きさ。大きいほど細かく振動する特徴になる\n",
    "- `n_features`（特徴数）：出力の次元。`sin` と `cos` を連結する都合で偶数になりがち\n",
    "\n",
    "### 入力テンソル形状\n",
    "- `x: Float[Array, 'b ... i']`\n",
    "  - `b`：バッチ次元\n",
    "  - `...`：任意の追加次元（例：系列長）\n",
    "  - `i`：入力特徴次元（最後の次元が入力チャンネル）\n",
    "\n",
    "※ Flax(linen) の `nn.Module` なら、単体テストは `init(...)` で変数を作ってから `apply(...)` で推論します（`__call__` を直接は叩かない）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95663cb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flax'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtimesfm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdense\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomFourierFeatures\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_rff\u001b[39m(**preferred_kwargs):\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    timesfm の RandomFourierFeatures はバージョン差で引数名が変わる可能性があるので、\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    signature に存在する引数だけを渡す。\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ts/lib/python3.11/site-packages/timesfm/flax/dense.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2025 Google LLC\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Dense layers for TimesFM.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflax\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nnx\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'flax'"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from timesfm.flax.dense import RandomFourierFeatures\n",
    "\n",
    "def build_rff(**preferred_kwargs):\n",
    "    \"\"\"\n",
    "    timesfm の RandomFourierFeatures はバージョン差で引数名が変わる可能性があるので、\n",
    "    signature に存在する引数だけを渡す。\n",
    "    \"\"\"\n",
    "    sig = inspect.signature(RandomFourierFeatures)\n",
    "    allowed = set(sig.parameters.keys())\n",
    "    filtered = {k: v for k, v in preferred_kwargs.items() if k in allowed}\n",
    "    return RandomFourierFeatures(**filtered)\n",
    "\n",
    "# --- ここが「TODO: __init__ args」に相当 ---\n",
    "# よくある名前：scale / n_features（実際に存在するものだけ渡されます）\n",
    "obj = build_rff(scale=1.0, n_features=256)\n",
    "\n",
    "# --- ここが「TODO: x」に相当 ---\n",
    "# x: Float[Array, 'b ... i'] 例）(batch=2, length=8, input_dim=3)\n",
    "x = jnp.ones((2, 8, 3), dtype=jnp.float32)\n",
    "\n",
    "# Flax Module の一般的な単体テスト流儀：init → apply\n",
    "# （__call__ を単体で直接叩かない）\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "if hasattr(obj, \"init\") and hasattr(obj, \"apply\"):\n",
    "    variables = obj.init(rng, x)\n",
    "    result = obj.apply(variables, x)\n",
    "else:\n",
    "    # もし Flax Module ではなく単なる callable 実装だった場合のフォールバック\n",
    "    result = obj(x)\n",
    "\n",
    "print(\"input shape :\", x.shape, x.dtype)\n",
    "print(\"output shape:\", result.shape, result.dtype)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
